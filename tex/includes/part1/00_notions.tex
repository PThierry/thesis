%%
%%
%% hardware_impacts.tex for thesis in /doctorat/these/tex
%%
%% Made by Philippe THIERRY
%% Login   <Philippe THIERRYreseau-libre.net>
%%
%% Started on  Fri Mar 12 16:36:41 2010 Philippe THIERRY
%% Last update Mon Aug 30 17:00:21 2010 Philippe THIERRY

\chapter{Notions techniques}
\doMinitoc

\section{Principes généraux de la sécurité}

\subsection{Ce que cible la sécurité logicielle}


\section{Principe généraux des systèmes temps réel}

\subsection{Définition du temps réel}

\paragraph{}
Comme le montre le schéma \ref{fig:rt_hard_soft}, il existe trois grandes familles de taches :
\begin{itemize}
\item Les taches temps réel dur. Ces dernières doivent respecter scrupuleusement un
temps maximum d'exécution, faute de quoi l'utilisabilité des résultats de la tache est nulle.
\item Les taches temps réel souple. L'utilisabilité des résultats de la tache décroît avec son retard.
\item Les taches non temps réel. Il n'est pas définit de temps maximum d'exécution. L'utilisabilité des résultats
reste constante. La Figure \ref{fig:rt_hard_soft} montre les différences entre les différents types de tache.
\end{itemize}

\begin{figure}[h]
\input{figures/real_time_lines.tex}
\caption{Les trois grandes familles de taches}
\label{fig:rt_hard_soft}
\end{figure}

\subsection{Définitions préliminaires}
\label{sec:primarydef}

\paragraph{}
Une tache temps réel $\tau_{i}$ possède trois attributs principaux:
\begin{itemize}
\item Sa durée d'exécution pire coût, ou \gls{wcet} (Worst Case Execution Time), notée $C_{i}$
\item Sa deadline, définissant l'instant auquel elle doit avoir fini de s'exécuter, notée $D_{i}$
\item Sa période, définissant sa latence minimum d'instanciation, notée $T_{i}$. Chaque instance de tache est appellé un {\it job}
\item Le temps de réponse d'un job, correspondant à la durée entre son instanciation et sa terminaison, noté $R_{i}$
\end{itemize}
\`{A} ces trois attributs, d'autres viendront s'ajouter, en fonction du type de tache.

\paragraph{}
Un ensemble de taches temps réel est appellé un ensemble de taches, ou {\it taskset}. Un ensemble de tache possède
des propriétés dérivées des taches qu'il possède. Ces dernières sont présentée dans la section \FIXME.

\subsection{Définition du WCET}

\paragraph{}
Le \gls{wcet} ({\it Worst Case Execution Time}) définit la durée d'exécution maximum d'une tache. Associée à une {\it deadline},
elle permet, pour une liste de taches donnée :
\begin{itemize}
\item de connaître à l'avance si cette liste est ordonnançable ou si certaines taches ne pourront pas
être terminées à temps.
\item de définir une répartition temporelle des taches pour respecter la {\it deadline} de chacune.
\end{itemize}

\paragraph{}
Les attributs définis dans la Section \ref{sec:primarydef} doivent avoir les propriétés définies dans les équations \ref{eqn:rtprop_1},
\ref{eqn:rtprop_2} et \ref{eqn:rtprop_3} pour que chaque tache $\tau_{i}$ soit ordonnançable.


\begin{eqnarray}
\label{eqn:rtprop_1}
\forall i, C_{i} \le D_{i}\\
\label{eqn:rtprop_2}
\forall i, R_{i} \le D_{i}\\
\label{eqn:rtprop_3}
\forall i, D_{i} \le T_{i}
%U_{i}, \displaystyle{\sum_{i \in [1, n]}} U_{i} \le 1
%\label{eqn:rtprop_3}
\end{eqnarray}

\subsection{Définition des ensembles de taches}

\paragraph{}
Pour aborder la problématique temps réel, on définit un profil d'environnement logiciel, nommé ensemble de taches ({\it
\gls{taskset}}). Cet ensemble se définit comme suit :
\begin{itemize}
\item Une liste de taches $\tau$, chaque tache possédant un {\it WCET} et une {\it deadline} qui lui est propre
\item Chaque tache se matérialise sous la forme d'une {\it instance de tache}, correspondant à un processus logiciel.
\item Un profil d'arrivée des instances de tache (arrivée périodique, sporadique ou apériodique)
\end{itemize}

\paragraph{}
On parle alors de l'ordonnançabilité d'une tache, au travers de la capacité d'exécution de toutes ses instances.\\
Un ensemble de tache est nécessairement finit, et possède des caractéristiques fixes dans le temps.

\subsubsection{Les ensembles périodiques}

\paragraph{}
Les ensembles périodiques respectent la propriété suivante :\\
Chaque tache $\tau_{i}$ est instanciée à une période fixe $P_{i}$ connue.

\paragraph{}
Ainsi, en connaissant la date de la première instanciation d'une tache, on est capable de prédire les dates d'instanciation
de cette tache durant toute la durée de vie de l'ensemble de taches.

\paragraph{}
On peut de plus mesurer la charge maximum induite par cette tache, en s'appuyant sur $WCET(\tau_{i})$ et $P_{i}$. De
plus, la variation de la charge induite par cette tache dépend uniquement du de la variation du coût d'exécution de
chaque instance de $\tau_{i}$.

\subsubsection{Les ensembles sporadiques}

\paragraph{}
Les ensemble sporadiques respectent la propriété suivante :\\
Chaque tache $\tau_{i}$ est instanciées avec une période {\it minimum} $P_{i}$ définie.

\paragraph{}
On connaît donc la charge pire cas de chaque tache,  en s'appuyant sur $WCET(\tau_{i})$ et $P_{i}$.\\
Par contre, on ne peux pas prédire les dates d'instanciation successives de cette tache. De plus,
la variation de la charge induite par cette tache dépend à la fois du coût d'exécution effectif de
chaque instance de $\tau_{i}$ et de l'élongation de la période $P_{i}$.

\subsubsection{Les ensembles apériodiques}

\paragraph{}
Les ensembles apériodique respectent la propriété suivante :\\
Aucune tache n''est soumise à une période d'instanciation minimum. Une instance peut être créée n'importe quand,
indépendament de la finition de l'instance précédente.

\paragraph{}
On ne connaît pas la charge pire cas de chaque tache, la valeur de $P_{i}$ appartenant à l'ensemble $[0, +\infty[$

\subsubsection{Les ensembles fenêtrés}

Une fenêtre glissante de temps ${\Delta}t$ est définie, et on lui associe un nombre d'interruptions
maximum. Cette valeur est utilisée en entrée de la mesure du WCET.







\section{Les domaines de sécurité}

\section{Les domaines de criticité}

\section{Qualité de service dans les réseaux IP}

\subsection{802.1p}
\subsection{802.1q}
\subsection{Diffserv et champs ToS}

\section{Les ineraction entre le matériel, la sécurité et le temps réel}

\subsection{Principes de base de la MMU}

\paragraph{}
Un applicatif, construit dans un fichier binaire lors de l'édition de lien, possède un certain nombre de sections
servant de conteneur à des données différentes, et ayant en conséquence des propriétés de sécurité (en terme d'accès)
également différente. De ce fait, lors du déploiement de ces différentes section en mémoire vive, il est nécessaire
de   considérer   pour   chacune   de   ces   section   des    propriétés    d'accès    spécifiques.

\paragraph{}
Lorsque plusieurs applicatifs s'exécutent de manière concurente, il est
important de les cloisonnner dans des environnement logiques séparés, ceci
afin d'éviter que ces derniers puissent interagir sans contrôle, par simple
accès à la mémoire physique.

\paragraph{}
Il existe plusieurs niveau d'abstraction de la mémoire physique :
\begin{itemize}
\item Sans aucune abstraction, on est en mode dit  {\it  flat  memory}.
L'ensemble  des  logiciels s'exécutant en mémoire traite des adresses physiques.
Ainsi,  ils  connaissent  leur  position  en mémoire,   et    ont    une    visibilité
sur    l'ensemble    de    la    mémoire    physique.\\
De ce fait, il leur est possible d'accéder au contenu des autres applicatifs en  cours  d'exécution.
Il devient ainsi aisé d'espionner les sections d'autres logiciels, comme les section data ou rodata,
dans  lesquelles   il   est   possible   de   récupérer   par   exemple   des   mots   de   passe.\\
Autre risque : l'accès à la pile des autres applicatifs peut permettre de dériver son exécution vers
un bloc de code spécifique, en remplaçant l'adresse de la fonction parente par une adresse  dont  on
maîtrise le contenu. Du fait que la pile est par essence accessible en écriture, tous les processus
peuvent modifier les champs d'un processus donné en mode {\it flat memory}, à partir  du  moment  où
l'adresse physique de la pile est connue.
\item Décomposition de la mémoire physique en segment.  Il s'agit du mode  de  segmentation  présent
uniquement sur les processeur de la famille Intel IA32.  Ce mode étant aujourd'hui remplacé  par  le
système de pagination, il ne sera pas décrit ici.
\item En s'appuyant sur une abstraction de la mémoire physique, appellé {\it mémoire virtuelle}. On
parle alors de mode dit de  {\it  pagination  mémoire}.   Un  processus  donné  possède  son  propre
environnement mémoire, dont la taille correspond à la valeur maximum d'un mot mémoire (e.g. dans un
processeur 32 bits, l'environnement  mémoire  virtuelle  à  une  taille  de  quatre  giga-octets).\\
Cet environnement mémoire est appellé AS (Address Space).  Le processus, lors de son  exécution,  ne
traîte alors que des adresses virtuelles, dont la  translation  en  adresses  physique  se  fait  au
travers d'un élément matériel nommé MMU.   La  conséquence  directe  est  que  le  coeur  processeur
travaille en adressage virtuel.
\end{itemize}

\paragraph{}
La gestion de la pagination est géré par le contrôleur MMU, sous les ordre du
noyau du système d'exloitation. Ce dernier construit l'association entre
mémoire physique et mémoire virtuelle dans des structures en mémoire nommées
Pages Tables. Sur les architecture x86, ces dernières sont accédées par la MMU
via un registre nommé PDBR. Ce dernier pointe vers la racine de l'arbre de
mapping de mémoire virtuelle du processus en cours, et s'appuie sur ce dernier
pour retrouver l'adresse mémoire physique associée à l'adresse mémoire
virtuelle demandée par le coeur processeur. Afin d'optimiser les accès à cet
arbre de page, la MMU s'appuie sur un cache local nommé TLB (Translation
Lookaside Buffer), réduisant ainsi les requêtes au contrôleur mémoire.

\paragraph{}
Le   principe   de   fonctionnement   de   la   pagination    fournit    plusieurs    avantages    :
\begin{itemize}
\item A chaque processus est associé un AS (espace d'adressage) qui  lui  est  propre.
Ainsi, il se voit seul en mémoire et il ne possède plus la capacité d'accéder aux autres  processus.
En effet, l'accès à la mémoire physique se fait via un accès à une adresse  virtuelle.   Pour  qu'un
processus y accède, une association entre adresse physique et adresse virtuelle, locale à cet espace
d'adressage, doit avoir été faite par le noyau en préalable, via configuration de la MMU. Lors de la
création d'un processus, seules les adresses physiques correspondant  à  ces  propres  données  sont
intégrées à l'espace d'adressage, interdisant à ce dernier  d'accéder  aux  données  de  tout  autre
processus.
\item De plus, la gestion de la fragmentation de la mémoire centrale, dépendant des  allocations  et
désallocations successives de la mémoire, est rendue opaque en mémoire virtuelle. Des espace mémoire
physiques disjoints peuvent être positionnés de manière contigüe dans l'espace
d'adressage virtuel,  faisant croire  au  processus  que  ses  données  sont  contigüe  sans  pour
autant  qu'elles  le   soient.
\item Le contrôleur MMU permet d'associer à chaque page mémoire des propriété
en terme d'accès:\\
\begin{itemize}
  \item Niveau d'exécution minimum (mode administrateur ou utilisateur)
  \item droits d'accès (lecture, écriture et/ou exécution)
\end{itemize}
Ces droits d'accès permettent, dans un espace d'adressage donnée, de limiter
les accès à chaque page au processus. Une telle protection réduit la surface
d'attaque, comme en complexifiant par exemple l'exploitation de dépassement de pile
\end{itemize}
La Figure \ref{fig:mmu} décrit le principe général de pagination entre mémoire
physique et mémoire virtuelle.

\begin{figure}
\label{fig:mmu}
\input{figures/mmu_as.tex}
\caption{Principe général du mapping de page mémoire dans les espaces
d'adressage des processus}
\end{figure}

\paragraph{}
Les attaques entre les différents processus ne sont pas les  seules  possibles.   Il  est  également
possible de corrompre le comportement d'un processus donné, pour lui faire faire un  traitement  non
prévu.\\
Une des attaques les plus classiques est l'injection  de  {\it  shellcode}.   L'attaque  consiste  à
intégrer quelque part dans l'espace d'adressage du processus  une  suite  d'instructions,  avant  de
corrompre   la   pile   pour   provoquer   un   branchement   vers   ce    bloc    d'instructions.\\
Cette attaque est donc faite en deux temps, en positionnant des instructions en  mémoire,  avant  de
forcer un branchement.  Cela implique de pouvoir écrire des données  dans  une  partie  de  l'espace
mémoire qui soit exécutable.  Cela implique de pouvoir écraser des données déjà présente  sans  pour
autant provoquer un plantage du processus.  En général, les shellcodes sont placés dans la pile,  en
amont du positionnement courant du pointeur de pile. Il convient, pour éviter ce type d'attaque, de
rendre l'espace mémoire utilisé pour la pile non-exécutable.  A l'inverse, l'espace  mémoire  où  se
situe le code du processus doit lui être exécutable, mais ne doit pas  être  écrivable,  ce  dernier
étant immuable durant l'exécution du processus.\\
On définit ici des droits d'accès à des  espace  mémoire  (lecture,  écriture,  exécution)  variants
selon le type de données qui y sont hébergé. Les MMU modernes, lors de la création d'une association
entre mémoire physique et mémoire virtuelles,  définissent  ces  trois  propriétés,  fournissant  un
durcissement   supplémentaire   pour   réduire   les   risques   de   corruption   d'un   processus.

\subsection{A propos des contrôleurs de cache}

\subsubsection{Description générale}

\paragraph{}
Le \index{Cache}cache est une mémoire statique à accès  rapide,  permettant  d'accélérer  les  accès
mémoire du CPU, en copiant les données (code et data) de manière à ce  qu'elles  soient  accessibles
directement  par  le  CPU,  sans   la   lourdeur   d'un   transfert   sur   le   bus\cite{cachalgo}.

\paragraph{}
La figure \ref{fig:cache_soa} montre une architecture mémoire générique, avec plusieurs  niveaux  de
cache, suivis d'un bus d'accès mémoire et  du  controlleur  mémoire  principale  (DDR  en  général).

\begin{figure}
\input{figures/cache_architecture.tex}
\caption{Architecture en couche générique\label{fig:cache_soa}}
\end{figure}

\paragraph{}
Il existe dans les processeurs modernes plusieurs niveaux de cache :
\begin{enumerate}
\item Cache niveau 1, le plus petit et accessible le plus rapidement.  Ce  cache  est  spécialisé  :
il définit un conteneur dit {\it cache code},
pour les éléments de code (section {\texttt .code} des binaires), et un  conteneur  dit  {\it  cache
data},   pour   le   reste   des    données    ({\texttt    .bss},    {\texttt    .rodata},    ...).
\item Cache niveau 2, de taille plus  conséquente,  mais  dont  l'accès  est  moins  rapide  que  le
cache L1. Ce cache n'est pas spécialisé.
\item Cache niveau  3,  de  taille  plus  grande  encore.   Partagé  entre  les  c{\oe}urs  sur  les
architectures multi-c{\oe}urs. Ce cache n'est pas spécialisé. Ce dernier n'est pas toujours présent.
\end{enumerate}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\textbf{Niveau de mémoire}} & {\textbf{Temps d'accès}} \\
\hline
Cache L1 & 2-8 nano secondes \\
Cache L2 (partagé) & 5-12 nano secondes \\
Mémoire centrale & 10-60 nano secondes \\
\hline
\end{tabular}
\end{center}
\label{tab:cache_access_time}
\caption{Temps d'accès mémoire sur un Intel Core 2 duo}
\end{table}

\paragraph{}
Le tableau \ref{tab:cache_access_time}\footnote{confer \url{http://expertester.wordpress.com/2008/05/30/core-2-duo-vs-pentium-dual-core/}} fournit
un exemple de comparaison  du  coût  d'accès  à   une   donnée   selon   son   positionnement   en   mémoire.

\paragraph{}
Les algorithmes de cache ont divers niveaux d'associativité :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Les algorithmes de type {\textit one-way associative} associent  une  cellule  mémoire  à  une
et une seule cellule de cache.  Il s'agit d'un algorithme surjectif, du fait de la taille du  cache.
Le choix de remplacement étant de type $1:1$, aucun algorithme spécifique n'est utilisé. Les données
sont écrites au seul emplacement possible en cache.
\item Les algorithmes de type {\textit N-way associative} et supérieur associe une cellule mémoire avec
{\it N} (resp. 2, 3, 4, ...)
cellules de cache. Cette associativité permet d'optimiser l'écrasement des cellules du cache en donnant
un choix de positionnement à la cellule mémoire à mettre en cache.  Les algorithmes de  ce  type  se
différencient par leur politique de choix de la cellule à remplacer.
\end{itemize}
On a ainsi :
\paragraph{{\it N}-Way Set Associative}
Cet algorithme permet, pour chaque élément mémoire à charger  en  cache,  de  pouvoir  se  placer  à
{\it N} endroits dans le cache.  Ces {\it N} positions sont dépendantes  de  l'adresse  physique  en
mémoire centrale de la donnée.\\
Le choix de  la  cellule  parmis  la  liste  des  cellules  admissibles  se  fait  au  travers  d'un
algorithme de gestion de cache tel que décrit plus loin.

\paragraph{Direct Mapped}
L'algorithme Direct Mapped est un algorithme simple, optimisant le coût d'exécution du choix de la cellule.
Un élément mémoire n'est placé qu'à un seul endroit dans le cache, en fonction de son adresse physique,
quelle que soit la donnée précédemment présente.


\paragraph{}
Lorsqu'une donnée n'est pas en cache, on parle de {\it cache-miss}. Le processeur va successivement
vérifier  les  caches  L1,  L2  et  L3  (siprésent),  avant   d'initier   une   recopie   à   partir
de    la    mémoire    centrale.

\paragraph{}
Il existe plusieurs algorithmes de gestion du cache, tous de type  probabiliste,  ce  qui  les  rend
malheureusement incompatibles avec les besoins du temps réel dur.\\
En conséquence, les systèmes temps réel très  contraints,  comme  dans  l'aviation  ou  le  spatial,
utilisent des architectures avec cache désactivé.

\subsubsection{Positionnement du cache}

\paragraph{Cache \index{Cache!Look-aside}look-aside}
Cette architecture place le cache et la mémoire centrale côte à côte.  Ainsi, le CPU peut accéder  à
la mémoire centrale sans passer par le cache.\\
Le principe de fonctionnement est le suivant :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le processeur démarre un cycle de lecture
\item Si le cache possède la donnée en local (cache hit), il répond au CPU, et clôture le  cycle  de
lecture sur le bus.
\item Si le cache ne possède pas la donnée (cache miss), il laisse la mémoire centrale  répondre  au
processeur,  et  espionne  les  données  transitant  sur  le  bus  afin  de  se   mettre   à   jour.
\end{itemize}

\begin{figure}[h]
\input{figures/cache_memory_bus.tex}
\caption{Comparaison   des   architectures   de   type   cache   through   et    cache    lookaside}
\label{fig:cache_look_aside}
\end{figure}


\paragraph{Cache \index{Cache!Look-through}look-through}
Cette architecture place le cache en coupure entre le processeur et la mémoire centrale. Le CPU passe
donc par le cache pour l'accès mémoire.\\
Le principe est le suivant :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le processeur démarre un cycle de lecture
\item  Si  le  cache  possède  la  donnée  en  local  (cache  hit),   il   répond   au   processeur.
\item Si le cache ne possède pas la donnée, il transfert la requête de lecture sur le bus. La mémoire
centrale renvoie la donnée, qui est mise en cache en même temps qu'elle est renvoyé  au  processeur.
\end{itemize}

\begin{figure}[h]
\input{figures/cache_look_through.tex}
\caption{Architecture de cache en mode {\it look-through}}
\label{fig:cache_look_through}
\end{figure}

Cette architecture provoque des cache miss plus long (vérification par le cache avant  de  faire  la
requête au contrôleur de bus), mais permet d'utiliser le cache indépendamment des autres processeurs
(pour un cache local).

\subsubsection{\index{Cache!Consistance}Consistance du cache}

\paragraph{}
Du fait que le cache contient des duplicatas d'une partie de la mémoire, il est important de s'assurer
que la copie  est  l'originale  restent  identique.   À  défaut,  cela  peut  entraîner  des  bogues
logiciels,  difficilement  détectables.

\paragraph{}
La consistance du cache peut être impactée de différentes manières, dont voici une liste non-exhaustive :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Mise à jour de la mémoire par un contrôleur DMA
\item  Écriture  en  mémoire  par  un  autre  processeur  (architecture   multi-CPU/culti-c{\oe}urs)
\item Modification de la donnée en cache sur un autre processeur
\end{itemize}

Il existe différentes méthodes permettant de garantir la consistance des données en caches. En voici
deux largement utilisées :

\paragraph{\index{Cache!Snooping}Snooping}
Le snooping consiste en l'espionnage du bus d'adresse, afin de détecter  une  modification  sur  une
adresse mémoire contenue en cache.\\
Il y a deux manières de réagir à une détection positive :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Prévoir un rechargement de la cellule impactée (méthode dite {\it write-back}). Le rechargement ne
se fait pas nécessairement sur le moment, mais peut être ordonnancée pour s'exécuter au moment du prochain
accès à cette cellule, ou avant, selon les implémentations.
\item Désactiver la cellule impactée.  Le prochain accès au données de cette cellule  provoquera  un
{\it    cache-miss},    suivi    d'une    recopie    à    partir    de    la    mémoire    centrale.
\end{itemize}

\paragraph{\index{Cache!Snarfing}Snarfing}
Le snarfing consiste à associer la détection d'une modification d'une donnée en cache à l'espionnage du
bus de donnée. En espionnant le bus, il est capable à tout instant de mettre à jour la cellule impactée,
et de maintenir la consistance.

\subsubsection{Synchronisation caches/mémoire centrale}

\paragraph{}
Lorsque des données sont modifiées en local sur le cache, il est nécessaire de mettre à jour la mémoire
centrale afin de prendre en compte ce changement.

\paragraph{write-back}
Le cache garde en mémoire, pour chaque cellule, sont état. Lorsque la cellule est écrasée, ou que le cache
est  vidé,  les  données  modifiées  localement  sont  synchronisées  avec  la  mémoire  centrale.\\
La mise à jour de la mémoire centrale n'est pas synchrone avec la modification locale. Cela implique, au
niveau logiciel, de prévoir les problèmes d'incohérence sur les architectures multi-c{\oe}urs, au travers
de l'usage des {\it \index{Memory barrier}memory barriers}.

\paragraph{write-through}
Une écriture sur cache implique une remontée synchrone de la mise à jour de la donnée, afin de maintenir
une cohésion.

\subsubsection{Les algorithmes de gestion de cache}

\paragraph{\index{Cache!LRU}Least Recently Used}
L'algorithme LRU s'appuie sur les dates d'accès aux items du cache. En cas de cache-miss, c'est l'élément
qui a été accédé il y a le plus longtemps qui est remplacé.\\
Pour des processus faisant des accès fréquents à un même espace mémoire, comme dans le cas de processus
calculatoires par exemple, LRU est efficace.

\paragraph{\index{Cache!MRU}Most Recently Used}
L'algorithme MRU, à l'opposé du LRU, remplace l'élément dont l'accès est le plus récent. Il considère
l'hypothèse  selon  laquelle  un  élément  accédé   très   récemment   ne   le   sera   plus   avant
une    certaine    durée.\\
MRU   est   plus   efficace   pour   des   processus   faisant   des   accès   mémoire    cycliques.

\paragraph{\index{Cache!Pseudo-LRU}Pseudo-LRU}
L'algorithme Pseudo-LRU utilise un arbre binaire représentant l'ensemble des cellules de caches pouvant
être remplacées. Chaque ensemble de cellules posséde un tag fournissant une indication sur l'emplacement
(branche droite, branche gauche) de la cellule qui sera remplacée.  Ce tag est mis à jour  à  chaque
traversée de l'arbre.

\paragraph{}
Il s'agit d'un algorithme jugé efficace en comparaison au LRU pour les caches dont l'associativité est
supérieure à 4.  Il est utilisé dans les architectures Intel 486, et dans les architecture  PowerPC,
comme le PowerPC G4 de Freescale \cite{plru}. Le schéma \ref{fig:cache_plru_e300} est tiré du manuel
de   référence    du    processeur    e300,    utilisé    dans    les    power    quicc    2    pro.

\begin{figure}
\input{figures/cache_plru_e300.tex}
\caption{Automate   à   état   de    l'algorithme    pseudo-LRU    du    Power    Quicc    2    Pro}
\label{fig:cache_plru_e300}
\end{figure}


\paragraph{\index{Cache!Segmented-LRU}Segmented LRU}
L'algorithme SLRU utilise deux segments de caches complémentaires, dont les cellules sont ordonnées de
la dernière utilisée (MRU) à la plus anciennement utilisée (LRU) :

\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le premier segment, nommé {\it probationary segment}, est celui sur lequel sont mises les données
mémoire, en cas de cache miss. C'est également le seul segment où des données peuvent être retirées
du cache.
\item Le second segment, nommé {\it protected segment}, contient la liste des cellules de cache ayant
été accédées au moins deux fois.
\end{itemize}
Le schéma \ref{fig:slru_1} montre ainsi que l'algorithme de cache fait une différence entre des données
accédées à faible fréquence et des données accédées régulièrement.

\begin{figure}
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_1.tex}
\end{minipage}\hfill
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_2.tex}
\end{minipage}\hfill
\caption{Intégration et déplacement des données accédées fréquemment}
\label{fig:slru_1}
\end{figure}


\paragraph{}
En effet, lorsqu'une cellule est accédée une seconde fois après un premier cache miss, celle-ci  est
transférée  sur  le  segment  protégé,  en  tête  de  liste  coté  MRU   (Most   Recently   Used).\\
Cela a pour conséquence, si le segment protégé est plein, de faire descendre la cellule la plus anciennement
accédée sur le segment probatoire, en tête de liste coté MRU, ceci afin de donner une deuxième chance à
toute donnée accédée au moins deux fois. Lorsque le segment probatoire est plein et que de nouvelles
données y sont intégrées, que ce soit via un déplacement d'une cellule du segment protégé ou du fait d'un
cache miss, la cellule la moins récemment utilisée et écrasée.

\begin{figure}
\input{figures/slru_3.tex}
\caption{Principe de deuxième chance du SLRU}
\end{figure}

\paragraph{}
Cet algorithme fait donc une différence entre les données accédées souvent (au moins deux fois) et les
données   accédées   rarement   (une   fois)   pendant    leur    durée    de    vie    en    cache.
Il  permet  donc  d'optimiser  la  gestion  du  cache  lors   d'accès   mémoire   peu   fréquents.\\
La  taille  des  deux   segments   est   dépendante   dynamiquement   du   schéma   d'entrée/sortie.

\paragraph{\index{Cache!LFU}Least Frequently Used}

Lors d'un cache miss, la cellule utilisée pour charger les données mémoire est celle dont le contenu est
utilisé le moins fréquemment. En cas d'égalité de fréquence, l'algorithme LRU est utilisé pour départager
les cellules.



