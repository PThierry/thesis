%%
%%
%% hardware_impacts.tex for thesis in /doctorat/these/tex
%%
%% Made by Philippe THIERRY
%% Login   <Philippe THIERRYreseau-libre.net>
%%
%% Started on  Fri Mar 12 16:36:41 2010 Philippe THIERRY
%% Last update Mon Aug 30 17:00:21 2010 Philippe THIERRY

\chapter{Notions techniques}
\label{sec:notions}
\doMinitoc

\section{Principes généraux de la sécurité}

\subsection{A propos des besoin de confidentialité, d'intégrité et de
disponibilité}

\paragraph{}
Dans le cadre du besoin sécuritaire, on considère initialement trois propriété
potentiellement séparables selon les exigences d'entrées:
\begin{enumerate}
\item La confidentialité:\\propriété assurant qu'une information n'est pas
divulguée à toute entité (utilisateur, processus, périphérique) si cette
dernière n'est pas initialement autorisé à y accéder.
\item L'intégrité:\\propriété assurant qu'une donnée n'a pas été
modifié sans autorisation préalable.
\item La disponibilité:\\propriété assurant qu'une donnée est à tout moment
accessible à toute entité ayant l'autorisation d'y accéder.
\end{enumerate}

\subsection{Fuite d'informations: Canaux cachés et canaux auxiliaires}

\paragraph{}
Assurer la confidentialité est une tâche complexe. En effet, limiter l'accès à
la donnée n'est pas suffisant pour assurer sa protection. En effet, il est
possible de récupérer des informations sur la donnée de plusieurs manières,
sans y accéder directement. On parle alors de cannal auxiliaire ou de cannal
caché, selon la méthode.

\subsubsection{Canal auxiliaire et étude comportementale}

\paragraph{}
Le principe d'un canal auxiliaire consiste à s'appuyer sur une etude
comportementale non-invasive sur des fonctions ou des traitements afin de
récupérer des informations sur les données traitées. La plupart des études sur
le sujet sont dans le domaine de la
cryptanalyse\cite{sidechancube}\cite{sidechan}. Ces
différentes études démontrent la capacité, au travers l'étude de la consommation électrique
des différents éléments d'un processeur\cite{sidechancube} ou via l'étude de
l'impact des caches processeurs sur le comportement temporel de
l'algorithmique\cite{sidechan}, de récupérer des données sur la clef
de chiffrement utilisée.\\
Le risque lié au canal auxiliaire est connu depuis longtemps et ont permis de
démontrer que des algorithmes cryptographique réputés sûr restent fragiles à
ce type d'attaque. C'est le cas par exemple d'AES\cite{aes} ou de
Serpent\cite{serpent}.

\subsubsection{Canal caché pour le transfert de données}

\paragraph{}
Le principe d'un canal caché consiste en la capacité à utiliser la bande
passante d'un canal logiciel ou matériel pour faire transiter des données
pour lesquels il n'est pas initialement prévu. L'usage d'un canal caché
implique un usage volontaire de part et d'autre pour émettre les données et
pour les recevoir.\\
Un exemple typique est de s'appuyer sur la période inter trame d'un flux
réseau donné pour fournir une information. En faisant varier la période, il
est possible d'émettre de l'information, qui peut être ainsi interprété par un
récepteur. De manière générale, les canaux cachés permettent l'émission de
données de faible taille, car leur débit est en général très faible. Ils sont
cependant tout à fait suffisant pour faire transiter des données de type
cryptographie (clefs symétrique et/ou asymétrique) ou des messages courts.

\subsection{Ce que cible la sécurité logicielle}

\paragraph{}
Dans leur globalité, les architectures de sécurité sont là pour répondre à ces
trois besoins initiaux. Selon le niveau de certifiabilité, les architectures
de sécurité peuvent prendre plusieurs formes, allant d'une séparation
matérielle complète et strict jusqu'à la simple intégration d'une politique de
gestion de contrôle d'accès (propriétaire, groupe, droit d'accès) au fichiers,
comme par exemple les systèmes DAC (Discretionary Access Control), MAC
(Mandatory Access Control) ou RBAC (Role-based Access Control).


\subsection{Cibles de sécurité et certifiabilité}

\paragraph{}
La complexité du système dépend du besoin de certifiabilité. On définit
initialement un {\it TOE} (Target Of Evaluation), définissant ce qui, dans le
système cible, doit être évalué, ainsi que le niveau de l'évaluation. Les
Critères Communs définissent une échelle dans l'évaluation sécuritaire
dénotée EAL (Evaluation Assurance Level) allant de 1 (la plus basse) à 7 (la
plus haute). Le niveau de sécurité du TOE est donc dépendant du niveau
souhaité. Les systèmes Linux bien configuré en terme de sécurité peuvent
atteindre un niveau de certification de l'ordre de EAL4. Il faut cependant
prendre soin d'étudier précisément quel est le TOE. En effet, le niveau de
sécurité d'un système certifié en étant totalement autonome n'est plus assuré
si ce dernier est connecté à un réseau. Il en va de même en cas de
modification même légère du TOE.

\section{Principes généraux des systèmes temps réel}

\subsection{Définition du temps réel}

\paragraph{}
Comme le montre le schéma \ref{fig:rt_hard_soft}, il existe trois grandes familles de taches :
\begin{itemize}
\item Les taches temps réel dur. Ces dernières doivent respecter scrupuleusement un
temps maximum d'exécution, faute de quoi l'utilisabilité des résultats de la tache est nulle.
\item Les taches temps réel souple. L'utilisabilité des résultats de la tache décroît avec son retard.
\item Les taches non temps réel. Il n'est pas définit de temps maximum d'exécution. L'utilisabilité des résultats
reste constante. La Figure \ref{fig:rt_hard_soft} montre les différences entre les différents types de tache.
\end{itemize}

\begin{figure}[h]
\input{figures/real_time_lines.tex}
\caption{Les trois grandes familles de taches}
\label{fig:rt_hard_soft}
\end{figure}

\subsection{Définitions préliminaires}
\label{sec:primarydef}

\paragraph{}
Une tache temps réel $\tau_{i}$ possède trois attributs principaux:
\begin{itemize}
\item Sa durée d'exécution pire coût, ou \gls{wcet} (Worst Case Execution Time), notée $C_{i}$
\item Sa deadline, définissant l'instant auquel elle doit avoir fini de s'exécuter, notée $D_{i}$
\item Sa période, définissant sa latence minimum d'instanciation, notée $T_{i}$. Chaque instance de tache est appellé un {\it job}
\item Le temps de réponse d'un job, correspondant à la durée entre son instanciation et sa terminaison, noté $R_{i}$
\end{itemize}
\`{A} ces trois attributs, d'autres viendront s'ajouter, en fonction du type de tache.

\paragraph{}
Un ensemble de taches temps réel est appellé un ensemble de taches, ou {\it taskset}. Un ensemble de tache possède
des propriétés dérivées des taches qu'il possède. Ces dernières sont présentée dans la section \FIXME.

\subsection{Définition du WCET}

\paragraph{}
Le \gls{wcet} ({\it Worst Case Execution Time}) définit la durée d'exécution maximum d'une tache. Associée à une {\it deadline},
elle permet, pour une liste de taches donnée :
\begin{itemize}
\item de connaître à l'avance si cette liste est ordonnançable ou si certaines taches ne pourront pas
être terminées à temps.
\item de définir une répartition temporelle des taches pour respecter la {\it deadline} de chacune.
\end{itemize}

\paragraph{}
Les attributs définis dans la Section \ref{sec:primarydef} doivent avoir les propriétés définies dans les équations \ref{eqn:rtprop_1},
\ref{eqn:rtprop_2} et \ref{eqn:rtprop_3} pour que chaque tache $\tau_{i}$ soit ordonnançable.


\begin{eqnarray}
\label{eqn:rtprop_1}
\forall i, C_{i} \le D_{i}\\
\label{eqn:rtprop_2}
\forall i, R_{i} \le D_{i}\\
\label{eqn:rtprop_3}
\forall i, D_{i} \le T_{i}
%U_{i}, \displaystyle{\sum_{i \in [1, n]}} U_{i} \le 1
%\label{eqn:rtprop_3}
\end{eqnarray}

\subsection{Définition des ensembles de taches}

\paragraph{}
Pour aborder la problématique temps réel, on définit un profil d'environnement logiciel, nommé ensemble de taches ({\it
\gls{taskset}}). Cet ensemble se définit comme suit :
\begin{itemize}
\item Une liste de taches $\tau$, chaque tache possédant un {\it WCET} et une {\it deadline} qui lui est propre
\item Chaque tache se matérialise sous la forme d'une {\it instance de tache}, correspondant à un processus logiciel.
\item Un profil d'arrivée des instances de tache (arrivée périodique, sporadique ou apériodique)
\end{itemize}

\paragraph{}
On parle alors de l'ordonnançabilité d'une tache, au travers de la capacité d'exécution de toutes ses instances.\\
Un ensemble de tache est nécessairement finit, et possède des caractéristiques fixes dans le temps.

\subsubsection{Les ensembles périodiques}

\paragraph{}
Les ensembles périodiques respectent la propriété suivante :\\
Chaque tache $\tau_{i}$ est instanciée à une période fixe $P_{i}$ connue.

\paragraph{}
Ainsi, en connaissant la date de la première instanciation d'une tache, on est capable de prédire les dates d'instanciation
de cette tache durant toute la durée de vie de l'ensemble de taches.

\paragraph{}
On peut de plus mesurer la charge maximum induite par cette tache, en s'appuyant sur $WCET(\tau_{i})$ et $P_{i}$. De
plus, la variation de la charge induite par cette tache dépend uniquement du de la variation du coût d'exécution de
chaque instance de $\tau_{i}$.

\subsubsection{Les ensembles sporadiques}

\paragraph{}
Les ensemble sporadiques respectent la propriété suivante :\\
Chaque tache $\tau_{i}$ est instanciées avec une période {\it minimum} $P_{i}$ définie.

\paragraph{}
On connaît donc la charge pire cas de chaque tache,  en s'appuyant sur $WCET(\tau_{i})$ et $P_{i}$.\\
Par contre, on ne peux pas prédire les dates d'instanciation successives de cette tache. De plus,
la variation de la charge induite par cette tache dépend à la fois du coût d'exécution effectif de
chaque instance de $\tau_{i}$ et de l'élongation de la période $P_{i}$.

\subsubsection{Les ensembles apériodiques}

\paragraph{}
Les ensembles apériodique respectent la propriété suivante :\\
Aucune tache n''est soumise à une période d'instanciation minimum. Une instance peut être créée n'importe quand,
indépendament de la finition de l'instance précédente.

\paragraph{}
On ne connaît pas la charge pire cas de chaque tache, la valeur de $P_{i}$ appartenant à l'ensemble $[0, +\infty[$

\subsubsection{Les ensembles fenêtrés}

Une fenêtre glissante de temps ${\Delta}t$ est définie, et on lui associe un nombre d'interruptions
maximum. Cette valeur est utilisée en entrée de la mesure du WCET.

\section{Les domaines de sécurité}

\paragraph{}
Un domaine de sécurité est un environnement ou un contexte incluant un
ensemble de ressources système et d'entités respectant une même politique de
sécurité.\\
Les interaction intra-domaines ne sont pas soumises à des exigences fortes de
sécurité. Cependant, il arrive qu'un même domaine de sécurité soit
physiquement séparé en plusieurs sous-systèmes qui, pour communiquer, doivent
traverser des domaines de sécurité différents. Les passerelles
d'inter-connexion doivent alors assurer le {\it noircissement} des données à
faire transiter. L'action de noircissement consiste à assurer la
confidentialité des données de diverses manière afin d'assurer que les
domaines par lesquels transitent ces données ne soient pas aptes à en tirer
une information dont le niveau de sécurité est supérieur au leur.\\
De manière générale, le noircissement implique un chiffrement des données et
potenciellement une modification du profil de flux, ceci afin de limiter la
problématique de cannal auxiliaire dérivant du profil de flux initial.

\paragraph{}


\section{Les domaines de criticité}

\section{Qualité de service dans les réseaux IP}

\subsection{802.1p}
\subsection{802.1q}
\subsection{Diffserv et champs ToS}

\section{La virtualisation}

\paragraph{}
Il existe plusieurs type de virtualisation, en fonction de l'architecture
logicielle et parfois matérielle qui entre en compte dans la solution.

\subsection{Les solution de virtualisation type I}

\paragraph{}
Les solutions de virtualisation de type I s'appuie sur un moniteur de machines
virtuelles autonome, s'exécutant directement sur le matériel. Il est seul
maître de l'environnement logiciel de la machine et assure l'ordonnancement
des compartiments. La Figure \ref{fig:vmtype1} décrit l'architecture
logicielle de ce type de solution.\\
Les interactions entre le moniteur de machines virtuelles et ces dernières
peuvent varier selon la solution utilisée pour assurer la bonne exécution de
l'environnement invité (i.e. l'environnement logiciel de la machine virtuelle)
et plus particulièrement du noyau invité.\\
Pour répondre à ce besoin, il existe trois solutions:
\begin{itemize}
  \item La virtualisation complète
  \item La paravirtualisation
  \item La virtualisation matérielle
\end{itemize}

\begin{figure}
  \label{fig:vmtype1}
  \input{figures/vmtype1.tex}
  \caption{Architecture logicielle générique des solutions de virtualisation de type I}
\end{figure}

\subsubsection{Principe de la virtualisation complète}

\paragraph{}
Historiquement la plus ancienne, elle fut formalisée en terme d'exigences par
G.J. Popek et R.P. Goldberg\cite{popek} dès 1974. Le principe est de pouvoir
exécuter un environnement logicielle dans une machine virtuelle tout en
assurant que sont comportement est identique à une exécution native.
L'environnement invité n'est pas modifié, et c'est au travers des exceptions
processeurs dûes à l'exécution en tant qu'environnement invité que le moniteur
de machines virtuelles réagit à des traitements qui ne peuvent être exécutés
directement par l'environnement invité sans impact sur la sécurité et la
sûreté du système.\\
Dans l'Article \cite{popek}, les auteurs décrivent les incompatibilité du
matériel avec ce type de principe. En effet, seules les instructions générant
des exception processeur peuvent être substituées par le moniteur de machine
virtuelles. Les
instructions sensibles ne générant pas d'exception sont une problématique
fortes car ces dernières peuvent s'exécuter sans que le moniteur de machines
virtuelles en soit informé. La conséquence peut être plus ou moins grave car
ces dernières impactent la sûreté et la sécurité du système.

\paragraph{}
Afin de pouvoir virtualiser des environnements logiciels sans l'aide du matériel
et sans modifier l'environnement à virtualiser, plusieurs solutions ont été
mises en place, entre autre par VMWare. Dans ses
solutions, ce dernier est capable de déterminer les instructions sensibles qui seront
demandées par l'environnement invité et de modifier ces dernières afin
d'injecter des traitements simulés en lieu et place. Ce type de solution est
transparent pour l'environnement invité, mais génère des variations dans le
coût effectif d'un certain nombre de traitement. Ces variations font parties
des mécanismes permettant de détecter que l'environnement d'exécution est
virtualisé.

\subsubsection{Principe de la paravirtualisation}

\paragraph{}
Afin de limiter la problématique de simulation des traitements consécutifs à
la détection d'instructions incompatibles avec la virtualisation, une solution
a été définie visant à impliquer le noyau de l'environnement invité dans le
support de la virtualisation. Le noyau est alors modifié pour substituer les
instructions sensibles par des appels volontaires à des fonctions du moniteur
de machines virtuelles. Dans ce cas, le moniteur traîte la fonction soit en
exécutant le traitement initial avec les droits qui sont les siens, soit en
simulant le traitement. On parle alors d'{\it hypercalls}.

\paragraph{}
Il existe un certain nombre de solution permettant la paravirtualisation.
C'est par exemple le cas de Xen ou encore de la solution propriétaire PikeOS
de Sysgo. La limitation du principe de virtualisation est dûe à l'éxigence
d'adaptation du noyau invité à l'API du moniteur de machine virtuelle. Cette
API n'étant pas normalisée, cela implique une implémentation souvent
spécifique à la solution de virtualisation utilisée. Cela implique également
d'avoir accès au code source du noyau invité, limitant ainsi les choix en
terme d'environnements paravirtualisables.

\subsubsection{Principe de la virtualisation matérielle}

\paragraph{}
Définie initialement par IBM, puis reprise depuis le début des années 2000 par
Intel et AMD, la virtualisation matérielle consiste à intégrer dans le
matériel des aides à la virtualisation. Le but est de limiter la complexité du
moniteur de machines virtuelles, ce dernier reposant sur des instructions
privilégiées de configuration du matériel pour gérer des environnements
matériels d'exécution des machines virtuelles séparés. Un exemple de solution
est l'architecture VT-d d'Intel, proposant des structure de configuration et
d'hébergement matérielles nommée VMCS (Virtual Machine Control Structure) qui
dupliquent les registres sensibles comme le PDBR. Lorsque le moniteur de
machines virtuelles ordonnance l'environnement virtualisé, il indique au
processeur le VMCS à utiliser en lieu et place des registres utilisés
par le moniteur lui même. La solution s'appuie également sur des élément de
filtrage d'accès au périphériques (nommés I/OMMU pour Intel, SMMU pour ARM
Cortex A15). Il n'existe à ce jour aucune norme et chaque fondeur possède
ses propres instructions.\\
Cette solution n'est pas considérée ici car incompatible des exigences de
portabilité.



\subsection{Les solution de virtualisation type II}

\paragraph{}
Les soltions de virtualisation type II s'appuie sur la présence d'un hôte qui
accueil le moniteur de machine virtuelle comme un processus. Ce dernier est
donc lui-même ordonnancé par l'hôte. Ce type de solution de virtualisation
correspond à des produits tels que VMWare Workstation ou encore VirtualBox.
La Figure \ref{fig:vmtype2} décrit l'architecture logicielle d'une telle
solution.
\begin{figure}
  \label{fig:vmtype2}
  \input{figures/vmtype2.tex}
  \caption{Architecture logicielle d'une solution de virtualisation type II}
\end{figure}

\paragraph{}
Ces solutions étant basée sur des hôtes de type GNU/Linux ou Windows dans
lesquels elles s'exécutent comme des tâches non temps réel, elles sont
donc incompatibles de besoin temps réel, y compris lorsque ces derniers sont
relativement faible. Elles sont également incompatibles des exigences de
sécurité forte, du fait de la non-certifiabilité du système hôte.

\paragraph{}
Les solutions de moniteurs de machines virtuelles utilisant ce type
d'architecture ne permettent pas d'assurer un cloisonnement spatial fort
(répartition stricte en mémoire physique) ni un cloisonnement temporel fort
(l'ordonnancement des machines virtuelles étant assujetti aux propriétés de
l'ordonnanceur de l'environnement hôte.

\subsection{Les solutions de virtualisation applicative}

\paragraph{}
Les solutions de virtualisation applicative sont parentes des solution de
virtualisation de type II, mais ne gèrent pas de noyau invité. Elles
permettent d'instancier aisément des compartiments dont le noyau est partagé
entre les compartiment, mais également avec l'hôte. La gestion de
l'ordonnancement des taches des compartiment est géré directement par
l'ordonnanceur de l'hôte, sous forme de groupes de tâches autonomes. Ce type
d'ordonnancement se rapproche ainsi du principe d'ordonnancement hiérarchique
qui sera décrit dans le Chapitre \ref{sec:hierarchique}.\\
Un exemple de solution de virtualisation applicative est la solution LXC
(LinuX Containers) de Linux. La Figure \ref{fig:vmlxc} décrit
l'architecture logicielle d'une telle solution. 

\paragraph{}
Les solution de virtualisation applicatives permettent de créer des compartiments
dont le système de fichiers, le jeu de tâches, les interfaces réseau et la
vision du stockage de masse est séparé. 

\begin{figure}
  \label{fig:vmlxc}
  \input{figures/vmlxc.tex}
  \caption{Architecture logicielle d'une solution de virtualisation applicative}
\end{figure}


\section{Les ineraction entre le matériel, la sécurité et le temps réel}

\subsection{Principes de base de la MMU}

\paragraph{}
Un applicatif, construit dans un fichier binaire lors de l'édition de lien, possède un certain nombre de sections
servant de conteneur à des données différentes, et ayant en conséquence des propriétés de sécurité (en terme d'accès)
également différente. De ce fait, lors du déploiement de ces différentes section en mémoire vive, il est nécessaire
de   considérer   pour   chacune   de   ces   section   des    propriétés    d'accès    spécifiques.

\paragraph{}
Lorsque plusieurs applicatifs s'exécutent de manière concurente, il est
important de les cloisonnner dans des environnement logiques séparés, ceci
afin d'éviter que ces derniers puissent interagir sans contrôle, par simple
accès à la mémoire physique.

\paragraph{}
Il existe plusieurs niveau d'abstraction de la mémoire physique :
\begin{itemize}
\item Sans aucune abstraction, on est en mode dit  {\it  flat  memory}.
L'ensemble  des  logiciels s'exécutant en mémoire traite des adresses physiques.
Ainsi,  ils  connaissent  leur  position  en mémoire,   et    ont    une    visibilité
sur    l'ensemble    de    la    mémoire    physique.\\
De ce fait, il leur est possible d'accéder au contenu des autres applicatifs en  cours  d'exécution.
Il devient ainsi aisé d'espionner les sections d'autres logiciels, comme les section data ou rodata,
dans  lesquelles   il   est   possible   de   récupérer   par   exemple   des   mots   de   passe.\\
Autre risque : l'accès à la pile des autres applicatifs peut permettre de dériver son exécution vers
un bloc de code spécifique, en remplaçant l'adresse de la fonction parente par une adresse  dont  on
maîtrise le contenu. Du fait que la pile est par essence accessible en écriture, tous les processus
peuvent modifier les champs d'un processus donné en mode {\it flat memory}, à partir  du  moment  où
l'adresse physique de la pile est connue.
\item Décomposition de la mémoire physique en segment.  Il s'agit du mode  de  segmentation  présent
uniquement sur les processeur de la famille Intel IA32.  Ce mode étant aujourd'hui remplacé  par  le
système de pagination, il ne sera pas décrit ici.
\item En s'appuyant sur une abstraction de la mémoire physique, appellé {\it mémoire virtuelle}. On
parle alors de mode dit de  {\it  pagination  mémoire}.   Un  processus  donné  possède  son  propre
environnement mémoire, dont la taille correspond à la valeur maximum d'un mot mémoire (e.g. dans un
processeur 32 bits, l'environnement  mémoire  virtuelle  à  une  taille  de  quatre  giga-octets).\\
Cet environnement mémoire est appellé AS (Address Space).  Le processus, lors de son  exécution,  ne
traîte alors que des adresses virtuelles, dont la  translation  en  adresses  physique  se  fait  au
travers d'un élément matériel nommé MMU.   La  conséquence  directe  est  que  le  coeur  processeur
travaille en adressage virtuel.
\end{itemize}

\paragraph{}
La gestion de la pagination est géré par le contrôleur MMU, sous les ordre du
noyau du système d'exloitation. Ce dernier construit l'association entre
mémoire physique et mémoire virtuelle dans des structures en mémoire nommées
Pages Tables. Sur les architecture x86, ces dernières sont accédées par la MMU
via un registre nommé PDBR. Ce dernier pointe vers la racine de l'arbre de
mapping de mémoire virtuelle du processus en cours, et s'appuie sur ce dernier
pour retrouver l'adresse mémoire physique associée à l'adresse mémoire
virtuelle demandée par le coeur processeur. Afin d'optimiser les accès à cet
arbre de page, la MMU s'appuie sur un cache local nommé TLB (Translation
Lookaside Buffer), réduisant ainsi les requêtes au contrôleur mémoire.

\paragraph{}
Le   principe   de   fonctionnement   de   la   pagination    fournit    plusieurs    avantages    :
\begin{itemize}
\item A chaque processus est associé un AS (espace d'adressage) qui  lui  est  propre.
Ainsi, il se voit seul en mémoire et il ne possède plus la capacité d'accéder aux autres  processus.
En effet, l'accès à la mémoire physique se fait via un accès à une adresse  virtuelle.   Pour  qu'un
processus y accède, une association entre adresse physique et adresse virtuelle, locale à cet espace
d'adressage, doit avoir été faite par le noyau en préalable, via configuration de la MMU. Lors de la
création d'un processus, seules les adresses physiques correspondant  à  ces  propres  données  sont
intégrées à l'espace d'adressage, interdisant à ce dernier  d'accéder  aux  données  de  tout  autre
processus.
\item De plus, la gestion de la fragmentation de la mémoire centrale, dépendant des  allocations  et
désallocations successives de la mémoire, est rendue opaque en mémoire virtuelle. Des espace mémoire
physiques disjoints peuvent être positionnés de manière contigüe dans l'espace
d'adressage virtuel,  faisant croire  au  processus  que  ses  données  sont  contigüe  sans  pour
autant  qu'elles  le   soient.
\item Le contrôleur MMU permet d'associer à chaque page mémoire des propriété
en terme d'accès:\\
\begin{itemize}
  \item Niveau d'exécution minimum (mode administrateur ou utilisateur)
  \item droits d'accès (lecture, écriture et/ou exécution)
\end{itemize}
Ces droits d'accès permettent, dans un espace d'adressage donnée, de limiter
les accès à chaque page au processus. Une telle protection réduit la surface
d'attaque, comme en complexifiant par exemple l'exploitation de dépassement de pile
\end{itemize}
La Figure \ref{fig:mmu} décrit le principe général de pagination entre mémoire
physique et mémoire virtuelle.

\begin{figure}
\label{fig:mmu}
\input{figures/mmu_as.tex}
\caption{Principe général du mapping de page mémoire dans les espaces
d'adressage des processus}
\end{figure}

\paragraph{}
Les attaques entre les différents processus ne sont pas les  seules  possibles.   Il  est  également
possible de corrompre le comportement d'un processus donné, pour lui faire faire un  traitement  non
prévu.\\
Une des attaques les plus classiques est l'injection  de  {\it  shellcode}.   L'attaque  consiste  à
intégrer quelque part dans l'espace d'adressage du processus  une  suite  d'instructions,  avant  de
corrompre   la   pile   pour   provoquer   un   branchement   vers   ce    bloc    d'instructions.\\
Cette attaque est donc faite en deux temps, en positionnant des instructions en  mémoire,  avant  de
forcer un branchement.  Cela implique de pouvoir écrire des données  dans  une  partie  de  l'espace
mémoire qui soit exécutable.  Cela implique de pouvoir écraser des données déjà présente  sans  pour
autant provoquer un plantage du processus.  En général, les shellcodes sont placés dans la pile,  en
amont du positionnement courant du pointeur de pile. Il convient, pour éviter ce type d'attaque, de
rendre l'espace mémoire utilisé pour la pile non-exécutable.  A l'inverse, l'espace  mémoire  où  se
situe le code du processus doit lui être exécutable, mais ne doit pas  être  écrivable,  ce  dernier
étant immuable durant l'exécution du processus.\\
On définit ici des droits d'accès à des  espace  mémoire  (lecture,  écriture,  exécution)  variants
selon le type de données qui y sont hébergé. Les MMU modernes, lors de la création d'une association
entre mémoire physique et mémoire virtuelles,  définissent  ces  trois  propriétés,  fournissant  un
durcissement   supplémentaire   pour   réduire   les   risques   de   corruption   d'un   processus.

\subsection{A propos des contrôleurs de cache}
\label{sec:notions_caches}

\subsubsection{Description générale}

\paragraph{}
Le \index{Cache}cache est une mémoire statique à accès  rapide,  permettant  d'accélérer  les  accès
mémoire du CPU, en copiant les données (code et data) de manière à ce  qu'elles  soient  accessibles
directement  par  le  CPU,  sans   la   lourdeur   d'un   transfert   sur   le   bus\cite{cachalgo}.

\paragraph{}
La figure \ref{fig:cache_soa} montre une architecture mémoire générique, avec plusieurs  niveaux  de
cache, suivis d'un bus d'accès mémoire et  du  controlleur  mémoire  principale  (DDR  en  général).

\begin{figure}
\input{figures/cache_architecture.tex}
\caption{Architecture en couche générique\label{fig:cache_soa}}
\end{figure}

\paragraph{}
Il existe dans les processeurs modernes plusieurs niveaux de cache :
\begin{enumerate}
\item Cache niveau 1, le plus petit et accessible le plus rapidement.  Ce  cache  est  spécialisé  :
il définit un conteneur dit {\it cache code},
pour les éléments de code (section {\texttt .code} des binaires), et un  conteneur  dit  {\it  cache
data},   pour   le   reste   des    données    ({\texttt    .bss},    {\texttt    .rodata},    ...).
\item Cache niveau 2, de taille plus  conséquente,  mais  dont  l'accès  est  moins  rapide  que  le
cache L1. Ce cache n'est pas spécialisé.
\item Cache niveau  3,  de  taille  plus  grande  encore.   Partagé  entre  les  c{\oe}urs  sur  les
architectures multi-c{\oe}urs. Ce cache n'est pas spécialisé. Ce dernier n'est pas toujours présent.
\end{enumerate}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\textbf{Niveau de mémoire}} & {\textbf{Temps d'accès}} \\
\hline
Cache L1 & 2-8 nano secondes \\
Cache L2 (partagé) & 5-12 nano secondes \\
Mémoire centrale & 10-60 nano secondes \\
\hline
\end{tabular}
\end{center}
\label{tab:cache_access_time}
\caption{Temps d'accès mémoire sur un Intel Core 2 duo}
\end{table}

\paragraph{}
Le tableau \ref{tab:cache_access_time}\footnote{confer \url{http://expertester.wordpress.com/2008/05/30/core-2-duo-vs-pentium-dual-core/}} fournit
un exemple de comparaison  du  coût  d'accès  à   une   donnée   selon   son   positionnement   en   mémoire.

\paragraph{}
Les algorithmes de cache ont divers niveaux d'associativité :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Les algorithmes de type {\textit one-way associative} associent  une  cellule  mémoire  à  une
et une seule cellule de cache.  Il s'agit d'un algorithme surjectif, du fait de la taille du  cache.
Le choix de remplacement étant de type $1:1$, aucun algorithme spécifique n'est utilisé. Les données
sont écrites au seul emplacement possible en cache.
\item Les algorithmes de type {\textit N-way associative} et supérieur associe une cellule mémoire avec
{\it N} (resp. 2, 4, 8, ...)
cellules de cache. Cette associativité permet d'optimiser l'écrasement des cellules du cache en donnant
un choix de positionnement à la cellule mémoire à mettre en cache.  Les algorithmes de  ce  type  se
différencient par leur politique de choix de la cellule à remplacer.
\end{itemize}
On a ainsi :
\paragraph{{\it N}-Way Set Associative}
Cet algorithme permet, pour chaque élément mémoire à charger  en  cache,  de  pouvoir  se  placer  à
{\it N} endroits dans le cache.  Ces {\it N} positions sont dépendantes  de  l'adresse  physique  en
mémoire centrale de la donnée.\\
Le choix de  la  cellule  parmis  la  liste  des  cellules  admissibles  se  fait  au  travers  d'un
algorithme de gestion de cache tel que décrit plus loin.

\paragraph{Direct Mapped}
L'algorithme Direct Mapped est un algorithme simple, optimisant le coût d'exécution du choix de la cellule.
Un élément mémoire n'est placé qu'à un seul endroit dans le cache, en fonction de son adresse physique,
quelle que soit la donnée précédemment présente.


\paragraph{}
Lorsqu'une donnée n'est pas en cache, on parle de {\it cache-miss}. Le processeur va successivement
vérifier  les  caches  L1,  L2  et  L3  (siprésent),  avant   d'initier   une   recopie   à   partir
de    la    mémoire    centrale.

\paragraph{}
Il existe plusieurs algorithmes de gestion du cache, tous de type  probabiliste,  ce  qui  les  rend
malheureusement incompatibles avec les besoins du temps réel dur.\\
En conséquence, les systèmes temps réel très  contraints,  comme  dans  l'aviation  ou  le  spatial,
utilisent des architectures avec cache désactivé.

\subsubsection{Positionnement du cache}

\paragraph{Cache \index{Cache!Look-aside}look-aside}
Cette architecture place le cache et la mémoire centrale côte à côte.  Ainsi, le CPU peut accéder  à
la mémoire centrale sans passer par le cache.\\
Le principe de fonctionnement est le suivant :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le processeur démarre un cycle de lecture
\item Si le cache possède la donnée en local (cache hit), il répond au CPU, et clôture le  cycle  de
lecture sur le bus.
\item Si le cache ne possède pas la donnée (cache miss), il laisse la mémoire centrale  répondre  au
processeur,  et  espionne  les  données  transitant  sur  le  bus  afin  de  se   mettre   à   jour.
\end{itemize}

\begin{figure}[h]
\input{figures/cache_memory_bus.tex}
\caption{Comparaison   des   architectures   de   type   cache   through   et    cache    lookaside}
\label{fig:cache_look_aside}
\end{figure}


\paragraph{Cache \index{Cache!Look-through}look-through}
Cette architecture place le cache en coupure entre le processeur et la mémoire centrale. Le CPU passe
donc par le cache pour l'accès mémoire.\\
Le principe est le suivant :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le processeur démarre un cycle de lecture
\item  Si  le  cache  possède  la  donnée  en  local  (cache  hit),   il   répond   au   processeur.
\item Si le cache ne possède pas la donnée, il transfert la requête de lecture sur le bus. La mémoire
centrale renvoie la donnée, qui est mise en cache en même temps qu'elle est renvoyé  au  processeur.
\end{itemize}

\begin{figure}[h]
\input{figures/cache_look_through.tex}
\caption{Architecture de cache en mode {\it look-through}}
\label{fig:cache_look_through}
\end{figure}

Cette architecture provoque des cache miss plus long (vérification par le cache avant  de  faire  la
requête au contrôleur de bus), mais permet d'utiliser le cache indépendamment des autres processeurs
(pour un cache local).

\subsubsection{\index{Cache!Consistance}Consistance du cache}

\paragraph{}
Du fait que le cache contient des duplicatas d'une partie de la mémoire, il est important de s'assurer
que la copie  est  l'originale  restent  identique.   À  défaut,  cela  peut  entraîner  des  bogues
logiciels,  difficilement  détectables.

\paragraph{}
La consistance du cache peut être impactée de différentes manières, dont voici une liste non-exhaustive :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Mise à jour de la mémoire par un contrôleur DMA
\item  Écriture  en  mémoire  par  un  autre  processeur  (architecture   multi-CPU/culti-c{\oe}urs)
\item Modification de la donnée en cache sur un autre processeur
\end{itemize}

Il existe différentes méthodes permettant de garantir la consistance des données en caches. En voici
deux largement utilisées :

\paragraph{\index{Cache!Snooping}Snooping}
Le snooping consiste en l'espionnage du bus d'adresse, afin de détecter  une  modification  sur  une
adresse mémoire contenue en cache.\\
Il y a deux manières de réagir à une détection positive :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Prévoir un rechargement de la cellule impactée (méthode dite {\it write-back}). Le rechargement ne
se fait pas nécessairement sur le moment, mais peut être ordonnancée pour s'exécuter au moment du prochain
accès à cette cellule, ou avant, selon les implémentations.
\item Désactiver la cellule impactée.  Le prochain accès au données de cette cellule  provoquera  un
{\it    cache-miss},    suivi    d'une    recopie    à    partir    de    la    mémoire    centrale.
\end{itemize}

\paragraph{\index{Cache!Snarfing}Snarfing}
Le snarfing consiste à associer la détection d'une modification d'une donnée en cache à l'espionnage du
bus de donnée. En espionnant le bus, il est capable à tout instant de mettre à jour la cellule impactée,
et de maintenir la consistance.

\subsubsection{Synchronisation caches/mémoire centrale}

\paragraph{}
Lorsque des données sont modifiées en local sur le cache, il est nécessaire de mettre à jour la mémoire
centrale afin de prendre en compte ce changement.

\paragraph{write-back}
Le cache garde en mémoire, pour chaque cellule, sont état. Lorsque la cellule est écrasée, ou que le cache
est  vidé,  les  données  modifiées  localement  sont  synchronisées  avec  la  mémoire  centrale.\\
La mise à jour de la mémoire centrale n'est pas synchrone avec la modification locale. Cela implique, au
niveau logiciel, de prévoir les problèmes d'incohérence sur les architectures multi-c{\oe}urs, au travers
de l'usage des {\it \index{Memory barrier}memory barriers}.

\paragraph{write-through}
Une écriture sur cache implique une remontée synchrone de la mise à jour de la donnée, afin de maintenir
une cohésion.

\subsubsection{Les algorithmes de gestion de cache}

\paragraph{\index{Cache!LRU}Least Recently Used}
L'algorithme LRU s'appuie sur les dates d'accès aux items du cache. En cas de cache-miss, c'est l'élément
qui a été accédé il y a le plus longtemps qui est remplacé.\\
Pour des processus faisant des accès fréquents à un même espace mémoire, comme dans le cas de processus
calculatoires par exemple, LRU est efficace.

\paragraph{\index{Cache!MRU}Most Recently Used}
L'algorithme MRU, à l'opposé du LRU, remplace l'élément dont l'accès est le plus récent. Il considère
l'hypothèse  selon  laquelle  un  élément  accédé   très   récemment   ne   le   sera   plus   avant
une    certaine    durée.\\
MRU   est   plus   efficace   pour   des   processus   faisant   des   accès   mémoire    cycliques.

\paragraph{\index{Cache!Pseudo-LRU}Pseudo-LRU}
L'algorithme Pseudo-LRU utilise un arbre binaire représentant l'ensemble des cellules de caches pouvant
être remplacées. Chaque ensemble de cellules posséde un tag fournissant une indication sur l'emplacement
(branche droite, branche gauche) de la cellule qui sera remplacée.  Ce tag est mis à jour  à  chaque
traversée de l'arbre.

\paragraph{}
Il s'agit d'un algorithme jugé efficace en comparaison au LRU pour les caches dont l'associativité est
supérieure à 4.  Il est utilisé dans les architectures Intel 486, et dans les architecture  PowerPC,
comme le PowerPC G4 de Freescale \cite{plru}. Le schéma \ref{fig:cache_plru_e300} est tiré du manuel
de   référence    du    processeur    e300,    utilisé    dans    les    power    quicc    2    pro.

\begin{figure}
\input{figures/cache_plru_e300.tex}
\caption{Automate   à   état   de    l'algorithme    pseudo-LRU    du    Power    Quicc    2    Pro}
\label{fig:cache_plru_e300}
\end{figure}


\paragraph{\index{Cache!Segmented-LRU}Segmented LRU}
L'algorithme SLRU utilise deux segments de caches complémentaires, dont les cellules sont ordonnées de
la dernière utilisée (MRU) à la plus anciennement utilisée (LRU) :

\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le premier segment, nommé {\it probationary segment}, est celui sur lequel sont mises les données
mémoire, en cas de cache miss. C'est également le seul segment où des données peuvent être retirées
du cache.
\item Le second segment, nommé {\it protected segment}, contient la liste des cellules de cache ayant
été accédées au moins deux fois.
\end{itemize}
Le schéma \ref{fig:slru_1} montre ainsi que l'algorithme de cache fait une différence entre des données
accédées à faible fréquence et des données accédées régulièrement.

\begin{figure}
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_1.tex}
\end{minipage}\hfill
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_2.tex}
\end{minipage}\hfill
\caption{Intégration et déplacement des données accédées fréquemment}
\label{fig:slru_1}
\end{figure}


\paragraph{}
En effet, lorsqu'une cellule est accédée une seconde fois après un premier cache miss, celle-ci  est
transférée  sur  le  segment  protégé,  en  tête  de  liste  coté  MRU   (Most   Recently   Used).\\
Cela a pour conséquence, si le segment protégé est plein, de faire descendre la cellule la plus anciennement
accédée sur le segment probatoire, en tête de liste coté MRU, ceci afin de donner une deuxième chance à
toute donnée accédée au moins deux fois. Lorsque le segment probatoire est plein et que de nouvelles
données y sont intégrées, que ce soit via un déplacement d'une cellule du segment protégé ou du fait d'un
cache miss, la cellule la moins récemment utilisée et écrasée.

\begin{figure}
\input{figures/slru_3.tex}
\caption{Principe de deuxième chance du SLRU}
\end{figure}

\paragraph{}
Cet algorithme fait donc une différence entre les données accédées souvent (au moins deux fois) et les
données   accédées   rarement   (une   fois)   pendant    leur    durée    de    vie    en    cache.
Il  permet  donc  d'optimiser  la  gestion  du  cache  lors   d'accès   mémoire   peu   fréquents.\\
La  taille  des  deux   segments   est   dépendante   dynamiquement   du   schéma   d'entrée/sortie.

\paragraph{\index{Cache!LFU}Least Frequently Used}

Lors d'un cache miss, la cellule utilisée pour charger les données mémoire est celle dont le contenu est
utilisé le moins fréquemment. En cas d'égalité de fréquence, l'algorithme LRU est utilisé pour départager
les cellules.



