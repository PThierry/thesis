%%
%%
%% hardware_security.tex for thesis in /doctorat/these/tex
%%
%% Made by Philippe THIERRY
%% Login   <Philippe.THIERRY@reseau-libre.net>
%%
%% Started on  Thu Jul 15 16:08:56 2010 Philippe THIERRY
%% Last update Wed Apr  6 10:33:12 2011 Philippe THIERRY
%%

\chapter{General principles of the security of Information \& Technologies}

{\it
This chapter quickly describes the security functions and threats considered in this document.
In the same time, hardware and software elements considered in the case of security problematic is
first explained.
}

\doMinitoc

\section{Introducing modern hardware architectures}

\subsection{First explanations}

Usual macroscopic modern hardware architectures are close to Figure \ref{fig:gal_hw_diag}.
\begin{figure}[h]
\input{figures/hw_std.tex}
\label{fig:gal_hw_diag}
\caption{Macroscopic bus-based hardware architecture}
\end{figure}

In such architecture, the communication layer is implemented through a communication bus,
controlled by a {\it bus controller}. The bus controller manages the time sharing between each
initiator (\index{GPP}\gls{gpp}, DMA controller(s)...).

\subsection{Memory controllers}

\begin{figure}[h]
\input{figures/ddr_memory_access.tex}
\caption{DDR1 memory controller internal management\label{fig:ddr_memory_access}}
\end{figure}

\subsection{Buses and interconnects}

\subsubsection{About classical buses}

\subsubsection{Interconnects and NoC}

\subsection{Interrupt Control Units}

\paragraph{}
Interruption Requests are asynchronous events generated by various hardware components in order to
report a local event, like a timer expiration (in the case of the internal clock) or a key pressed
(case of a keyboard).\\
Such event is received by the ICU (Interrupt Control Unit), which freeze the current execution on
the associated core and load the associated ISR (Interrupt Service Routine) in order to treat the
event. Hardware interrupts are asynchronous events. Some can't be predictable and generate a slow
down through the preemption of the currently executed task.

\paragraph{}
Usually, ICU are implemented through an \gls{apic}, configured by the boot-loader and the kernel.
Some of the hardware interrupts are not maskable. They are called \index{NMI}\gls{nmi} (Non Maskable
Interrupts), and react on critical events like abnormal temperature.

\paragraph{}
The number of active interrupt lines may be relatively high, generating an overhead.
Thus, modern architectures reduce the usage of hardware interrupt to the usage of polling, reducing
the number of associated context switches.

\subsection{Cache controllers}

\subsubsection{About cache management policies}

\paragraph{}
Today's systems usually use multi-level cache architecture, for better performance. The Level 1 (L1) cache is a specialized
memory, separated in a data cache memory (the one studied in this paper), and an instruction cache memory, for instruction
pre-fetch.
L2 and sometimes L3 caches are general purpose fast access memory, used as a backup memory for L1 cache.\\
Accessing the main memory is done through a memory bus. Its access depends on an external agent, which can be a {\it
bus controller} or an {\it interconnect}. This agent schedules the access to the memory bus for all the bus accessors (DMA,
processor(s), etc) to avoid any collision.
Cache access is faster than main memory access because of its proximity to the CPU core. L1 caches are accessible without
using shared bus, avoiding shared access overhead. L2 and L3 caches may be shared between cores in SMP
architectures, and may generate an extra access latency, depending on the access sharing policy.

\begin{figure}[h]
\input{figures/cache_architecture.tex}
\caption{Sample stacked memory architecture\label{fig:soa_caches}}
\end{figure}

As shown in Figure \ref{fig:cache_look_through} cache controller can be a necessary step for the core to access the memory bus
(case of cache look-through architectures. Otherwise, they can be placed as a side element (cache look-aside architectures).
In this latter case, the cache controller catches the CPU core requests to the memory bus in order to
check a possible cache-hit, as shown in Figure \ref{fig:cache_look_aside}.\\
In this paper, we only consider L1 data cache, higher level caches being deactivated.

\begin{figure}[h]
\input{figures/cache_memory_bus.tex}
\caption{Cache look-aside positioning}
\label{fig:cache_look_aside}
\end{figure}

\begin{figure}[h]
\caption{Cache look-through positioning}
\label{fig:cache_look_through}
\end{figure}

\subsubsection{L1 cache memory management implementations}

\paragraph{}
Cache algorithms have three main properties which are:
\begin{itemize}
\item the {\bf cache associativity}, defining, for each memory cache line, the number of possible
location in cache, from $1$ (direct mapped) to $N$ ($N$-way associative cache), or any (fully
associative).
\item the {\bf cache line replacement algorithm}, using several techniques\footnote{MRU: Most Recently
Used, LRU: Least Recently Used, LFU: Least Frequently Used} like MRU, LRU, Pseudo-LRU, Segmented-LRU (SLRU)
or LFU, when the cache is set-associative.
\item the {\bf cache consistency} support, which can be:
  \begin{itemize}
    \item snooping: spying the address bus, detecting modification on main memory. The cache may then deactivate the
    associated cache line, or anticipate a reload before the next cache line access
    \item snarfing: spying the address and data bus in order to maintain the cache content versus the main
    memory consistency
  \end{itemize}
\item {\bf the memory Synchronisation method}, which can be:
  \begin{itemize}
    \item write-back: waiting as long as possible before writing back the modification in the main
    memory.
    \item write-through: synchronize the cache content with the main memory at each modification,
    avoiding the problematic of multi-core caches incoherency.
  \end{itemize}
\end{itemize}

\paragraph{}
Direct-mapped caches use a predictable algorithm. For a given memory address, the caching time of the
associated memory block, using a fixed, known associated cache line is fixed as soon as the access
to the main memory is given by the memory bus. Usually, implemented algorithms are set-associative,
generating unpredictability of the memory caching due to the cache internal memory management
algorithm. Although, for such algorithms, a worst-case memory caching cost can be defined,
depending on the associativity.\\
The set-associative data caches are based on probabilistic algorithms in order to generate the best
average cache-hit ratio (e.g. (P)LRU, (S)MRU, see Section \ref{sec:cache_algo}).
\paragraph{}
Whatever the data cache algorithm is, the cache content directly depends on the overlying software
behavior. For sporadic or aperiodic task sets, when using preemptive scheduling policy, the
variation of the data cache content is very difficult not to say impossible to predict. As a
consequence, even with predictable data cache algorithms like direct mapped, the WCET computation
can't take into account a data cache usage in order to reduce the task set execution cost.\\
Moreover, probabilistic cache memory management implementations never flush the entire cache memory
without an explicit software request.
The first consequence is that a part of the data cache may be left unchanged from a job execution to
another, allowing the last one to be informed of the amount of cache consumption of the previous one,
using advanced time measurements.\\
Such covert channels are common in generic hardware implementations.

\subsubsection{Cache management algorithms}
\label{sec:cache_algo}

All this algorithms is used when the cache controller uses FIXMEFIXME

\paragraph{\index{Cache!LRU}Least Recently Used}
LRU algorithm uses cache line access time in order to chose which cache line copy should be purged
from the cache memory. When there is no more space in the cache controller memory, the least recently
used cache line copy is replaced.

\paragraph{\index{Cache!MRU}Most Recently Used}
MRU algorithm replaces the most recently acceded cache line copy when there is no more space in the
cache controller memory. In this algorithm, it is considered that a data is usually not accedded
with a high frequency. This may be a problem with counter-based software implementations.

\paragraph{\index{Cache!Pseudo-LRU}Pseudo-LRU}
Instead of reading all the copied cache lines meta-informations in order to get back the least recently
used cache line copy, the PLRU algorithm use a tree-based system which use a small tag for each
node, updated at each access. Using this tag, it is then possible to get back the leaf which should
be replaced. Such algorithm is faster than a LRU algorithm and reduce the memory needed in order to
store cache lines copy meta-data.\\
This algorithm is implemented in the Freescale PowerPC G4 e300. Figure \ref{fig:cache_plru_e300}
describes the implemented algorithm.

\begin{figure}
\input{figures/cache_plru_e300.tex}
\caption{State automaton of the Pseudo-LRU algorithm used in PPC PowerQuicc 2 Pro architectures}
\label{fig:cache_plru_e300}
\end{figure}

\paragraph{\index{Cache!Segmented-LRU}Segmented LRU}
Segmented LRU is based on the second chance principle. It is considered here that a cache line copy
which have been acceded at least two times should not be removed from the cache.\\
This algorithm uses two {\it segments}. When a cache line is acceded for the first time, it is
duplicated in the first segment, named {\it probationary segment}, replacing another cache line
copy of it if needed, using the LRU algorithm.\\
When this cache line is acceded a second time, the cache line copy is moved in the second segment,
named {\it protected segment}.
If there is no space in the protected segment, the MRU algorithm is used to select the cache line
copy to be pushed back to the probationary segment. Figure \ref{fig:slru}

\begin{figure}
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_1.tex}
\end{minipage}\hfill
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_2.tex}
\end{minipage}\hfill\\
\input{figures/slru_3.tex}
\caption{SLRU algorithm behaviour}
\label{fig:slru}
\end{figure}

\paragraph{\index{Cache!LFU}Least Frequently Used}

This algorithm is based on the access frequency in order to elect the cache line copy to be deleted.
When there is no space in the cache controller memory, the least frequently used cache line copy is
deleted.

\subsection{SoC and MPSoC embedded architectures}

\section{Introducing modern software architectures}

\subsection{Real-Time operating systems}

\FIXME{writing a correct description of real-time monitors}

\subsection{Modular and monolithic kernel architectures}

\paragraph{}
Monolithic and modular kernels implements a huge quantity of services in kernel mode, containing in
the same time memory management, scheduling or interruptions management and third elements like
drivers and network stacks.\\
Such architecture generates huge overheads, among them asyncrhonous and unmastered events, through
the driver implementations. Due to the limitation of the context switching, such architectures are
efficient, but can't be certified in term of security or real-time. This is the case of the Linux
or BSD kernels.

\subsection{Micro-kernel architecture}

\paragraph{}
Micro-kernel contains only the necessary services in order to support user tasks. Implemented
services are usually CPU and board management, memory allocators, interrupt handlers and scheduling
sub-system.\\
Such kernels can be certifiable as much for real-time needs as for security needs. Third party
drivers (like network drivers and stacks, I2C drivers, etc) are then implemented in userspace, and
executed through the control of the kernel scheduler.\\
The size of the kernel seriously reduce the kernel overhead, and permit a better measurement of the
impact of the kernel execution on the execution time of user space tasks.

\section{Memory management}

\subsection{From flat memory management to pagination}

\subsubsection{About the hardware memory managers}

\paragraph{}

\begin{figure}[h]
\input{figures/mmu_as.tex}
\caption{Translation d'adresse - vue générale}
\end{figure}

\subsubsection{Memory Management Unit}

\subsubsection{I/O controllers memory access policy}

\paragraph{}

\subsubsection{Software integration and implementations}
\paragraph{}

\section{The hardware-based side-channels and covert-channels}

\subsection{About cache controlers}

\subsection{DMA controllers and others memory accessors}

\subsection{ACPI and ICU}

\section{The operating system and kernel security problematics}

\subsection{Protecting the kernel memory}

\subsection{About the boot process}

\subsection{Reacting to a security breach}
