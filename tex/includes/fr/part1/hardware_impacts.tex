%%
%%
%% hardware_impacts.tex for thesis in /doctorat/these/tex
%%
%% Made by Philippe THIERRY
%% Login   <Philippe THIERRYreseau-libre.net>
%%
%% Started on  Fri Mar 12 16:36:41 2010 Philippe THIERRY
%% Last update Wed Mar 17 17:50:22 2010 Philippe THIERRY
%%

\chapter{IMPACT DES ÉLÉMENTS MATÉRIELS SUR LE TEMPS RÉEL}
\doMinitoc

\section{Rappels}

\subsection{Éléments matériels en jeu dans les architectures modernes}

Une architecture matérielle type est proche de la figure \ref{fig:gal_hw_diag}
\begin{figure}[h]
\input{figures/gal_hw_diag.tex}
\label{fig:gal_hw_diag}
\caption{Schéma général des composants matériels impactant le temps réel}
\end{figure}

L'élément central est le bus, dont l'accès est contrôlé par le {\it contrôleur de bus}. C'est ce dernier qui permet
la communication entre les différents organes matériel, y compris le processeur\footnote{Le terme processeur définit ici un \index{GPP}GPP (General Purpose Unit)}.

\subsection{Gestion de l'adressage}

\paragraph{}
Les processeurs modernes sont équipés d'une \index{MMU}\gls{mmu} ({\it Memory Management Unit}), voire, selon les familles de
processeurs, d'une \index{I/OMMU}I/OMMU.
Ces deux éléments jouent un rôle de filtre permettant de fournir des capacités de sécurisation des accès mémoire,
ainsi qu'un service d'abstraction de celle-ci, au travers de la pagination.

\paragraph{}
La pagination permet de fournir au logiciel une vue logique de la mémoire, permettant ainsi une gestion de l'adressage physique
plus aisée car invisible au niveau logiciel. Elle définit des {\it \index{Espace d'adressage}espaces d'adressage} pour chaque
processus, s'appuyant sur un adressage logique, lui donnant l'impression d'être seul à s'exécuter.

\begin{figure}[h]
\input{figures/mmu_as.tex}
\caption{Translation d'adresse - vue générale}
\end{figure}

\paragraph{}
À l'inverse, les éléments matériels utilisent tous un adressage physique. Ainsi, une recopie \index{DMA}DMA se fait directement vers une adresse physique
en mémoire centrale.

\paragraph{}
À ces deux organes du micro-processeur sont associés des éléments comme le \index{TLB}TLB ({\it Table Lookahead Buffer}),
permettant d'accélérer la translation d'adresse, entre logique et physique.\\
L'ensemble des accès du processeur vers la mémoire passe par la \gls{mmu}. L'ensemble des accès des contrôleurs d'I/O passe par l'\gls{iommu}.

\paragraph{}
La configuration de ces éléments matériels se fait par le logiciel. C'est le noyau qui est en charge de veiller à la gestion de la mémoire,
et donc de maintenir à jour les informations de la \gls{mmu} et de l'\gls{iommu}

\section{Les contrôleurs d'entrée/sortie}

\section{Les contrôleurs DMA}

\subsection{Principe de fonctionnement}

\subsection{Les transferts de données}

\subsubsection{Le mode {\it burst}}

\section{Les arbitreurs de bus}

\section{Les interruptions}

\section{Les processeurs}

\subsection{Les caches}

\subsubsection{Description générale}

\paragraph{}
Le \index{Cache}cache est une mémoire statique à accès rapide, permettant d'accélérer les accès mémoire du CPU, en copiant
les données (code et data) de manière à ce qu'elles soient accessibles directement par le CPU, sans la lourdeur
d'un transfer sur le bus\cite{cachalgo}.

\paragraph{}
Il existe dans les processeurs modernes plusieurs niveaux de cache :
\begin{enumerate}
\item Cache level 1, le plus petit et accessible le plus rapidement.
\item Cache level 2, de taille plus conséquente, mais dont l'accès est moins rapide que le cache L1.
\item Cache level 3, de taille plus grande encore. Partagé entre les c{\oe}urs sur les architectures multi-c{\oe}urs.
Ce dernier n'est pas toujours présent.
\end{enumerate}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\textbf{Niveau de mémoire}} & {\textbf{Temps d'accès}} \\
\hline
Cache L1 & 2-8 nano secondes \\
Cache L2 (partagé) & 5-12 nano secondes \\
Mémoire centrale & 10-60 nano secondes \\
\hline
\end{tabular}
\end{center}
\label{tab:cache_access_time}
\caption{Temps d'accès mémoire sur un Intel Core 2 duo}
\end{table}

\paragraph{}
Le tableau \ref{tab:cache_access_time}\footnote{confer \url{http://expertester.wordpress.com/2008/05/30/core-2-duo-vs-pentium-dual-core/}} fournit
une comparaison du coût d'accès à une donnée selon son positionnement en mémoire.

\paragraph{}
Les algorithmes de cache ont divers niveaux d'associativité :
\begin{itemize}
\item Les algorithmes de type {\textit one-way associative} associent une cellule mémoire à une et une seule
cellule de cache. Il s'agit d'un algorithme surjectif, du fait de la taille du cache.
\item Les algorithmes de type {\textit 2-way associative} et supérieur associe une cellule mémoire avec deux (resp. 3, 4, ...)
cellules de cache. Cette associativité permet d'optimiser l'écrasement des cellules du cache en donnant un choix de positionnement
à la cellule mémoire à mettre en cache. Les algorithmes de ce type se différencient par leur politique de choix de la cellule à remplacer.
\end{itemize}

\paragraph{}
Lorsqu'une donnée n'est pas en cache, on parle de {\it cache-miss}. Le processeur va successivement vérifier les caches L1, L2 et L3 (si
présent), avant d'initier une recopie à partir de la mémoire centrale.

\paragraph{}
Il existe plusieurs algorithmes de gestion du cache, tous de type probabiliste, ce qui les rend malheureusement incompatibles avec les
besoins du temps réel dur.\\
En conséquence, les systèmes temps réel très contraints, comme dans l'aviation ou le spacial, utilisent des architectures
avec cache désactivé.

\subsubsection{Positionnement du cache}

\paragraph{Cache \index{Cache!Look-aside}look-aside}
Cette architecture place le cache et la mémoire centrale côte à côte. Ainsi, le CPU peut accéder à la mémoire centrale sans passer par le cache.\\
Le principe de fonctionnement est le suivant :
\begin{itemize}
\item Le processeur démarre un cycle de lecture
\item Si le cache possède la donnée en local (cache hit), il répond au CPU, et cloture le cycle de lecture sur le bus.
\item Si le cache ne possède pas la donnée (cache miss), il laisse la mémoire centrale répondre au processeur, et sniffe les
données transitant sur le bus afin de se mettre à jour.
\end{itemize}

\begin{figure}[h]
\input{figures/cache_look_aside.tex}
\caption{Architecture de cache en mode {\it look-aside}}
\label{fig:cache_look_aside}
\end{figure}


\paragraph{Cache \index{Cache!Look-through}look-through}
Cette architecture place le cache en coupure entre le processeur et la mémoire centrale. Le CPU passe donc par le cache pour l'accès mémoire.\\
Le principe est le suivant :
\begin{itemize}
\item Le processeur démarre un cycle de lecture
\item Si le cache possède la donnée en local (cache hit), il répond au processeur.
\item Si le cache ne possède pas la donnée, il transfer la requête de lecture sur le bus. La mémoire centrale renvoie la donnée, qui est mise en
cache en même temps qu'elle est renvoyé au processeur.
\end{itemize}

\begin{figure}[h]
\input{figures/cache_look_through.tex}
\caption{Architecture de cache en mode {\it look-through}}
\label{fig:cache_look_through}
\end{figure}

Cette architecture provoque des cache miss plus long (vérification par le cache avant de faire la requête au contrôleur de bus), mais permet d'utiliser
le cache indépendamment des autres processeurs (pour un cache local).

\subsubsection{\index{Cache!Consistance}Consistance du cache}

\paragraph{}
Du fait que le cache contient des duplicata d'une partie de la mémoire, il est important de s'assurer que la copie est l'originale restent
identique. À défaut, cela peut entraîner des bugs logiciels, difficilement détectables.

\paragraph{}
La consistance du cache peut être impacté de différentes manières, dont voici une liste non-exhaustive :
\begin{itemize}
\item Mise à jour de la mémoire par un contrôleur DMA
\item Écriture en mémoire par un autre processeur (architecture multi-CPU/culti-c{\oe}urs)
\item Modification de la donnée en cache sur un autre processeur
\end{itemize}

Il existe différentes méthodes permettant de garantir la consistance des données en caches. En voici deux largement utilisées :

\paragraph{\index{Cache!Snooping}Snooping}
Le snooping consiste en l'espionnage du bus d'adresse, afin de détecter une modification sur une adresse mémoire contenue en cache.\\
Il y a deux manières de réagir à une détection positive :
\begin{itemize}
\item Prévoir un rechargement de la cellule impactée (méthode dite {\it write-back}). Le rechargement ne
se fait pas nécessairement sur le moment, mais peut être ordonnancée pour s'exécuter au moment du prochain
accès à cette cellule, ou avant, selon les implémentations.
\item Désactiver la cellule impactée. Le prochain accès au données de cette cellule provoquera un {\it cache-miss}, suivi d'une
recopie à partir de la mémoire centrale.
\end{itemize}

\paragraph{\index{Cache!Snarfing}Snarfing}
Le snarfing consiste à associer la détection d'une modification d'une donnée en cache à l'espionnage du bus de donnée. En espionnant
le bus, il est capable à tout instant de mettre à jour la cellule impactée, et de maintenir la consistance.

\paragraph{}
Lorsque des données sont modifiées en local sur le cache, il est nécessaire de mettre à jour la mémoire centrale afin de prendre
en compte ce changement.\\
Le cache garde en mémoire, pour chaque cellule, sont état. Lorsque la cellule est écrasée, ou que le cache est vidé, les données
modifiées localement sont synchronisées avec la mémoire centrale.\\
La mise à jour de la mémoire centrale n'est pas synchrone avec la modification locale. Cela implique, au niveau logiciel, de prévoir
les problèmes d'incohérence sur les architectures multi-c{\oe}urs, au travers de l'usage des {\it \index{Memory barrier}memory barriers}.

\subsubsection{Les algorithmes de gestion de cache}

\paragraph{Least Recently Used}
L'algorithme LRU s'appuie sur les dates d'accès aux items du cache. En cas de cache-miss, c'est l'élément qui a été accédé
il y a le plus longtemps qui est remplacé.\\
Pour des processus faisant des accès fréquents à un même espace mémoire, comme dans le cas de processus calculatoires par
exemple, LRU est efficace.

\paragraph{Most Recently Used}
L'algorithme MRU, à l'opposé du LRU, remplace l'élément dont l'accès est le plus récent. Il considère l'hypothèse selon laquelle un
élément accédé très récement ne le sera plus avant une certaine durée.\\
MRU est plus efficace pour des processus faisant des accès mémoire cycliques.

\paragraph{Pseudo-LRU}
L'algorithme Pseudo-LRU utilise un arbre binaire représentant l'ensemble des cellules de caches pouvant être remplacées. Chaque cellule possédant un
tag fournissant une indication sur l'emplacement (branche droite, branche gauche) de la cellule qui sera remplacée. Ce tag est mis à jour à
chaque traversée de l'arbre.\\
Il s'agit d'un algorithme jugé efficace en comparaison au LRU pour les caches dont l'associativité est supérieure à 4. Il est utilisé dans les
architectures Intel 486, et dans les architecture PowerPC, comme le PowerPC G4 de Freescale \cite{plru}.

\paragraph{Segmented LRU}
L'algorithme SLRU utilise deux segments de caches complémentaires, dont les cellules sont ordonnées de la dernière utilisée (MRU) à la plus anciènement utilisée (LRU) :

\begin{itemize}
\item Le premier segment, nommé {\it probationary segment}, est celui sur lequel sont mises les données
mémoire, en cas de cache miss. C'est également le seul segment où des données peuvent être retirées du cache.
\item Le second segment, nommé {\it protected segment}, contient la liste des cellules de cache ayant été accédées au moins
deux fois.
\end{itemize}
Le schéma \ref{fig:slru_1} montre ainsi que l'algorithme de cache fait une différence entre des données accédées à faible fréquence et des données accédées régulièrement.

\begin{figure}
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_1.tex}
\end{minipage}\hfill
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_2.tex}
\end{minipage}\hfill
\caption{Intégration et déplacement des données accédées féquement}
\label{fig:slru_1}
\end{figure}


\paragraph{}
En effet, lorsqu'une cellule est accédée une seconde fois après un premier cache miss, celle-ci est transférée sur le segment protégé,
en tête de liste coté MRU (Most Recently Used).\\
Cela a pour conséquence, si le segment protégé est plein, de faire descendre la cellule la plus anciènement accédée sur le segment probatoire, en tête de
liste coté MRU, ceci afin de donner une deuxième chance à toute donnée accédée au moins deux fois. Lorsque le segment probatoire est plein et que de nouvelles
données y sont intégrées, que ce soit via un déplacement d'une cellule du segment protégé ou du fait d'un cache miss, la cellule la moins récement utilisée et écrasée.

\begin{figure}
\input{figures/slru_3.tex}
\caption{Principe de deuxième chance du SLRU}
\end{figure}

\paragraph{}
Cet algorithme fait donc une diifférence entre les données accédées souvent (au moins deux fois) et les données accédées rarement (une fois) pendant leur durée de vie en cache.
Il permet donc d'optimiser la gestion du cache lors d'accès mémoire peu fréquents.\\
La taille des deux segments est dépendante dynamiquement du schéma d'entrée soortie.

\paragraph{2-Way Set Associative}

Cet algorithme permet, pour chaque élément mémoire à charger en cache, de pouvoir se placer à deux endroits dans
le cache. Ces deux positions sont dépendantes de l'adresse physique en mémoire centrale de la donnée.\\
La cellule choisie est celle étant la moins récement utilisée, cela implique, pour chaque paire de cellules du chache,
un champ indiquant laquelle est la moins récement utilisée.

\paragraph{Direct Mapped}

L'algorithme Direct Mapped est un algorithme simple, optimisant le coût d'exécution du choix de la cellule.
Un élément mémoire n'est placé qu'à un seul endroit dans le cache, en fonction de son adresse physique, quelle que soit la donnée précédement présente.

\paragraph{Least Frequently Used}

Lors d'un cache miss, la cellule utilisée pour charger les données mémoire est celle dont le contenu est utilisé le moins fréquement. En cas d'égalité de fréquence, l'algorithme LRU est utilisé pour départager les cellules.


\subsection{Les pipelines}

\subsection{L'hyperthreading}

\subsection{La prédiction de branchement}

\section{Les controlleurs mémoire}

\section{Les problématiques de partages}

\subsection{Les accès concurrents}

\subsection{Le maintien de la cohésion mémoire}

\subsection{Les architectures SMP}
