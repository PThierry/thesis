%%
%%
%% hardware_impacts.tex for thesis in /doctorat/these/tex
%%
%% Made by Philippe THIERRY
%% Login   <Philippe THIERRYreseau-libre.net>
%%
%% Started on  Fri Mar 12 16:36:41 2010 Philippe THIERRY
%% Last update Mon Aug 30 17:00:21 2010 Philippe THIERRY

\chapter{De la compatibilité d'une architecture sécuritée avec des contraintes temps-réel}
\doMinitoc

\section{Architectures compartimentées et temps réel}

\paragraph{}
Afin de repondre aux exigences sécuritaires de la solution, l'usage
d'architectures logicielles compartimentées dans le temps est nécessaire.
Cette compartimentation réduit en effet les interactions dues aux variations
de comportement (charge processeur, accès mémoire, etc) des différents
compartiment.\\
Lorsqu'il est nécessaire d'intégrer dans le même temps des exigences temps
réel sur les flux traité par la passerelle, il devient nécessaire de
considérer la bonne adéquation de la solution de compartimentation avec les
contraintes d'exécution (WCET, période, Dead-line) des services de traitement de flux.

\subsection{A propos des exigences temps réelles}

\paragraph{}
En plus des différentes exigences de sécurité décrite dans le chapitre
précédant, plusieurs exigences temps-réelles doivent être respectée dans le
cadre d'une passerelle d'interconnexion entre deux domaines dont les niveaux
de criticités sont à la fois multiples dans chacun d'entre eux et
potentiellement différents de chaque coté de la passerelle.

\begin{requirement}
Il doit être possible de faire communiquer entre-eux des domaines de criticité hétérogène\label{req:multi_crit}
\end{requirement}

\begin{requirement}
Le temps de traversée d'une passerelle entre deux domaines de criticité doit être bornée\label{req:rt}
\end{requirement}

\begin{requirement}
En fonction du besoin, il doit être possible de garantir un débit minimum de traversée entre deux domaine de criticité\label{req:debit}
\end{requirement}

\subsection{De l'ordonnancement hiérarchique à la virtualisation}
\label{sec:hierarchicalrt}


\subsection{Hiérarchie et Time Division Multiplexing}

\paragraph{}
Les architectures logicielles compartimentées sont ordonnancées à la manière
des systèmes temps réels à ordonnancement hiérarchique (\FIXME{ref}). On
définit alors un ordonnancement dit {\it global}, correspondant à l'ordonnancement des compartiments, et un ordonnancement dit {\it local}
correspondant à l'ordonnancement des tâches dans un compartiment donné.\\
A chaque compartiment est associé un {\it slot}. Ce slot est une fraction
temporelle périodique pendant laquelle le compartiment est autorisé à être
exécuté. L'exécution successive de l'ensemble des slots du systèmes forme un
motif temporel strict définissant la période TDM.

\paragraph{}
Dans le cadre de ma thèse, j'ai étudié la problématique d'ordonnancement temps
réel des fonctions logicielles de la passerelle sur une base TDM, pour les
raisons de sécurité évoquée dans le Chapitre \ref{chap:solution_secu}. Les
fonctions de traitement des différents compartiments devant respecter des
exigences de temps-réel s'appuie sur une politique d'ordonnancement de type
EDF (Earliest Deadline First). La solution s'appuie donc sur un ordonnancement
global TDM avec un ordonnancement local EDF, qui sera noté dans la suite
TDM/EDF.

\paragraph{}
Dans le cadre de cette architecture, il est cependant nécessaire de
déterminer, pour l'ensemble des compartiments ayant des tâches devant
respecter des exigences de temps réel, la période et la durée nécessaire
du slot permettant d'assurer le respect. La particularité de la solution de
compartimentation pour la sécurité sur base TDM est que l'ordonnanceur global,
géré au niveau de l'hyperviseur, n'est pas informé des propriétés temps réel
des tâches hébergées dans les compartiments. De plus, le respect du
partitionnement TDM strict interdit toute variation du motif d'ordonnancement
des compartiments. En conséquence, bien que l'ordonnancement local des tâches d'un
compartiment donné puisse être considéré en ligne (c'est le cas de la
politique d'ordonnancement EDF), l'ordonnancement global est calculé hors
ligne. C'est ce calcul qui est considéré ici.

\paragraph{}
Plusieurs variables sont introduite dans la Table \ref{tab:vars} afin de formaliser l'ordonnançabilité
d'une architecture logicelle compartimentée sur une base TDM (Time Division
Multiplexing) stricte.
\begin{table}
\label{tab:vars}
\begin{tabular}{ll}
 {\it Variables de niveau {\it global}} & {\it Variables de niveau {\it local}} \\
 \begin{minipage}{0.48\linewidth}
 \fbox{
    \begin{minipage}{0.97\linewidth}
        Considérant $m$ compartiments, on définit:\\
        $U_{i}:$ la charge associée au compartiment $C_{i}$\\
        $SC=\{SC_1,\ldots,SC_m\}$ la durée du slot $S_i$ associé à $C_{i}$\\
        $\tau=\{\tau_1,\ldots,\tau_m\}$ l'ensemble des ensembles tâches associés à $C_i$\\
        $ST_{i}$ La période du slot $S_{i}$\\
        $SC_{i}$ La durée du slot $S_i$\\
    \end{minipage}
 }
 \end{minipage}
 &
 \begin{minipage}{0.48\linewidth}
 \fbox{
    \begin{minipage}{0.97\linewidth}
        Considering $n$ tâches, on définit:\\
        $\tau_{i}$ l'ensemble de tâches du compartiment $C_{i}$, $\tau_{i} = \{ \tau_i^{1},
            \ldots, \tau_i^{n} \}$\\
            $d_{j}$ la deadline de la tâche $\tau_{i}^{j}$ de $C_{i}$\\
            $c_{j}$ le WCET de la tâche $\tau_{i}^{j}$ de $C_{i}$\\
            $t_j$ la période de la tâche $\tau_{i}^{j}$ de $C_{i}$\\
            \vspace{10.5mm}
    \end{minipage}
 }
 \end{minipage}
 \\
\end{tabular}
\caption{Définition des variables de niveau compartiment et de niveau tâches}
\end{table}


\begin{figure}{h}
\input{figures/hypervisor_global_sched.tex}
\caption{S}
\label{fig:hyp_global_sched}
\end{figure}


%-------------8<

\paragraph{}
Afin de garantir l'ordonnançabilité d'un système de compartimenté sur base
TDM, plusieurs contraintes doivent être satisfaites. Certaines sont des
contraintes {\it globales} (au niveau hyperviseur) d'autres {\it locales} (au
niveau compartiment). Ces contraintes sont définies ci-après:\\
%\noindent
\vspace{1cm}
\begin{minipage}{0.98\textwidth}
\fbox{
    \begin{minipage}{0.98\textwidth}
        %\begin{linearProg}
        %\end{linearProg}
        %\vspace{-5mm}
            \noindent \textbf{Considérant la table \ref{tab:vars}, soit le
              problème:} {\it Maximiser $min_{i=1\ldots,n} SC_i(1-U_{i})$}
            {\bf sous les contraintes:}

        \begin{tabular}{l|l}
                \noindent \textit{contraintes de niveau global} &
                \noindent \textit{contraintes de niveau local}\\
        \begin{minipage}{0.45\textwidth}
                    {\small
            \begin{equation}
                \sum_{i=1}^m U_i \leq 1
                                \label{eqn:vm_load_sum}
            \end{equation}
            \begin{equation}
                \forall i\in \{ 1, \ldots, m \}, \frac{ST}{ST_i}\in
                \mathbb{N}^*
                                \label{eqn:slots_relative}
            \end{equation}
            \begin{equation}
                \sum_{i=1}^m SC_i \leq gcd(ST_1,\ldots,ST_m)
                                \label{eqn:slots_pgcd}
            \end{equation}
                        }
        \end{minipage} &
        \begin{minipage}{0.52\textwidth}
                    {\small
            \begin{equation}
                \forall i\in \{ 1, \ldots, m \}, \forall \tau_i^j \in S_i, d_j>ST_i-SC_i
                                \label{eqn:deadline_vs_ST}
            \end{equation}
                        \begin{center}
                        $\forall k \in \mathbb{N}^*, \forall t \in [ kST_{i} - SC_{i}, kST_{i}],$
            \begin{equation}
                            h(t) \leq t - k(ST_{i} - SC_{i})
                                \label{eqn:edf_sched}
            \end{equation}
                        {\it avec} $h(t) = \sum_{j = 1}^{n} max (0, 1 + \lfloor \frac{t - d_{j}}{t_{j}} \rfloor)c_{j}$,
                        \end{center}
                        }
        \end{minipage}\\
        \end{tabular}
    \end{minipage}
}
\end{minipage}

Les contraintes globales sont les suivantes:\\
L'Equation \ref{eqn:vm_load_sum} garantie que la charge processeur est
inférieur à 1. L'Equation \ref{eqn:slots_relative} est une condition
nécessaire pour assurer l'usage de slots temporels périodiques. L'Equation
\ref{eqn:slots_pgcd} est une confition suffisante pour assurer que les slots
ne se chevauchent pas, comme définit dans \cite{schedcond}.\\
La Figure \ref{fig:hyp_global_sched} décrit un exemple de répartition
temporelle entre différents slots.\\
Il est également nécessaire de considérer une contrainte locale. L'Equation
\ref{eqn:deadline_vs_ST} est une condition nécessaire pour garantir que les
tâches des compartiments exécutés dans le cadre de slots temporels ont
toujours une deadline supérieure à la période d'inactivité associées à ce
compartiment. On s'appuie, dans le cadre d'une hiérarchie d'ordonnancement de
type TDM/EDF, sur l'Equation \ref{eqn:edf_sched}, qui est une condition
nécessaire et suffisante de faisabilité dans le cadre de l'ordonnancement EDF.
Pour démontrer cette Equation, considérons le slot $i$ et l'ensemble de tâches
exécuté localement avec une politique d'ordonnancement EDF dans ce slot.\\

\noindent
Le Slot $i$ est exécuté de manière périodique par l'hyperviseur, avec une
période de $ST_i$ unités de temps. Les tâches exécutées dans ce slots le sont
pour une durée contigüe de $SC_i$ unité de temps. On commence par déterminer
le scénario pire cas en terme de faisabilité pour cet ensemble de tâches. Ce
dernier est arrive si:
\begin{itemize}
\item Toutes les tâches sont relachées simultanément (en $t_0$)
\item Le slot $i$ a déjà été exécuté entre $t_{-SC_i}$ et $t_0$, de telle
sorte que les tâches ont raté l'échéance du slot et ne pourront être exécutées
que lors du prochain slot, soit $ST_i - SC_i$ unité de temps plus tard
\end{itemize}
Ce scénario maximise clairement à la fois la charge associée à l'ensemble de
tâches du slot et le délai avant l'exécution de cet ensemble de tâches.
Le scénario pire cas étand déterminé, il reste ensuite à définir la condition
de faisabilié d'un ensemble de tâche ordonnancé par une politique de type EDF
dans le slot $i$ dans le cadre de ce scénario.\\
Toutes les tâches relachées dans l'intervale de temps $[0, t]$ avec une
deadline absolie inférieure ou égale à $t$ sont ordonnançables et respectent
leur deadlines si et seulement si elles sont exécutées durant au pire $t -
k(ST_i - SC_i)$ unités de temps. Cela se traduit en:\\
Pour tout entier $k$ avec $k \geq 1$, pour tout $t$ dans l'intervalle $[k ST_i
- SC_i, k ST_i]$, la fonction de demande processeur au temps $t$t, $h(t)$ est
  inférieure ou égale à $t - k(ST_i - SC_i)$ (condition de l'équation
  \ref{eqn:edf_sched})

\section{Moniteurs de sécurité, fonctions temps réel et environnements non sécurisables}

\subsection{De la compatibilité des moniteurs de sécurité avec le temps réel}

\paragraph{}

\subsection{De la compatibilité des environnements non certifiables avec le temps réel}

\paragraph{}
Dans le cadre de mes travaux, j'utilise dans les compartiments non certifiable
le système d'exploitation Linux. Ce dernier fournit une grande richesse
applicative et une pile réseau très complète. Cependant, Linux n'est pas un
système d'exploitation compatible avec des exigences temps réel dur. En effet,
ce dernier possède plusieurs points problématiques. En effet, le noyau Linux
étant très riche il s'appuie:
\begin{itemize}
  \item sur des thread noyau ordonnancés avec les politiques d'ordonnancement
    temps réel de POSIX (SCHED\_FIFO et SCHED\_RR) qui intègre un mécanisme
    complexe de vieillissement. La charge associés à ces threads est
    difficilement bornable car elle correspond pour la plupart à la
    conséquences de traitements d'entrées/sorties comme la gestion du disque.
    Ces threads possèdent néanmoins une affinité CPU, ce qui évite de les
    migrer en fonction de la charge. Malheureusement, l'assignation d'un
    traitement à un coeur processeur plutôt qu'un autre n'est pas définissable
    sous forme d'une configuration, mais s'établit en fonction de la charge
    courante de chacun des coeurs.
  \item sur des fonctions logicielles qui cassent la mécanique
    d'ordonnancement via le {\it vol de cycle}. En effet, ces fonctions
    (nommées {\it softirq}) ont été intégrées sous formes de code
    complémentaires exécutés à la suite de divers services comme les appels
    systèmes ou les interruptions, afin d'avoir une réactivité moyenne très forte.
    Cependant, une telle solution ne permet pas de garantir un coût d'exécution
    pire cas pour les processus ordonnancés car leur propre exécution est impactée
    par ces fonctions. Ces dernières étant de plus en plus fréquentes avec les
    version du noyau et difficile à dimensionner, la charge associée est
    difficilement bornable.
\end{itemize}

\paragraph{}
Malgré tous ces éléments problématiques pour le temps réel, il existe
plusieurs patchs permettant de rétablir un peu de comportement temps réel dans
le système d'exploitation. Dans le cadre de ma thèse, je ne parle cependant
pas des solutions RTAI ou Xenomai, qui impliquent des exigences particulières
sur les tâches temps réel, leur interdisant l'usage des API standard Linux
sous peines de perdre leur propriétés temps réel. Dans le cadre de la
définition d'une passerelle pour du traitement réseau, il est nécessaire que
ces tâches soient aptes à traiter des flux télécom, et donc à s'interfacer
avec l'API POSIX de traitement réseau. Afin de répondre à cette problématique,
je me suis donc appuyer sur la solution Linux-RT, qui intègre les travaux du
patch PREEMPT\_RT d'Ingo Molnar\cite{koolwal2009myths}. Cette solution permet
de modifier le comportement du noyau Linux en divers points:
\begin{itemize}
  \item Les threads noyaux ne s'exécutent plus avec une haute priorité temps
    réel, supprimant la collision de ses derniers avec l'ensemble de tâches
    temps réel
  \item Les {\it softirqs} ne s'exécutent plus via du vol de cycle, mais
    exclusivement dans un thread kernel spécifique
\end{itemize}

\paragraph{}
Dans le cadre de mes travaux, j'ai dimentionner l'impact de ce patch sur la
variation de latence à l'initialisation d'un job de priorité maximum. Cette
mesure permet de déterminer si il est effectivement possible de définir une
borne supérieur à cette initialisation.\\

Pour cela, je me suis appuyé sur de
l'outillage de test\cite{abeni2002measurement} pour simuler une forte charge à
la fois en terme d'entrées/sorties, d'accès mémoire et de charge processeur.
La mesure a été faite sur le système décrit dans la Table \ref{tab:rttest}\\

\begin{table}
\label{tab:rttest}
\begin{center}
\begin{tabular}{|l|l|}
  \hline
  {\bf Élément} & {\bf Description} \\
  \hline
  \hline
  {\it Architecture} & i686 core 2 duo 2Ghz \\
  {\it NICs} & DLINK-RTL8139, VIA VT6105 \\
  {\it OS} & Debian Squeeze \\
  {\it Kernel} & linux 3.2.12-rt24 \\
         & pas de support ACPI\\
         & mode FULL\_PREEMPT\\
  {\it Securité} & patch grsecurity plus compléments specifiques\\
  & (memoire \& compartimentation) \\
  {\it Load-average} & 40 \\
  {\it outil de test} & cycletest \\
  {\it nombre d'échantillons} & $10^8$ \\
  {\it durée de la mesure} & ~5 heures \\
  \hline
\end{tabular}
\end{center}
\caption{Description de la cible pour la mesure de latence d'ordonnancement avec Linux-RT}
\end{table}

\paragraph{}
Cette mesure a permis de démontrer que pour une mesure portant sur $10^8$
échantillons pendant une durée de 5 heures, la latence pire cas à
l'ordonnancement est de $350 \mu s$, comme le montre la Figure \ref{fig:rttest}.

\begin{figure}
  \includegraphics[width=8.9cm]{figures/rttest.pdf}
  \caption{Latence d'ordonnancement pour la tâche de plus haute priorité sous
    Linux avec PREEMPT\_RT\label{fig:rttest}}
\end{figure}

%\subsection{Répartition des moniteurs en environnement multi-processeur}

%\subsubsection{Localisation des moniteurs et des compartiments par domaines de
%sécurité}

\section{Intégration du support des flux à criticité mixte}

\subsection{Rappels}

\paragraph{}
Comme vu dans la problématique, les systèmes Systroniques possèdent des
exigences impliquant de considérer des niveaux de criticités hétérogènes dans
un même ensemble de tâches. Ainsi, le WCET des tâches de fortes criticité est
estimés plusieurs fois, avec des méthodes plus ou moins pessimistes selon le
niveau de certification demandé. Plus l'autorité de certification veut
garantir le respect du WCET de la tâche, plus la valeur estimée est
pessimiste. Cela implique que lors de l'exécution du système, si une autre
tâche atteint une estimation de son WCET à un niveau d'assurance qui est
supérieur au niveau de criticité de la tâche initiale, cette dernière est
alors suspendue, les conditions nécessaires à sa faisabilité n'étant plus
respectées. Néanmoins, cette approche est clairement pessimiste, les tâches de
plus faibles criticité pouvant potentiellement continuer de s'exécuter, ou du
moins continuer leur traitements pour quelques temps sans pour autant
compromettre les contraintes temporelles des tâches de plus forte criticité.\\
En conséquence, j'applique une stratégie précédemment proposée pour les ensembles de
tâches traditionnels\cite{bougueroua_george_midonnet}, habituellement appelée
{\it analyse de sensibilité} et que je nomme dans ma thèse comme {\it marge de
  tolérance} dans le cadre des espaces de tâches à criticité mixte.
Je propose ensuite une implémentation simple de la marge de tolérance qui
s'appuie exclusivement sur les timers, que j'appelle {\it Latest Execution
  Time} (LET). Afin d'assurer une bonne continuité de l'exécution de l'espace
de tâches à criticité mixte, je montre également qu'il existe un instant dans
l'exécution de l'espace de tâche où il est possible d'exécuter à nouveau
l'intégralité des tâches de l'ensemble.

%%%%%%%%%% Model and Definitions %%%%%%%%%%
\subsection{Modèle et Définitions}\label{modelDef}
Dans le cadre de ces travaux, la cible est l'ordonnancement préemptif sur
architecture mono-c{\oe}ur. La cible de ma thèse associant des mécanismes de
compartimentations et d'affinité, on considère alors que les environnements
d'exécution nécessitant de la criticité mixte doivent être associés par
affinité CPU à un et un seul c{\oe}ur, afin de rester compatibles des
hypothèses nécessaires dans le cadre de l'algorithme LET.\\
On considère un ensemble de tâche 
$\tau = \{\tau_1, \tau_2, ..., \tau_n\}$
où le niveau de criticité maximum d'une tâche est lié à $\mathnormal{L}$.
Dans un système à criticité mixte, une tâche à criticité mixte est caractérisé
par un 5-uplet 
$\tau_i = \{R_i, T_i, D_i, \chi_i, C_i\}$ où:
%where the maximum criticality of a task is bound by
%$\mathnormal{L}$. A task in a mixed-criticality system is characterized by a 5-tuple
\begin{itemize}
	\item[$\bullet$] $R_i \in \mathbb{N}$ est la charge associée au
          premier job de la tâche $\tau_i$;
	\item[$\bullet$] $T_i \in \mathbb{N}^*$ est la période de la tâche $\tau_i$;
	\item[$\bullet$] $D_i \in \mathbb{N}^*$ est la deadline de la tâche $\tau_i$, $D_i \leq T_i$;
	\item[$\bullet$] $\chi_i \in \mathbb{N}$ est le niveau de criticité
          maximum de la tâche $\tau_i$,
          $\chi_i \leq \mathnormal{L}$;
	\item[$\bullet$] $C_i \in \mathbb{N}^{\mathnormal{L}}$ est un vecteur
          de taille $L$ de coûts d'exécution pire cas, où $C_i(\ell)$ est une
          estimation du WCET de la tâche $\tau_i$ au niveau de criticité $\ell \in [1,\mathnormal{L}]$.
\end{itemize}
We considère $C_i(\ell)$ s'accroît de manière monotone pour un accroissement
de $\ell$. Plus précisément, pour une tâche $\tau_i$:
\begin{itemize}
	\item[$\bullet$] $\forall m \in [1, \chi_i[$, $C_i(m) \leq C_i(m+1)$;
	\item[$\bullet$] $\forall m \in [\chi_i, \mathnormal{L}[$, $C_i(m) = C_i(\chi_i)$.
\end{itemize}
Cela suppose qu'aucune tâche ne s'exécute plus longtemps que son WCET pour son
niveau courant de criticité.
Le $k^\text{n}$-ème job $J_k^i$ instancié par une tâche à criticité multiple $\tau_i$
est caractérisé par un 5-uplet $\{r_k^i, d_k^i, \mathcal{X}_k^i, C_k^i,
  c_k^i\}$ où:
\begin{itemize}
	\item[$\bullet$] $r_k^i \in \mathbb{N}$ est l'instant où $J_k^i$ est
          instancié.
          Étant donné que l'on considère un ensemble de tâches sporadique, on
          a $r_k^i \geq r_{k-1}^i + T_i$, et $r_1^i = R_i$;
	\item[$\bullet$] $d_k^i \in \mathbb{N}^*$ est la deadline absolue de
          $J_k^i$. Plus précisément: $d_k^i = r_k^i + D_i$;
	\item[$\bullet$] $\mathcal{X}_k^i \in \mathbb{N}$ est la criticité de
          $J_k^i$, héritée de la tâche $\tau_i$: $\mathcal{X}_k^i = \mathcal{X}_i$;
	\item[$\bullet$] $C_k^i \in \mathbb{N}^{\mathnormal{L}}$ est la taille
          $L$ du vecteur de WCETs pour $J_k^i$, hérité de la tâche $\tau_i$: $C_k^i = C_i$;
	\item[$\bullet$] $c_k^i \in \mathbb{N}^*$ est l'exact coût d'exécution
          du job $J_k^i$. Du fait des spécifications de $\tau_i$, on peut dire
          que $c_k^i \leq C_i(\mathcal{X}_i)$, mais la valeur exacte de
          $c_k^i$ n'est connue que lorsque $J_k^i$ termine sont exécution.
\end{itemize}

\begin{definition}
Une tache $\tau_i$ de criticité minimum $\ell+1$ est sujette à un \emph{dépassement
d'exécution de niveau $\ell$}, si un job instancié par $\tau_i$ dépasse sont
coût d'exécution pire cas de niveau $\ell$.
\end{definition}

A tout moment $t$, on appelle le $j^\text{ème}$ job $J_j^i$ instancié par la
tache $\tau_i$ \emph{disponible} si $t \geq r_j^i$ et $J_j^i$ n'a pas encore
terminé son exécution. Le coût d'exécution effectif du job $J_j^i$ n'est pas
mesurable à partir des spécifications de $\tau_i$, mais ne sera connu qu'une
fois que $J_j^i$ aura terminé sont exécution.
A tout moment $t$ on appelle le \emph{scénario} $s_i^t$ de la tache $\tau_i$
comme l'ensemble des coûts d'exécution exacts $\{c_1^i, c_2^i, ..., c_k^i\}$
pour chacun des $k$ jobs instanciés par $\tau_i$ qui ont déjà terminé leur
exécution à l'instant $t$. Le scénario de l'ensemble de taches à criticité
mixte $\tau$ à l'instant $t$ est défini comme $s^t = \{s_1^t, s_2^t, ...,
  s_n^t\}$\\
Le \emph{niveau de criticité} du scénario $s^t$ est définit comme le plus
petit entier $\ell$ pour lequel, pour chaque $s_i^t \in s^t$, $c_k^i \leq C_i(\ell)\, \forall k$
Intuitivement, il représente le plus petit niveau de criticité dans lequel aucune tâche ne
dépasse son coût d'exécution pire cas de niveau de criticité correspondant.
Si cet entier $\ell$ n'existe pas, alors le scénario est dit \emph{erroné},
car au moins une tâche dépasse son coût d'exécution pire cas de son
niveau de criticité propre ($c_j^i > C_i(\chi_i)$).

\begin{definition}
Un ordonnancement pour un scénario $s^t$ de niveau de criticité $\ell$ est
faisable si tous les jobs $J_j^i \in s_i^t$, $\forall i$, avec $\chi_i \geq \ell$,
termine sont temps d'exécution $c_j^i$ entre son instanciation et sa deadline.
\end{definition}
Cette définition implique que l'ordonnancement à criticité mixte n'a pour but
que de garantir les contraintes temporelles des tâches dont le niveau de
criticité est supérieur ou égal au niveau de criticité du scénario.

\begin{definition}
Une politique d'ordonnancement en ligne est correct pour un ensemble de taches
$\tau$ si, pour tout scénario non-erroné de $\tau$, la politique génère un
ordonnancement faisable.
\end{definition}
Du fait qu'une politique d'ordonnancement en ligne ne découvre le coût
d'exécution exact des jobs qu'une fois que ces derniers ont terminé leur
exécution, le niveau de criticité du scénario ne peut être connu qu'après
coup. Les jobs instancié sont alors ordonnancés. Dès qu'un job dépasse son
coût d'exécution pire cas de niveau $\ell-1$, la criticité du scénario est
accrue au niveau $\ell$, les jobs instancié de criticité inférieure à $\ell$
sont abandonnés, et les requêtes futures des taches de criticité inférieure à
$\ell$ ne sont plus considérées.

\begin{definition}
Un ensemble de tâche de criticité mixte $\tau$ est MC-ordonnançable si il
admet au moins une politique d'ordonnancement en ligne correcte.
\end{definition}

La Figure \ref{extendedModel} étend le model traditionnel des états des taches
\cite{osekvdx} avec des transitions complémentaires, afin de prendre en
considération le mécanisme de suspension de tache.

\begin{figure}[h!]
	\begin{center}
		\begin{tt}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
				            thick,every label/.style={draw,black}]
		\tikzstyle{every state}=[fill=white,rectangle,draw=black,text=black, rounded corners = 3]
		\tikzstyle{trait}=[rectangle,fill=white,inner sep=0pt,rounded corners = 10]
		\tikzstyle{traite}=[ellipse,fill=white,inner sep=0pt,rounded corners = 10]
		  
		\node[state]		(0)		at (10,2.5)				{Exécuté};
		\node[state]		(1)		at (10,-2.5)				{Prêt};
		\node[state]		(2)		at (14,0)				{Suspendu};
		
		\path 	(0)	edge[bend left]			node[below]				{}	(1)
				(0)	edge[]					node[above,sloped]		{}	(2)
				(1)	edge[bend left]			node[below]				{}	(0)
				(2)	edge[]					node[below]				{}	(1)
				(1)	edge[in=290,out=340]	node[above, sloped]		{}	(2)
				(2)	edge[loop right]		node[below]				{}	(2);
				
		
		\node[trait]		(3)		at (16.1,0.85)					{	\begin{tabular}{c}
									{\footnotesize Active }\\
									{\footnotesize $[\ell > \chi_i]$}
								\end{tabular}
							};
		\node[traite]		(3)		at (13,-2.5)					{\footnotesize Abandonne};
		
		\node[trait]		(3)		at (12.1,-1.2)					{	\begin{tabular}{c}
									{\footnotesize Active }\\
									{\footnotesize $[\ell \leq \chi_i]$}
								\end{tabular}
							};
		\node[traite]		(3)		at (12,1.25)					{\footnotesize Termine};
		\node[traite]		(4)		at (11,0)						{\footnotesize Préempte};
		\node[traite]		(5)		at (9,0)						{\footnotesize Démarre};
		
		%\node[traite]		(6)		at (11.5,4)					{\footnotesize [Condition]};
		%\node[traite]		(7)		at (13.5,4)					{\footnotesize $\langle$Event$\rangle$};

		  
		\end{tikzpicture}
		\end{tt}
	\end{center}
\caption{Modèle étendu des états de taches}\label{extendedModel}
\end{figure}


Une tache ne peut être dans un des états suivants:
\begin{itemize}
	\item[$\bullet$] Exécutée: une tache $\tau_i$ est exécutée lorsque
          l'ordonnanceur choisi $\tau_i$ pour l'exécuter. Dans cet état, les
          instructions de $\tau_i$ sont traités par le CPU. A tout moment, au
          plus une tache est dans un état {\it Exécutée}.
	\item[$\bullet$] Prête: une tache $\tau_i$ atteint l'état prêt si tous
          les prérequis fonctionnels pour une transition vers l'état {\it
            Exécutée} sont présents, et $\tau_i$ est en attente uniquement
          d'une élection de la part de l'ordonnanceur.
	\item[$\bullet$] Suspendue: une tache $\tau_i$ atteint l'état suspendu
          si elle n'est actuellement pas candidate à une élection par
          l'ordonnanceur.
\end{itemize}
Les transition ont les signification suivantes:

\begin{itemize}
	\item[$\bullet$] Démarre: La tache est sélectionnée par l'ordonnanceur
          pour être exécutée.
	\item[$\bullet$] Préempte: La tache est préemptée par une autre tache.
	\item[$\bullet$] Termine: La tache à compléter correctement sont
          exécution.
	\item[$\bullet$] Abandonne: La criticité du système atteint un niveau
          supérieur à celui de la tache, qui est en conséquence abandonnée.
          Cela n'arrive que lorsque $\tau_i$ est préemptée par une tache de
          priorité et de criticité supérieure.
	\item[$\bullet$] Active [$\ell > \chi_i$]: La tache souhaite instancié
          un nouveau job, mais le niveau courant de criticité du système est
          supérieur au niveau de criticité de la tache.
	\item[$\bullet$] Active [$\ell \leq \chi_i$]: La tache souhaite instancié
          un nouveau job, et le niveau de criticité du système est inférieur
          ou égal à celui de la tache.
\end{itemize}

\begin{exmpl}\label{ex:mcTaskSet1}
Considérons un ensemble de taches à triple criticité ($\mathnormal{L} = 3$) 
$\tau = \{\tau_1, \tau_2, \tau_3\}$, avec:
  
\begin{itemize}
	\item[$\bullet$] $\tau_1: \{0, 5, 5, 1, (1,1,1)\}$;
	\item[$\bullet$] $\tau_2: \{0, 7, 7, 2, (1,3,3)\}$;
	\item[$\bullet$] $\tau_3: \{0, 8, 8, 3, (1,3,7)\}$.\\
\end{itemize}
Tout scénario pour lesquels aucun job ne dépasse un coût d'exécution de 1 est
de criticité 1, tandis que tout scénarios pour lequel les taches $\tau_2$ et
$\tau_3$ s'exécutent pour au moins 2 unités de temps et au plus 3 unités de
temps est de criticité 2. Si tout job de la tache $\tau_3$ s'exécute entre 4
et 7 unités de temps, alors cela définit un scénario de criticité 3. Pour
finir, tout autre scénario est erroné.
\end{exmpl}

\begin{definition}
Le coût d'exécution pire cas $r_i$ de la tache $\tau_i$ est la durée maximum
pour exécuter la tache $\tau_i$, en prenant en compte l'exécution de toutes
les taches de priorités supérieures à $\tau_i$. On a $r_i \geq C_i(\chi_i)$.
\end{definition}

\begin{definition}
  Un \emph{instant critique} pour une tache $\tau_i$ est défini comme étant un
  instant où $\tau_i$ instancie un job dont le temps de réponse sera le plus
  grand parmis tous les jobs instanciés par $\tau_i$.
\end{definition}

Liu \cite{Liu2000} a montré que, lorsque l'on considère un ensemble de taches
traditionnel ordonnancé en priorité fixe (FP), un instant critique pour une
tache $\tau_i$ se présente à chaque fois que $\tau_i$ instancie un job de
manière simultanée avec toutes les taches $tau_j$ de priorité supérieure.
Comme montré dans le théorème \ref{theo:criticalInstant}, ce principe peut
être étendu aux ensemble de taches à criticté mixte.

\begin{theorem}\label{theo:criticalInstant}
Soit $\tau = \{\tau_1, \tau_2, ..., \tau_n\}$ un ensemble de taches
sporadiques à criticité mixte ordonnancé avec une politique d'ordonnancement à
priorités fixes. L'instant critique d'une tache $\tau_i$ se présente dès lors
que $\tau_i$ instancie un job simultanément avec toutes les taches $tau_j$ de
priorité supérieure.
\end{theorem}
\begin{proof}
Sachant que chaque tache $\tau_i$ est certifiée pour un niveau d'assurance qui
est égal à son niveau de criticité $\mathcal{X}_i$, on peut associer chaque
tache $\tau_j \in \tau$ à une tache traditionnelle $\tau_j'$, pour laquelle le
coût d'exécution pire cas de $\tau_j'$ est égal à $C_j(\mathcal{X}_j)$. En
revenant à un ensemble de taches traditionnel, les résultats démontrés par
Liu \cite{Liu2000} sont réutilisable, ce qui prouve notre théorème.
\end{proof}

\subsection{Determinaison de la MC-ordonnançabilité}\label{sufficientCondition}

\paragraph{}
Baruah et al. \cite{bbalmms} ont démontré que le problème de
MC-ordonnançabilité, appliqué à un ensemble fini de jobs, est ${NP}$-complet.
Plutôt que de s'appuyer sur les conditions exactes de la MC-ordonnançabilité,
on cible une condition \emph{suffisante} qui peut être vérifiée dans un temps
polynomial.\\
L'algorithme utilisé pour vérifier cette condition, introduit par Vestal \cite{Vestal2007},
détermine hors ligne un ordre total des tâches dans $\tau$. Chaque tache est
assignée à une priorité distincte dont ses jobs héritent. L'assignation des
priorité est faite en utilisé l'approche d'Audsley \cite{Audsley_1991}, basée
sur la définition suivante (la priorité $n$ est la plus basse priorité, la
priorité 1 la plus élevée).

\begin{definition}
  Une tache $\tau_i$ dans un ensemble de taches à criticité mixte $\tau$ est
  dite \emph{viable au plus bas niveau de priorité} si toutes les conditions
  suivantes sont satisfaites:
	\begin{enumerate}
		\item la plus basse priorité est assignée à $\tau_i$;
		\item à toutes les autres taches de $\tau$ peut être assignée
                  n'importe quelle priorité, du moment que cette priorité est
                  supérieure à celle assignée à $\tau_i$;
		\item tout job instancié par $\tau_i$ respecte sa deadline
                  lorsqu'il est exécuté pour au plus $C_i(\chi_i)$ unité de
                  temps et toutes les autres taches $\tau_j \in \tau$
                  instancient des jobs qui s'exécutent pour au plus
                  $C_j(\chi_i)$ unités de temps.
	\end{enumerate}
\end{definition}

%\begin{algorithm}[H]
\begin{algorithm}
%\SetLine
\KwIn{$\tau$}
$\mathsf{pr} \leftarrow |\tau|$ \\
\While{$\tau \neq \varnothing$}{
	\If{aucune tache $\tau_i \in \tau$ n'est viable à la priorité la plus basse}{
		\textbf{return} erreur\;
	}
	\Else{
		Soit $\tau_i$ une tache viable à la priorité la plus basse\;
		assigne $\tau_i$ la priorité la plus basse $\mathsf{pr}$\;
		$\tau \leftarrow \tau\setminus\{\tau_i\}$\;
		$\mathsf{pr} \leftarrow \mathsf{pr}-1$\;
	}
}
\caption{Assignation de priorité}\label{alg:priorityAssignment}
\end{algorithm}

\paragraph{}
L'ordre de priorité est alors construit de manière itérative en utilisant
l'algorithme \ref{alg:priorityAssignment}.
Si un tel ordre de priorité existe, i.e. si une tache viable à la priorité la
plus basse est trouvé à chaque étape (de l'ensemble restant de taches de
priorité non assignée), alors chaque tache $\tau_i$ est garantie de respecter
sa deadline si elle s'exécute pour au plus $C_i(\chi_i)$ unités de temps et
qu'aucune tache $\tau_j$ de priorité supérieure à $\tau_i$ s'exécute pendant
plus de $C_j(\chi_i)$ unité de temps.

\paragraph{}
Parce que la priorité d'une tache est basée sur son niveau de criticité, on
dit qu'un ensemble de taches \emph{ordonnançables selon une priorité basée sur
  leur propre criticité} (Own Criticality Based Priority (ou
OCPB)-ordonnançable) si on peut trouvé un ordre complet des taches en
utilisant l'algorithme \ref{alg:priorityAssignment}.

\begin{theorem}\label{theo:mcschedulability}
Si un ensemble de taches à criticité mixte $\tau$ est OCBP-ordonnançable sur
un processeur donné, alors $\tau$ est MC-ordonnançable sur ce même processeur.
\end{theorem}
\begin{proof}
Cette preuve est principalement inspirée par celle présenté par Baruah et al. \cite{baruah_li_stougie}.
Soit $\tau$ OCBP-ordonnançable, et soit, après renommage des taches, $\tau_1 \vartriangleright
\tau_2 \vartriangleright ... \vartriangleright \tau_n$ représentant un ordre
complet des taches de l'ensemble. Soit $\tau_i$ une tache dans cet ordre de
priorité. Afin de démontrer la MC-ordonnançabilité, on doit pouver que chaque
job instancié par la tache $\tau_i$ peut avoir $C_i(\chi_i)$ unités de temps
d'exécution entre son instanciation et sa deadline dans tout scénarion de
niveau de criticité $\chi_i$ ou inférieur. Mais dans n'importe lequel de ces
scénarios, chaque job instancié par la tache $\tau_j$ s'exécute pendant pas
plus de $C_j(\chi_i)$ unités de temps. L'OCBP-ordonnançabilité de $\tau$ avec
un ordre de priorité $\tau_1 \vartriangleright \tau_2 \vartriangleright ...
\vartriangleright \tau_n$ implique que chaque job instancié par la tache
$\tau_i$ aura $C_i(\chi_i)$ unités de temps d'exécution si aucun job instancié par une tache 
 $\tau_j \in \{\tau_1, \tau_2, ..., \tau_{i-1}\}$ s'exécute plus de
 $C_j(\chi_i)$ unités de temps. En conséquence, $\tau_i$ respecte
 effectivement sa deadline dans tout scénario de criticité $chi_i$ ou
 inférieure.
\end{proof}

\paragraph{}
\begin{exmpl}\label{ex:mcTaskSet2}
Considérons de nouveau l'ensemble de tache $\tau$ présenté dans l'exemple
\ref{ex:mcTaskSet1}. Une assignation de priorité qui rend $\tau$ ordonnançable
est $\tau_3 \vartriangleright \tau_2 \vartriangleright \tau_1$.
Dans ce cas, $\tau$ étant un ensemble de taches périodique à instanciation
simultanées, $t=0$ est un instant critique pour toutes les taches, ce qui
implique qu'il est nécessaire de considérer uniquement le temps de réponse du
premier job de chaque tache pour déterminer l'ordonnançabilité. A La tache
$\tau_1$ peut être assignée la plus basse priorité car son temps de réponse ne
dépasse pas $3 < D_1$ unités de temps quand aucune des autres taches ne
dépasse son coût d'exécution pire cas de niveau de criticité 1. A La tache
$\tau_2$ peut être assignée la priorité moyenne parce que son temps de rémonse
ne dépasse pas $6 < D_2$ unités de temps quand la tache $\tau_3$ ne dépasse
pas son coût d'exécution pire cas de niveau de criticité 2. Pour finir, à la
tache $\tau_3$ peut être assignée la plus haute priorité car son temps de
rémonse ne dépasse pas $7 < D_3$ unités de temps. $\tau$ étant
OCBP-ordonnançable, on peut alors dire, selon le théorème
\ref{theo:mcschedulability}, que $\tau$ est également MC-ordonnançable.
\end{exmpl}

\paragraph{}
Dans la section suivante, on considère les ensembles de taches
OCBP-ordonnançable. De plus, dans la Section \ref{sec:allowanceFP}, les
priorités fixes sont considérées comme assignées selon l'algorithme 
\ref{alg:priorityAssignment}.

%%%%%%%%%% delayingCriticalityRising %%%%%%%%%%
\subsection{Retarder l'accroissement de criticité}\label{delayingCriticalityRising}

%%%%%%%%%
Traditional mixed-criticality scheduling policies stop a job as soon as the criticality 
of the scenario becomes higher than the criticality level of that job. In particular, as
soon as the criticality level of the scenario reaches $\ell$, all tasks $\tau_i$ with
$\chi_i < \ell$ are suspended, even though they could still run for some time without
compromising the temporal constraints of the active jobs of criticality at least $\ell$.
This is because the OCBP-schedulability condition presented in Section~\ref{sufficientCondition}
can no longer guarantee their temporal constraints. \\
We argue that this principle is too restrictive and can be relaxed under certain conditions.
In the case of task sets that are OCBP-schedulable, the following lemma already allows us to
reduce the set of tasks that need to be suspended.

\begin{lemma}\label{lem1}
Let $\tau$ be a mixed-criticality task set which is OCBP-schedulable, and $\tau_i$ be any
task in the priority order constructed by algorithm \ref{alg:priorityAssignment}. When a job
released by $\tau_i$ reaches its worst-case execution time at criticality level
$\ell \leq \mathcal{X}_i$, it is never necessary to suspend the tasks
$\tau_j$ with $\chi_j < \ell$ that have a higher priority than $\tau_i$.
\end{lemma}
\begin{proof}
Let $\tau_i \in \tau$ be a task that reaches its worst-case execution time at criticality
level $\ell$. Since $\tau$ is OCBP-schedulable, each job released by $\tau_i$ can execute
up to $C_i(\chi_i)$ time units and still meet its deadline if no higher priority task
$\tau_j$ executes longer than $C_j(\chi_i)$. Let $\tau_p$ be a higher priority task, whose
criticality $\chi_p$ is lower than $\chi_i$. Since $\chi_i > \chi_p$, $C_p(\chi_i) = C_p(\chi_p)$,
and the OCBP-schedulability condition guarantees that each job released by $\tau_i$ will meet
its deadline, even though lower criticality tasks that have a higher priority than $\tau_i$
execute for their worst-case execution time at their own criticality level. Therefore, only
the lower criticality tasks that have a lower priority than $\tau_i$ need to be suspended. \\
\end{proof}

Lemma \ref{lem1} prevents us from aborting lower criticality tasks that would still have enough
time to fully complete their execution, but we could still go further and allow lower criticality
tasks that would normally have to be suspended immediately, to proceed with their execution as
long as all the jobs are able to meet their deadlines.  This would require to determine for how
long tasks $\tau_i$ of criticality at least $\ell+1$ could delay the suspension of lower criticality
tasks in case of a level-$\ell$ execution overrun. We call this duration the $\ell$-\emph{allowance}
of task $\tau_i$, which is denoted by $A_i(\ell)$. Based on this allowance, we can determine the
latest instant at which criticality has to be raised, since there is no more hope that all lower
criticality tasks meet their deadline after. \\

%%%%%%%%%% Allowance %%%%%%%%%%
\subsection{Allowance}\label{sec:allowance}
The $\ell$-allowance of a task $\tau_i$, the criticality of which is at least $\ell+1$, consists
in computing the margin in its worst-case execution time of level $\ell$. It represents the maximum
execution time that can be added to $C_i(\ell)$ such that lower criticality tasks $\tau_j$ can
still meet their deadline if they execute up to $C_j(\chi_j)$ time units and $\tau_i$ executes
for at most $C_i(\ell) + A_i(\ell)$ time units. The $\ell$-allowance of tasks of which the 
criticality is less than or equal to $\ell$ will be set to zero.

%%%%%%% Concepts and notations %%%%%%%
\subsubsection{Concepts and Notations}
Throughout this section, the following concepts and notations are used:
\begin{itemize}
	\item[$\bullet$] $U_{\ell} = \sum\limits_{\tau_p \, | \, \chi_p \geq \ell} \dfrac{C_p(\ell)}{T_p}$
          is the processor utilization factor at criticality level $\ell$.
	\item[$\bullet$] FP$_\text{MC}$ denotes the preemptive \emph{Fixed Priority Highest
            Priority First} algorithm, with a priority assignment determined by algorithm
          \ref{alg:priorityAssignment}, and which only schedules tasks of criticality greater
          than or equal to the criticality of the scenario.
	\item[$\bullet$] A fault model denoted $\dfrac{k}{W}$. This means that for each 
          criticality level $\ell \in [0, L]$, at most $k$ tasks ($0\leq k \leq n$) will be
          subject to a level-$\ell$ execution overrun, over a sliding window of size $W$. We
          set $W\leq\min\limits_i(T_i)$ to prevent a task to be subject to multiple level-$\ell$
          execution overruns in the same window. Indeed, it might be too pessimistic to assume
          that each task of criticality at least $\ell+1$ will be subject to a level-$\ell$ execution
          overrun. The value of $k$ is bound to the task set and can be obtained statistically by 
          observing the real number of level-$\ell$ execution overruns when executing the system,
          or according to certification constraints.
	\item[$\bullet$] For any task $\tau_i$ scheduled with FP$_\text{MC}$, and a fault model $\dfrac{k}{W}$:
		\begin{itemize}
			\item[$\circ$] $\mathrm{hp}(i)$: the set of tasks $\tau_j$ having a priority higher than $\tau_i$;
			\item[$\circ$] $\mathrm{hp}^R(i)$: the set of available jobs released by tasks $\tau_j \in \mathrm{hp}(i)$;
			\item[$\circ$] $\mathrm{hp}_{k-1}(i,\ell)$: the set of at most $k-1$ tasks $\tau_j$ having a priority higher than $\tau_i$, and a criticality $\chi_j > \ell$. If more than $k-1$ tasks satisfy these conditions, we select the more recurrent tasks, i.e. the $k-1$ tasks of which the periods are the smallest;
			\item[$\circ$] $\mathrm{lp}(i,\ell)$: the set of tasks $\tau_j$ having a priority lower than $\tau_i$ and a criticality $\chi_j \geq \ell$.
			\item[$\circ$] $\mathrm{lp}^R(i,\ell)$: the set of available jobs released by tasks $\tau_j \in \mathrm{lp}(i,\ell)$;
			\item[$\circ$] The following equation allows to compute an upper bound on the worst-case response time $r_i$ of $\tau_i$:
				\begin{equation}\label{equ:responseTime}
					r_i = C_i(\chi_i) + \sum\limits_{\tau_p \in \mathrm{hp}(i)} \left\lceil \dfrac{r_i}{T_p} \right\rceil \times C_p(\chi_i)
				\end{equation}
		\end{itemize}
\end{itemize}
In the following, we will assume a fair distribution of the $\ell$-allowance between all $k$ 
tasks of criticality at least $\ell+1$, i.e. each task of criticality at least $\ell+1$ is
granted the same amount of $\ell$-allowance.

\begin{proposition}
Let $\tau$ be an MC-schedulable mixed-criticality task set. It follows that for any criticality 
level $\ell$, the following condition is satisfied:
	\begin{equation}
		U_{\ell} \leq 1
	\end{equation}
\end{proposition}

%%%%%%% Allowance with FP %%%%%%%
\subsubsection{Allowance with FP}\label{sec:allowanceFP}
The following theorem shows how to compute the static $\ell$-allowance of a task $\tau_i$ with FP 
scheduling. The priorities assigned to the tasks are the ones that bear witness to the OCBP-schedulability
condition. The computation of $A_i(\ell)$ is based on the worst-case response time of the tasks with
the allowance use.
\begin{theorem}
Let $\tau = \{\tau_1, \tau_2, ..., \tau_n\}$ be a set of $n$ mixed-criticality periodic tasks scheduled
with FP, and let $\tau_1 \vartriangleright \tau_2 \vartriangleright ... \vartriangleright \tau_n$ be
the priority order that yields OCBP-schedulability. The maximum $\ell$-allowance that can be granted
to a task $\tau_i$, of criticality at least $\ell+1$, with a fair allowance distribution, and a fault
model $\frac{k}{W}$, is the minimum positive value of $A_i(\ell)$ satisfying the following equations:
	\begin{equation}\label{equ:cond3}
		C_i(\ell) + A_i(\ell) \leq C_i(\chi_i)
	\end{equation}
	
	\begin{equation}\label{equ:cond1}
		U_{\ell} + \sum\limits_{\tau_p \in \mathrm{hp}_{k-1}(i,\ell) \cup \tau_i} \dfrac{A_i(\ell)}{T_p}\leq 1
	\end{equation}
		
	\begin{equation}\label{equ:cond2}
		\begin{split}
			&\forall \tau_j \in \mathrm{lp}(i,\ell): \\
			&r_j^* = C_j(\chi_j) + \sum\limits_{\tau_p \in \mathrm{hp}(j)} \left\lceil \dfrac{r_j^*}{T_p} \right\rceil \times C_p(\chi_j) \\
			&\quad\quad\quad\quad\quad+ \sum\limits_{\tau_p \in \mathrm{hp}_{k-1}(i,\ell) \cup \tau_i} \left\lceil \dfrac{r_j^*}{T_p} \right\rceil \times A_i(\ell) \leq D_j
		\end{split}
	\end{equation}
	
\end{theorem}
\begin{proof}
Equation (\ref{equ:cond3}) grants that the allowance should not allow a task to execute for
more than its worst-case execution time at its own criticality level. Equation (\ref{equ:cond1})
is the processor utilization condition at criticality level $\ell$ updated to take into account
the $\ell$-allowance consumption of at most $k$ tasks. We want to ensure that lower cri\-ti\-cality
tasks still meet their deadline in case of a level-$\ell$ execution overrun  of at most $k$
higher criticality tasks, for a duration limited by the allowance, which leads us to equation
(\ref{equ:cond2}). This equation represents the worst-case response time of $\tau_j\in lp(i,l)$
when it executes at its highest criticality level, higher priority tasks $\tau_p$ including
$\tau_i$ execute for at most $C_p(\chi_j)$ time units, and $k$ higher priority tasks $\tau_r$
including $\tau_i$ consume their $\ell$-allowance.
This equation is only verified for tasks having a lower priority than $\tau_i$ because of Lemma\ref{lem1}.
\end{proof}

It follows that tasks with a criticality $\ell$ should not be dropped until a higher priority
task $\tau_i$ with criticality $\chi_i > \ell$ executes for more than $C_i(\ell) + A_i(\ell)$
time units.

%%%%%%% Allowance with EDF %%%%%%%
\subsubsection{Allowance with EDF}
The allowance principle that was initially introduced by Bougueroua et al. \cite{bougueroua_george_midonnet}
was applied both to Fixed Priority and Earliest Deadline First scheduling strategies, and dealt
only with traditional task sets. Nevertheless, as the following proposition states, the Earliest
Deadline First scheduling strategy is not optimal for mixed-criticality task sets.

\begin{proposition}
Every mixed-criticality task set $\tau$ that is MC-Schedulable does not necessarily admit Earliest 
Deadline First as a correct on-line scheduling policy. In other words, EDF is not optimal for
mixed-criticality task sets.
\end{proposition}
\begin{proof}
Let $\tau$ be the mixed-criticality task set which was proven to me MC-Schedulable in example
\ref{ex:mcTaskSet2}. Figure \ref{fig:edfNotOptimal} shows that the criticality of the scenario
is not raised soon enough to guarantee the temporal constraint of task $\tau_3$ when all tasks
execute for their worst-case execution time at their own criticality level.
\end{proof}
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.40]{./figures/EDFNonOptimal.png}
		\caption{Non-Optimality of EDF algorithm.}\label{fig:edfNotOptimal}
	\end{center}
\end{figure}
Since our study will only focus on task sets $\tau$ which are OCBP-Schedulable, and because this
property does not allow us to determine if Earliest Deadline first is a correct scheduling policy
for $\tau$, we will not implement the allowance principle for the Earliest Deadline First algorithm.

%%%%%%% Allowance Domain %%%%%%%
\subsubsection{Allowance Domain}\label{sec:allowanceDomain}
Section \ref{sec:allowanceFP} showed how to compute a fair $\ell$-allowance for task sets scheduled
with FP. Nevertheless, one might want to use a different allocation rule for the $\ell$-allowance.
In this section, we present a way of representing all possible values for the $\ell$-allowance, by
characterizing the space of feasible $\ell$-allowance.

\begin{definition}
Let $A(\ell) = \{A_1(\ell), A_2(\ell), ..., A_n(\ell)\}$ be the design variables for a mixed-criticality
task set $\tau$. Then, the feasibility region in the $A(\ell)$-space is the set of values of $A(\ell)$
such that $\tau$ remains feasible, if every task $\tau_i \in \tau$ consumes up to $A_i(\ell)$ extra
time units at criticality level $\ell$.
\end{definition}

Bini et al. \cite{bini_buttazzo} introduced a schedulability condition for periodic task sets. This
condition has been adapted to mixed-criticality task sets in the following theorem.
\begin{theorem}
A periodic mixed-criticality task set $\tau$ is schedulable under fixed priorities if and only if:
	\begin{multline}
		\forall i = 1,...,n, \; \exists t \in \mathrm{schedP}_i \, \text{such that} \, \\
		C_i(\chi_i) + \sum_{\tau_p \in \mathrm{hp}(i)} \left\lceil \dfrac{t}{T_p} \right\rceil C_p(\chi_i) \leq t \label{eq:domain}
	\end{multline}
where $\mathrm{schedP}_i$ is a set of scheduling points defined as $\mathrm{schedP}_i = \mathcal{P}_{i-1}(D_i)$, and $\mathcal{P}_i(t)$ is defined as follows:
	\begin{equation}
		\left\{
			\begin{split}
				&\mathcal{P}_0(t) \\
				&\mathcal{P}_i(t) = \mathcal{P}_{i-1}\left(\left\lfloor \frac{t}{T_i} \right\rfloor\right) \cup \mathcal{P}_{i-1}(t)
			\end{split}
		\right.
	\end{equation}
By using a compact notation and logical operators, equation \ref{eq:domain} can be rewritten as:
	\begin{equation}
		\bigwedge \limits_{i = 1,...,n} \bigvee \limits_{t \in \mathrm{schedP}_i} n_i \cdot C_i \leq t \label{eq:domainCompact}
	\end{equation}
where $n_i = \left(\left\lceil \frac{t}{T_1} \right\rceil, \left\lceil \frac{t}{T_2} \right\rceil, ..., \left\lceil \frac{t}{T_{i-1}} \right\rceil, 1\right)$ and $C_i = \left(C_1(\chi_i), C_2(\chi_i), ..., C_i(\chi_i)\right)$
\end{theorem}

Since what we want to represent is the feasibility region in the $A(\ell)$-space, we need to
introduce the allowance variables. Equation \ref{eq:domainCompact} must be extended so as to
consider a level-$\ell$ allowance for each task of criticality at least $\ell$.

\begin{theorem}
Let $\tau$ be a mixed-criticality task set. The feasibility region in the $A(\ell)$-space is
computed as follows:
	\begin{equation}
		\bigwedge \limits_{\tau_i \, | \, \chi_i \geq \ell} \bigvee \limits_{t \in \mathrm{schedP}_i} \left\lgroup n_i \cdot C_i^{\ell} \leq t \wedge A_i(\ell) \leq C_i(\chi_i) - C_i(\ell) \right\rgroup \label{eq:domainCompactMC}
	\end{equation}
where $C_i^{\ell} = \left(C_1(\ell) + A_1(\ell), C_2(\ell) + A_2(\ell), ..., C_i(\ell) + A_i(\ell)\right)$.
\end{theorem}
\begin{proof}
Equation \ref{eq:domainCompactMC} simply extends Equation \ref{eq:domainCompact} by allowing
each task to consume up to its worst-case execution time at criticality level $\ell$ plus its 
allowance, and prevents the allowance to allow a task to exceed its worst-case execution time
at its own criticality level.
\end{proof}

\begin{exmpl}
Let us compute the $A(\ell)$-spaces, for $\ell = 1,2,3$, of the mixed-criticality task set
presented in example \ref{ex:mcTaskSet1}. Let us first rename the task according to the priority
assignment. Task $\tau_3$ becomes task $\tau_1$, and task $\tau_1$ becomes task $\tau_3$. 
Task $\tau_2$ remains unchanged.
\begin{itemize}
	\item[$\bullet$] $A(1)$-space: The set of scheduling points to consider is given in
          table \ref{schedP::1}. \\
		\begin{figure}[h!]
		\begin{center}
			\begin{tabular}{|c||c|}
				\hline
				$\tau_i$ & $\mathrm{schedP}_i$ \\
				\hline
				\hline
				$\tau_1$ & \{8\} \\
				\hline
				$\tau_2$ & \{7\} \\
				\hline
				$\tau_3$ & \{5\} \\
				\hline
			\end{tabular}
		\end{center}
		\caption{Scheduling points in $A(1)$-space}\label{schedP::1}
		\end{figure}
		
		The $A(1)$-space is then represented by the set of equations \ref{equations::1},
                and is depicted in green in Figure \ref{space::1}, with $A_3(1)=0$.
		\begin{equation}
			\left\{
				\begin{split}
					& A_1(1) \leq 7 \wedge A_1(1) \leq 6 \\
					& A_2(1) + A_1(1) \leq 5 \wedge A_2(1) \leq 2 \\
					& A_3(1) + A_2(1) + A_1(1) \leq 2 \wedge A_3(1) \leq 0
				\end{split}\label{equations::1}
			\right.
		\end{equation}
		
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.25]{./figures/A1_space.png}
		\caption{$A(1)$-space}\label{space::1}
	\end{figure}
	\item[$\bullet$] $A(2)$-space: The set of scheduling points to consider is given in 
          table \ref{schedP::2}. \\
		\begin{figure}[h!]
		\begin{center}
			\begin{tabular}{|c||c|}
				\hline
				$\tau_i$ & $\mathrm{schedP}_i$ \\
				\hline
				\hline
				$\tau_1$ & \{8\} \\
				\hline
				$\tau_2$ & \{7\} \\
				\hline
			\end{tabular}
		\end{center}
		\caption{Scheduling points in $A(1)$-space}\label{schedP::2}
		\end{figure}
	
	
		The $A(2)$-space is then represented by the set of equations \ref{equations::2}.
		
		\begin{equation}
			\left\{
				\begin{split}
					& A_1(2) \leq 5 \wedge A_1(2) \leq 4 \\
					& A_2(2) + A_1(2) \leq 1 \wedge A_2(2) \leq 0
				\end{split}\label{equations::2}
			\right.
		\end{equation}
		Leading to $A_1(2)=1$ and $A_2(2)=0$.
	\item[$\bullet$] $A(3)$-space: The set of scheduling points to consider is given in
          table \ref{schedP::3}. \\
		\begin{figure}[h!]
		\begin{center}
			\begin{tabular}{|c||c|}
				\hline
				$\tau_i$ & $\mathrm{schedP}_i$ \\
				\hline
				\hline
				$\tau_1$ & \{8\} \\
				\hline
			\end{tabular}
		\end{center}
		\caption{Scheduling points in $A(1)$-space}\label{schedP::3}
		\end{figure}
		
		The $A(3)$-space is then represented by equation \ref{equation::3}. As one can
                easily see, no strictly positive allowance can satisfy this equation, leading
                to $A_1(3)=0$.
		\begin{equation}
			A_1(3) \leq 1 \wedge A_1(3) \leq 0
		\end{equation}\label{equation::3}
\end{itemize}
\end{exmpl}

%%%%%%%%%% Allowance Implementation %%%%%%%%%%
\subsection{Allowance implementation}\label{sec:allowanceImplementation}
We now present an on-line mechanism for the allowance management than can be implemented using
traditional timers. We call this mechanism the \emph{Latest Execution Time (LET)}.

\begin{definition}
Let $t_i$ be the request time of the $k^\text{th}$ job $J_k^i$ released by task $\tau_i$, with
$\chi_i \geq \ell$. $\mathsf{LET}_i^{\ell}(t_i)$ is defined as the Latest Execution Time $J_k^i$,
if subject to a level-$\ell$ execution overrun, can proceed with its execution without having
to drop lower criticality tasks for the system to still respect temporal constraints of tasks
of criticality greater than or equal to $\ell$.
\end{definition}

We will now show how to compute the $\mathsf{LET}$ of a task $\tau_i$, and this $\mathsf{LET}$
will be used to determine the instant at which lower criticality tasks should be dropped.

\begin{definition}
Let $J_k^i$ be the $k^\text{th}$ job released at time $t_i$ by task $\tau_i$ with criticality
$\chi_i \geq \ell$. The Latest Execution Time for $J_k^i$ at criticality level $\ell$ is computed
at time $t_i$ as follows:
	\begin{equation}
		\begin{split}
			&\forall \ell \in [1,\mathnormal{L}]: \\
			&\mathsf{LET}_i^{\ell}(t_i) = \max\left\lgroup t_i, \max\limits_{\tau_j \in \mathrm{hp}^R(i)} \left( \mathsf{LET}_j^\ell(t_j) \right) \right\rgroup \\
			&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad+ C_i(\ell) + A_i(\ell)
		\end{split}
	\end{equation}
The Latest Execution Time of jobs $J$ in $\mathrm{lp}^R(i,\ell)$ is then updated as follow:
	\begin{equation}
		\forall J \in \mathrm{lp}^R(i,\ell): \mathsf{LET}_j^{\ell}(t_j) = \mathsf{LET}_j^{\ell}(t_j) + C_i(\ell) + A_i(\ell)
	\end{equation}
\end{definition}
Based on the $\mathsf{LET}_i^{\ell}(t_i)$ of a job of which the criticality is greater than $\ell$,
we can initialize a timer in charge of detecting a level-$\ell$ execution overrun after which all
tasks of criticality lower than $\ell$ need to be dropped. We do not initialize such a timer for 
jobs of which the criticality is equal to $\ell$, since they will not be subject to a level-$\ell$
execution overrun. But we need to compute their $\mathsf{LET}$ though since it may be necessary to
compute the $\mathsf{LET}$ of other jobs. \\

\begin{exmpl}
The allowance implementation will be illustrated using example \ref{ex:mcTaskSet1}, and we will
assume that every task of criticality at least $\ell+1$ can be subject to a level-$\ell$ execution
overrun. The computed allowance as well as the Latest Execution Times of the first job of each task
are represented in table \ref{ex:table}.
\begin{figure}[h!]
	\begin{center}
		\begin{tabular}{|c||c|c|c|c|c|}
		\hline
			Task & $A_i(1)$ & $A_i(2)$ & $\mathsf{LET}_i^{1}(0)$ & $\mathsf{LET}_i^{2}(0)$ & $\mathsf{LET}_i^{3}(0)$ \\
		\hline
		\hline
			$\tau_3$ & 1 & 1 & 2 & 4 & 7 \\
		\hline
			$\tau_2$ & 1 & 0 & 4 & 7 & $\times$ \\
		\hline
			$\tau_1$ & 0 & 0 & 5 & $\times$ & $\times$ \\
		\hline
		\end{tabular}\caption{Example of three tasks with FP.}\label{ex:table}
	\end{center}
\end{figure}

Figure \ref{ex:LET0} illustrates a scenario where both $\tau_3$ and $\tau_2$ exceed their worst-case
execution time at criticality level 1, but without having to interrupt $\tau_1$, which still has enough
time left to complete its execution. If the allowance mechanism had not been used, the criticality
of the scenario would have been raised at time instant 1, thus preventing task $\tau_1$ to complete
its execution.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.45]{./figures/Let(0).png}
		\caption{$\mathsf{LET}_i^1(0)$ example.}\label{ex:LET0}
	\end{center}
\end{figure}

Similarly, Figure\ref{ex:LET1} illustrates a scenario where $\tau_3$ exceeds its worst-case execution
time at criticality level 2, but completes its execution soon enough to prevent the interruption of
task $\tau_2$, which still has enough time to complete its execution for a duration equal to its
worst-case execution time at its own criticality level. If the allowance  mechanism had not been used,
the criticality of the scenario would have been raised at time instant 3, thus preventing task
$\tau_2$ to complete its execution.
	
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.45]{./figures/Let(1).png}
		\caption{$\mathsf{LET}_i^2(0)$ example.}\label{ex:LET1}
	\end{center}
\end{figure}

Finally, Figure\ref{ex:allowanceReuse} illustrates the allowance recovery. Indeed, task $\tau_3$ was
granted an allowance of one time unit, but completes its execution before making use of it. This
allows task $\tau_2$ to make use of two extra time units to complete its execution without having
to interrupt task $\tau_1$.
	
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.45]{./figures/AllowanceReuse.png}
		\caption{Allowance recovery.}\label{ex:allowanceReuse}
	\end{center}
\end{figure}
\end{exmpl}

%%%%%%%%%% Resetting Criticality %%%%%%%%%%
\subsection{Resetting Criticality}\label{sec:resettingCriticality}
The mechanism presented in Section\ref{sec:allowanceImplementation} allows us to delay the suspension
of lower critically tasks at the latest possible time, thus avoiding to drop jobs that would still have
enough time to complete their execution. Nevertheless, in some cases, at a given criticality level,
a task might need more time to complete its execution than what the allowance offers, and the criticality
of the scenario needs to be raised. \\
In this section, we investigate the relevance of keeping lower criticality tasks suspended. We argue that,
if it is indeed ne\-cessary to suspend lower criticality tasks to guarantee temporal constraints of higher
criticality tasks, this decision is not irreversible, and there exists a time at which all tasks can
again execute concurrently, whatever their criticality level. \\

Throughout this section, we will assume that contrary to what Lemma\ref{lem1} suggests, as soon as the
scenario's criticality reaches level $\ell$, every task of criticality less than $\ell$ will be dropped.
The definitions and results we present can nevertheless be a adapted to take into account Lemma\ref{lem1}.

\begin{definition}
If the current scenario is of criticality level $\ell$, a \emph{level-$\ell$ idle time} $t$ on a processor
is defined as an instant in time, such that there are no available jobs of criticality at least $\ell$
waiting to be scheduled at time $t$.
\end{definition}

\begin{theorem}\label{theo:decreaseCriticality}
Let $\tau$ be a periodic mixed-criticality task set which is MC-Schedulable. Whenever level-$\ell$ idle
time occurs in the scheduling of $\tau$, the criticality of the scenario can be reset to its lowest level.
\end{theorem}
\begin{proof}
Let $t_{\text{idle}}$ be the first level-$\ell$ idle time in the scheduling of $\tau$ (with
$t_{\text{idle}} > 0$). This means that every job released by a task of which the criticality is at least
as high as the scenario criticality at time $t_{\text{idle}}$ completed its execution, and no job is
currently available. Let $\tau_i$ be any task in $\tau$ that had to be dropped before $t_{\text{idle}}$.
We need to show that at time $t_{\text{idle}}$, every job released by $\tau_i$ can again receive $C_i(\chi_i)$
units of execution in any scenario of criticality at most $\chi_i$. Let $J_i$ be the first job released
by $\tau_i$ at time $t_i$ ($t_i \geq t_{\text{idle}}$). We distinguish two cases:
\begin{enumerate}
	\item $t_i$ is a critical instant for $\tau_i$: it follows that the response time of $J_i$ will be
          the highest among all jobs generated by $\tau_i$. But since $\tau$ is MC-Schedulable, we know that
          $J_i$ will meet its deadline if it executes up to $C_i(\chi_i)$, and none of the other tasks
          $\tau_j$ releases a job that executes for more than $C_j(\chi_i)$ time units.
	\item $t_i$ is not a critical instant for $\tau_i$: in this case, we know that the response time
          of $J_i$ will be less than or equal to the worst response time of task $\tau_i$. But we proved
          that $J_i$ could meet its deadline if it was released at a critical instant. It follows that
          $J_i$ will also meet its deadline if it is released at a non-critical instant.
\end{enumerate}
\end{proof}

\begin{exmpl}
Let us consider again the task set $\tau$ presented in example \ref{ex:mcTaskSet1}, and a possible scenario
for this task set illustrated in Figure\ref{ex:decreaseCriticality}. The first job released by task $\tau_3$
completes its execution after 7 time units. The criticality of the scenario reached level 2 at time $t = 1$,
and level 3 at time $t = 3$, so both $\tau_1$ and $\tau_2$ were dropped. At time $t = 7$, a level-3 idle
time occurs, so the criticality level can be reset to level 1, and the jobs released by $\tau_1$ and $\tau_2$
can be considered again. Task $\tau_2$ releases its next job at time $t = 7$, but is preempted by task
$\tau_3$ at time $t = 8$. Since $\tau_3$ completed its execution after 3 time units, the criticality of the
scenario reached level 2, and task $\tau_1$ had to be dropped. But $\tau_2$ is still able to meet its deadline.
At time $t = 13$, a level-2 idle time occurs, and the criticality level can be reset again. This allows every
task to generate a job which is able to meet its deadline.
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.5]{./figures/decreaseCriticality.png}
		\caption{Priority decreasing.}\label{ex:decreaseCriticality}
	\end{center}
\end{figure}
\end{exmpl}

%%%%%%%%%% Simulations %%%%%%%%%%
\subsection{Simulations}\label{sec:simulation}
In this section we compare the performances of three possible solutions to deal with the criticality rising
that happens when scheduling mixed-criticality task sets:
\begin{enumerate}
	\item The traditional approach (TA): as soon as a task $\tau_i$ exceeds its worst-case execution 
          time at criticality level $\ell$, the criticality of the scenario is raised, and all task $\tau_j$ 
          with $\chi_j = \ell$, having a priority lower than $\tau_i$, are dropped.
	\item The traditional approach, extended with a criticality decreasing mechanism (CD): as soon as a
          task $\tau_i$ exceeds its worst-case execution time at criticality level $\ell$, the criticality
          of the scenario is raised, and all task $\tau_j$ with $\chi_j = \ell$, having a priority lower than
          $\tau_i$, are dropped. Never\-theless, as soon as a level-$\ell$ idle time occurs, the criticality
          of the system is reset to its lowest level.
	\item The traditional approach, extended with a criticality decreasing mechanism and the allowance
          concept (CD-A): as soon as a task $\tau_i$ exceeds its worst-case execution time at criticality
          level $\ell$ plus its allowance, the criticality of the scenario is raised, and all task $\tau_j$
          with $\chi_j = \ell$, having a priority lower than $\tau_i$, are dropped. Nevertheless, as soon
          as a level-$\ell$ idle time occurs, the criticality of the scenario is reset to its lowest level.
\end{enumerate}

Each job $J_k^i$ released by a task $\tau_i$ is assigned an exact duration $c_{i,k}^e$ in the interval
$[1,C_i(\chi_i)]$, using a triangular distribution. As a job usually does not complete its execution after
a very short nor a very long amount of time, but rather after an intermediate one, the use of a triangular
distribution is a way to represent this. Indeed, the latter makes it possible to concentrate most of the 
probabilities around a value called the mode. As Figure\ref{fig:triangularDistribution} depicts, for a task
$\tau_i$, the distribution lower limit is set to $0$, the distribution upper limit is set to $C_i(\chi_i)$,
and the distribution mode is equal to $\frac{C_i(\chi_i)}{3}$. This means that a job will most of the time
complete its execution after an amount of time close to 33\% of its worst-case execution time at its own
criticality level. We then generate a random number $r$ following that particular distribution, and $c_{i,k}^e = \lceil r \rceil$.
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.3]{./figures/triangularDistribution.png}
		\caption{Triangular Distribution for task $\tau_i$.}\label{fig:triangularDistribution}
	\end{center}
\end{figure}
Each job's exact duration is completely independant of the exact duration of the previous jobs released by
the same task. A job $J_k^i$ will complete its execution as soon as the scheduler grants him a total of
$c_{i,k}^e$ time units, but this duration is not known beforehand by the scheduler. \\

We analyze task sets $\tau$ composed of a number of mixed-criticality tasks $|\tau|$ varying between 4 and 8,
and with a criticality level bounded by 4. For the CD-A approach, the number of faulty tasks is set to
$k = |\tau|$, to make sure the criticality is raised due to a task exceeding its worst-case execution time at
a particular criticality level plus its allowance, and not because the number of faulty tasks exceeds $k$.

We compared the average number of jobs that had to be dropped for the three methods. We therefore generated
100 mixed-criticality task sets, and we reiterated the simulations for various upper bounds on each task
utilization (this upper bound is refered to as $U_{\text{max}}(\tau_i)$). Figures \ref{fig:results} and
\ref{fig:resultsGraphical} depicts the results of those simulations. The x-axis represents the maximum 
utilization per task $U_{\text{max}}(\tau_i)$, while the y-axis represents the average percentage of jobs
that were dropped.
\begin{figure}[h!]
	\begin{center}
		\begin{tabular}{|c||c|c|c|}
			\hline
			$U_{\text{max}}(\tau_i)$ & TA & CD & CD-A \\
			\hline
			\hline
			0.1	& 33.4223\% & 9.3976\% & 1.4071\% \\
			\hline
			0.2 & 32.9436\% & 11.7511\% & 4.2982\% \\
			\hline
			0.3	& 34.0782\% & 13.3883\% & 6.2279\% \\
			\hline
			0.4	& 38.2408\% & 15.6153\% & 9.4358\% \\
			\hline
			0.5	& 42.7098\% & 19.4652\% & 13.052\% \\
			\hline
			0.6	& 42.3251\% & 20.0583\% & 14.2854\% \\
			\hline
			0.7	& 42.9011\% & 21.8738\% & 16.7087\% \\
			\hline
			0.8	& 48.0304\% & 23.05\% & 17.1582\% \\
			\hline
			0.9	& 50.266\% & 24.0808\% & 19.1672\% \\
			\hline
			1.0 & 50.1776\% & 26.1121\% & 21.1875\% \\
			\hline
		\end{tabular}\caption{Average percentage of jobs dropped w.r.t. $U_{\text{max}}(\tau_i)$}\label{fig:results}
	\end{center}
\end{figure}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.57]{./figures/resultats.png}
		\caption{Average number of jobs dropped (Graphical).}\label{fig:resultsGraphical}
	\end{center}
\end{figure}

We observe that the criticality decreasing mechanism could reduce the number of dropped jobs by 25\% in
the worst case, while this reduction could reach more than 30\% when each task utilization is low. We
furthermore notice that when this mechanism is supplemented with the allowance, the number of dropped
jobs could be reduced even more by 5\% to 9\%. \\

We observe that the lower each task utilization is, the more efficient the allowance mechanism is. This
is due to the fact that a higher allowance can be granted to each task.

%%%%%%%%%% Conclusion %%%%%%%%%%
\subsection{Conclusion}\label{conclusion}
In this paper, we have studied two principles that allow us to relax the strictness of mixed-criticality
tasks scheduling using a fixed priority strategy. Those principles are the allowance on WCETs and the
criticality decreasing mechanism. We showed how the allowance could be computed using feasibility conditions
relying on the worst-case response time of a task according to a criticality level, and suggested a simple
mechanism that implements it denoted LET. We then proved that idle times could be used to decrease the
overall criticality of the system. Experiments furthermore attested that the criticality decreasing mechanism 
could reduce up to 24\% the number of jobs that had to be suspended, while the allowance could decrease
this number by an additional 8\%.




%%%% END


Changement de criticité: priorisation aux flux dont la QoS IP n'est pas la
plus élevée


\FIXME{exigences de criticité}:\\
La transmission des flux doit respecter les exigences temporelles du domaine
de criticité le plus haut

