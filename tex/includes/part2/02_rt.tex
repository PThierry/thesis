%%
%%
%% hardware_impacts.tex for thesis in /doctorat/these/tex
%%
%% Made by Philippe THIERRY
%% Login   <Philippe THIERRYreseau-libre.net>
%%
%% Started on  Fri Mar 12 16:36:41 2010 Philippe THIERRY
%% Last update Mon Aug 30 17:00:21 2010 Philippe THIERRY

\chapter{De la compatibilité d'une architecture sécuritée avec des contraintes temps-réel}
\doMinitoc

\section{Architectures compartimentées et temps réel}

\paragraph{}
Afin de repondre aux exigences sécuritaires de la solution, l'usage
d'architectures logicielles compartimentées dans le temps est nécessaire.
Cette compartimentation réduit en effet les interactions dues aux variations
de comportement (charge processeur, accès mémoire, etc) des différents
compartiment.\\
Lorsqu'il est nécessaire d'intégrer dans le même temps des exigences temps
réel sur les flux traité par la passerelle, il devient nécessaire de
considérer la bonne adéquation de la solution de compartimentation avec les
contraintes d'exécution (WCET, période, Dead-line) des services de traitement de flux.

\subsection{A propos des exigences temps réelles}

\paragraph{}
En plus des différentes exigences de sécurité décrite dans le chapitre
précédant, plusieurs exigences temps-réelles doivent être respectée dans le
cadre d'une passerelle d'interconnexion entre deux domaines dont les niveaux
de criticités sont à la fois multiples dans chacun d'entre eux et
potentiellement différents de chaque coté de la passerelle.

\begin{requirement}
Il doit être possible de faire communiquer entre-eux des domaines de criticité hétérogène\label{req:multi_crit}
\end{requirement}

\begin{requirement}
Le temps de traversée d'une passerelle entre deux domaines de criticité doit être bornée\label{req:rt}
\end{requirement}

\begin{requirement}
En fonction du besoin, il doit être possible de garantir un débit minimum de traversée entre deux domaine de criticité\label{req:debit}
\end{requirement}

\subsection{De l'ordonnancement hiérarchique à la virtualisation}
\label{sec:hierarchicalrt}


\subsection{Hiérarchie et Time Division Multiplexing}

\paragraph{}
Les architectures logicielles compartimentées sont ordonnancées à la manière
des systèmes temps réels à ordonnancement hiérarchique (\FIXME{ref}). On
définit alors un ordonnancement dit {\it global}, correspondant à l'ordonnancement des compartiments, et un ordonnancement dit {\it local}
correspondant à l'ordonnancement des tâches dans un compartiment donné.\\
A chaque compartiment est associé un {\it slot}. Ce slot est une fraction
temporelle périodique pendant laquelle le compartiment est autorisé à être
exécuté. L'exécution successive de l'ensemble des slots du systèmes forme un
motif temporel strict définissant la période TDM.

\paragraph{}
Dans le cadre de ma thèse, j'ai étudié la problématique d'ordonnancement temps
réel des fonctions logicielles de la passerelle sur une base TDM, pour les
raisons de sécurité évoquée dans le Chapitre \ref{chap:solution_secu}. Les
fonctions de traitement des différents compartiments devant respecter des
exigences de temps-réel s'appuie sur une politique d'ordonnancement de type
EDF (Earliest Deadline First). La solution s'appuie donc sur un ordonnancement
global TDM avec un ordonnancement local EDF, qui sera noté dans la suite
TDM/EDF.

\paragraph{}
Dans le cadre de cette architecture, il est cependant nécessaire de
déterminer, pour l'ensemble des compartiments ayant des tâches devant
respecter des exigences de temps réel, la période et la durée nécessaire
du slot permettant d'assurer le respect. La particularité de la solution de
compartimentation pour la sécurité sur base TDM est que l'ordonnanceur global,
géré au niveau de l'hyperviseur, n'est pas informé des propriétés temps réel
des tâches hébergées dans les compartiments. De plus, le respect du
partitionnement TDM strict interdit toute variation du motif d'ordonnancement
des compartiments. En conséquence, bien que l'ordonnancement local des tâches d'un
compartiment donné puisse être considéré en ligne (c'est le cas de la
politique d'ordonnancement EDF), l'ordonnancement global est calculé hors
ligne. C'est ce calcul qui est considéré ici.

\paragraph{}
Plusieurs variables sont introduite dans la Table \ref{tab:vars} afin de formaliser l'ordonnançabilité
d'une architecture logicelle compartimentée sur une base TDM (Time Division
Multiplexing) stricte.
\begin{table}
\label{tab:vars}
\begin{tabular}{ll}
 {\it Variables de niveau {\it global}} & {\it Variables de niveau {\it local}} \\
 \begin{minipage}{0.48\linewidth}
 \fbox{
    \begin{minipage}{0.97\linewidth}
        Considérant $m$ compartiments, on définit:\\
        $U_{i}:$ la charge associée au compartiment $C_{i}$\\
        $SC=\{SC_1,\ldots,SC_m\}$ la durée du slot $S_i$ associé à $C_{i}$\\
        $\tau=\{\tau_1,\ldots,\tau_m\}$ l'ensemble des ensembles tâches associés à $C_i$\\
        $ST_{i}$ La période du slot $S_{i}$\\
        $SC_{i}$ La durée du slot $S_i$\\
    \end{minipage}
 }
 \end{minipage}
 &
 \begin{minipage}{0.48\linewidth}
 \fbox{
    \begin{minipage}{0.97\linewidth}
        Considering $n$ tâches, on définit:\\
        $\tau_{i}$ l'ensemble de tâches du compartiment $C_{i}$, $\tau_{i} = \{ \tau_i^{1},
            \ldots, \tau_i^{n} \}$\\
            $d_{j}$ la deadline de la tâche $\tau_{i}^{j}$ de $C_{i}$\\
            $c_{j}$ le WCET de la tâche $\tau_{i}^{j}$ de $C_{i}$\\
            $t_j$ la période de la tâche $\tau_{i}^{j}$ de $C_{i}$\\
            \vspace{10.5mm}
    \end{minipage}
 }
 \end{minipage}
 \\
\end{tabular}
\caption{Définition des variables de niveau compartiment et de niveau tâches}
\end{table}


\begin{figure}{h}
\input{figures/hypervisor_global_sched.tex}
\caption{S}
\label{fig:hyp_global_sched}
\end{figure}


%-------------8<

\paragraph{}
Afin de garantir l'ordonnançabilité d'un système de compartimenté sur base
TDM, plusieurs contraintes doivent être satisfaites. Certaines sont des
contraintes {\it globales} (au niveau hyperviseur) d'autres {\it locales} (au
niveau compartiment). Ces contraintes sont définies ci-après:\\
%\noindent
\vspace{1cm}
\begin{minipage}{0.98\textwidth}
\fbox{
    \begin{minipage}{0.98\textwidth}
        %\begin{linearProg}
        %\end{linearProg}
        %\vspace{-5mm}
            \noindent \textbf{Considérant la table \ref{tab:vars}, soit le
              problème:} {\it Maximiser $min_{i=1\ldots,n} SC_i(1-U_{i})$}
            {\bf sous les contraintes:}

        \begin{tabular}{l|l}
                \noindent \textit{contraintes de niveau global} &
                \noindent \textit{contraintes de niveau local}\\
        \begin{minipage}{0.45\textwidth}
                    {\small
            \begin{equation}
                \sum_{i=1}^m U_i \leq 1
                                \label{eqn:vm_load_sum}
            \end{equation}
            \begin{equation}
                \forall i\in \{ 1, \ldots, m \}, \frac{ST}{ST_i}\in
                \mathbb{N}^*
                                \label{eqn:slots_relative}
            \end{equation}
            \begin{equation}
                \sum_{i=1}^m SC_i \leq gcd(ST_1,\ldots,ST_m)
                                \label{eqn:slots_pgcd}
            \end{equation}
                        }
        \end{minipage} &
        \begin{minipage}{0.52\textwidth}
                    {\small
            \begin{equation}
                \forall i\in \{ 1, \ldots, m \}, \forall \tau_i^j \in S_i, d_j>ST_i-SC_i
                                \label{eqn:deadline_vs_ST}
            \end{equation}
                        \begin{center}
                        $\forall k \in \mathbb{N}^*, \forall t \in [ kST_{i} - SC_{i}, kST_{i}],$
            \begin{equation}
                            h(t) \leq t - k(ST_{i} - SC_{i})
                                \label{eqn:edf_sched}
            \end{equation}
                        {\it avec} $h(t) = \sum_{j = 1}^{n} max (0, 1 + \lfloor \frac{t - d_{j}}{t_{j}} \rfloor)c_{j}$,
                        \end{center}
                        }
        \end{minipage}\\
        \end{tabular}
    \end{minipage}
}
\end{minipage}

Les contraintes globales sont les suivantes:\\
L'Equation \ref{eqn:vm_load_sum} garantie que la charge processeur est
inférieur à 1. L'Equation \ref{eqn:slots_relative} est une condition
nécessaire pour assurer l'usage de slots temporels périodiques. L'Equation
\ref{eqn:slots_pgcd} est une confition suffisante pour assurer que les slots
ne se chevauchent pas, comme définit dans \cite{schedcond}.\\
La Figure \ref{fig:hyp_global_sched} décrit un exemple de répartition
temporelle entre différents slots.\\
Il est également nécessaire de considérer une contrainte locale. L'Equation
\ref{eqn:deadline_vs_ST} est une condition nécessaire pour garantir que les
tâches des compartiments exécutés dans le cadre de slots temporels ont
toujours une deadline supérieure à la période d'inactivité associées à ce
compartiment. On s'appuie, dans le cadre d'une hiérarchie d'ordonnancement de
type TDM/EDF, sur l'Equation \ref{eqn:edf_sched}, qui est une condition
nécessaire et suffisante de faisabilité dans le cadre de l'ordonnancement EDF.
Pour démontrer cette Equation, considérons le slot $i$ et l'ensemble de tâches
exécuté localement avec une politique d'ordonnancement EDF dans ce slot.\\

\noindent
Le Slot $i$ est exécuté de manière périodique par l'hyperviseur, avec une
période de $ST_i$ unités de temps. Les tâches exécutées dans ce slots le sont
pour une durée contigüe de $SC_i$ unité de temps. On commence par déterminer
le scénario pire cas en terme de faisabilité pour cet ensemble de tâches. Ce
dernier est arrive si:
\begin{itemize}
\item Toutes les tâches sont relachées simultanément (en $t_0$)
\item Le slot $i$ a déjà été exécuté entre $t_{-SC_i}$ et $t_0$, de telle
sorte que les tâches ont raté l'échéance du slot et ne pourront être exécutées
que lors du prochain slot, soit $ST_i - SC_i$ unité de temps plus tard
\end{itemize}
Ce scénario maximise clairement à la fois la charge associée à l'ensemble de
tâches du slot et le délai avant l'exécution de cet ensemble de tâches.
Le scénario pire cas étand déterminé, il reste ensuite à définir la condition
de faisabilié d'un ensemble de tâche ordonnancé par une politique de type EDF
dans le slot $i$ dans le cadre de ce scénario.\\
Toutes les tâches relachées dans l'intervale de temps $[0, t]$ avec une
deadline absolie inférieure ou égale à $t$ sont ordonnançables et respectent
leur deadlines si et seulement si elles sont exécutées durant au pire $t -
k(ST_i - SC_i)$ unités de temps. Cela se traduit en:\\
Pour tout entier $k$ avec $k \geq 1$, pour tout $t$ dans l'intervalle $[k ST_i
- SC_i, k ST_i]$, la fonction de demande processeur au temps $t$t, $h(t)$ est
  inférieure ou égale à $t - k(ST_i - SC_i)$ (condition de l'équation
  \ref{eqn:edf_sched})

\section{Moniteurs de sécurité, fonctions temps réel et environnements non sécurisables}

\subsection{De la compatibilité des moniteurs de sécurité avec le temps réel}

\paragraph{}

\subsection{De la compatibilité des environnements non certifiables avec le temps réel}

\paragraph{}
Dans le cadre de mes travaux, j'utilise dans les compartiments non certifiable
le système d'exploitation Linux. Ce dernier fournit une grande richesse
applicative et une pile réseau très complète. Cependant, Linux n'est pas un
système d'exploitation compatible avec des exigences temps réel dur. En effet,
ce dernier possède plusieurs points problématiques. En effet, le noyau Linux
étant très riche il s'appuie:
\begin{itemize}
  \item sur des thread noyau ordonnancés avec les politiques d'ordonnancement
    temps réel de POSIX (SCHED\_FIFO et SCHED\_RR) qui intègre un mécanisme
    complexe de vieillissement. La charge associés à ces threads est
    difficilement bornable car elle correspond pour la plupart à la
    conséquences de traitements d'entrées/sorties comme la gestion du disque.
    Ces threads possèdent néanmoins une affinité CPU, ce qui évite de les
    migrer en fonction de la charge. Malheureusement, l'assignation d'un
    traitement à un coeur processeur plutôt qu'un autre n'est pas définissable
    sous forme d'une configuration, mais s'établit en fonction de la charge
    courante de chacun des coeurs.
  \item sur des fonctions logicielles qui cassent la mécanique
    d'ordonnancement via le {\it vol de cycle}. En effet, ces fonctions
    (nommées {\it softirq}) ont été intégrées sous formes de code
    complémentaires exécutés à la suite de divers services comme les appels
    systèmes ou les interruptions, afin d'avoir une réactivité moyenne très forte.
    Cependant, une telle solution ne permet pas de garantir un coût d'exécution
    pire cas pour les processus ordonnancés car leur propre exécution est impactée
    par ces fonctions. Ces dernières étant de plus en plus fréquentes avec les
    version du noyau et difficile à dimensionner, la charge associée est
    difficilement bornable.
\end{itemize}

\paragraph{}
Malgré tous ces éléments problématiques pour le temps réel, il existe
plusieurs patchs permettant de rétablir un peu de comportement temps réel dans
le système d'exploitation. Dans le cadre de ma thèse, je ne parle cependant
pas des solutions RTAI ou Xenomai, qui impliquent des exigences particulières
sur les tâches temps réel, leur interdisant l'usage des API standard Linux
sous peines de perdre leur propriétés temps réel. Dans le cadre de la
définition d'une passerelle pour du traitement réseau, il est nécessaire que
ces tâches soient aptes à traiter des flux télécom, et donc à s'interfacer
avec l'API POSIX de traitement réseau. Afin de répondre à cette problématique,
je me suis donc appuyer sur la solution Linux-RT, qui intègre les travaux du
patch PREEMPT\_RT d'Ingo Molnar\cite{koolwal2009myths}. Cette solution permet
de modifier le comportement du noyau Linux en divers points:
\begin{itemize}
  \item Les threads noyaux ne s'exécutent plus avec une haute priorité temps
    réel, supprimant la collision de ses derniers avec l'ensemble de tâches
    temps réel
  \item Les {\it softirqs} ne s'exécutent plus via du vol de cycle, mais
    exclusivement dans un thread kernel spécifique
\end{itemize}

\paragraph{}
Dans le cadre de mes travaux, j'ai dimentionner l'impact de ce patch sur la
variation de latence à l'initialisation d'un job de priorité maximum. Cette
mesure permet de déterminer si il est effectivement possible de définir une
borne supérieur à cette initialisation.\\

Pour cela, je me suis appuyé sur de
l'outillage de test\cite{abeni2002measurement} pour simuler une forte charge à
la fois en terme d'entrées/sorties, d'accès mémoire et de charge processeur.
La mesure a été faite sur le système décrit dans la Table \ref{tab:rttest}\\

\begin{table}
\label{tab:rttest}
\begin{center}
\begin{tabular}{|l|l|}
  \hline
  {\bf Élément} & {\bf Description} \\
  \hline
  \hline
  {\it Architecture} & i686 core 2 duo 2Ghz \\
  {\it NICs} & DLINK-RTL8139, VIA VT6105 \\
  {\it OS} & Debian Squeeze \\
  {\it Kernel} & linux 3.2.12-rt24 \\
         & pas de support ACPI\\
         & mode FULL\_PREEMPT\\
  {\it Securité} & patch grsecurity plus compléments specifiques\\
  & (memoire \& compartimentation) \\
  {\it Load-average} & 40 \\
  {\it outil de test} & cycletest \\
  {\it nombre d'échantillons} & $10^8$ \\
  {\it durée de la mesure} & ~5 heures \\
  \hline
\end{tabular}
\end{center}
\caption{Description de la cible pour la mesure de latence d'ordonnancement avec Linux-RT}
\end{table}

\paragraph{}
Cette mesure a permis de démontrer que pour une mesure portant sur $10^8$
échantillons pendant une durée de 5 heures, la latence pire cas à
l'ordonnancement est de $350 \mu s$, comme le montre la Figure \ref{fig:rttest}.

\begin{figure}
  \includegraphics[width=8.9cm]{figures/rttest.pdf}
  \caption{Latence d'ordonnancement pour la tâche de plus haute priorité sous
    Linux avec PREEMPT\_RT\label{fig:rttest}}
\end{figure}

\subsection{Répartition des moniteurs en environnement multi-processeur}

\subsubsection{Localisation des moniteurs et des compartiments par domaines de
sécurité}

\section{Intégration du support des flux à criticité multiple}

\subsection{Rappels}\label{intro}
Our work is driven by the increasing trend in embedded systems towards integrating multiple
functionalities on shared resources. In such systems though, all functionalities do not
necessarily share a common criticality level. This means that the system may be subject
to various certification processes, which are only concerned with the validation of a subset
of the functionalities. Furthermore, the certification processes are carried out using analysis
methods whose rigor depends on the criticality of the tasks that need to be certified.
Mixed-criticality systems are an attempt to model systems that need to be certified at various
levels of assurance. In practice, a task that is subject to multiple certification processes
will be characterized by multiple estimations of its Worst-Case Execution Time (WCET), some of
which are more pessimistic than others. This reflects the differences in rigor adopted by the
certification authorities. The more a certification authority wants to ensure a task will never
exceed its WCET, the more conservative its estimated value will be. Nevertheless, when certifying
that a task will meet its constraints, the assurance level that is used for other tasks is equal
to the criticality of that particular task. This means that when running the system, if another
task reaches an estimation of its WCET at a level of assurance that is higher than the criticality
of the initial task, then this task will be suspended, since the conditions that guarantee its
feasibility are no longer met. Nevertheless, this approach can be overly pessimistic, as lower
criticality tasks may still have time to complete their execution, or at least to go on with their
computation for some time, without compromising the temporal constraints of higher criticality tasks.
Therefore, we applied a strategy previously presented for traditional task sets
\cite{bougueroua_george_midonnet}, commonly known as sensitivity analysis, and referred to as the
allowance in this article, to mixed-criticality task sets. The allowance is meant to avoid, or at
worst to delay at the latest time as possible time, the rise in criticality level in order to
minimize the number of tasks that will miss their deadline. We then present a simple implementation
of the allowance that only relies on timers, called the Latest Execution Time (LET). Furthermore,
we tackle the problem of reducing the criticality of the system. Indeed, if the criticality of
the system can be reduced, this means that the number of tasks being ignored will drop. We show
that there exists a time when all tasks can be scheduled again.\\

\subsubsection{Related work}\label{related}
Mixed-Criticality scheduling is an emerging research domain and one which is gaining
increasing interest. Vestal \cite{Vestal2007} initially introduced
the mixed-criticality task model. In his work, he highlighted the difficulty in computing
exact worst-case execution times, and observed that in practice, the higher the degree
of assurance required that a task will never exceed its worst-case execution time, the
more conservative the approximation of the latter becomes. This degree of assurance is
characterized by a level of criticality. He also suggested a fixed-task-priority strategy
based on the Audsley priority assignment scheme \cite{Audsley_1991}. Dorin et al.
\cite{Dorin2010} proved that under the restricted case of independent 
task systems with constrained-deadlines, Vestal's modified Audsley's approach was optimal
in the class of fixed-task priority algorithms. Nowadays, the Mixed-Criticality
(MC)-Schedulability problem is commonly known to arise in two different contexts. The first
one is concerned with applications that are subject to multiple certification requirements.
In this context, different Certification Authorities (CA) need to validate the application
functionalities. Nevertheless, the more critical a functionality is, the more pessimistic
the CA will be in the approximation of the WCET. Baruah et al. \cite{Baruah2010}
studied mixed-criticality systems in this context, but restricted their work to a set of
mixed-criticality jobs. In particular, Baruah \cite{bbalmms} pointed out the intractability
of the MC-Schedulability problem, and quantified the fundamental limitations of MC-Scheduling
for certification considerations. To tackle the intractability of MC-Scheduling, they suggest
two sufficient schedulability conditions, referred to as the WCR-schedulability and
OCBP-schedulability conditions. Later, Baruah and Li \cite{Li2010} extended
their previous work and suggested a fixed-job-priority scheduling strategy based on their
OCBP-schedulability condition. Baruah et al. also adapted the Earliest Deadline First algorithm
to mixed-criticality systems, by modifying the deadlines of tasks. This approach is known as
EDF-VD. More recently, Guan et al. \cite{gpmw} presented a new approach for scheduling
mixed-criticality systems, which relies on an offline fixed-job-priority ordering computation,
which is then used on-line by the scheduler. At the same time, Baruah et al. \cite{RBaruah2011a}
formalized the response time analysis for mixed-criticality tasks. \\
The second context in which mixed-criticality is defined considers that among all functionalities
deployed on a single computing platform, some might be more critical, in the sense that they are
more important, than others. In this context, Lakshmanan et al.
\cite{Niz2009,Lakshmanan2011} observe that a reservation
based approach, meant to isolate functionalities and prevent interferences, might lead to a
criticality inversion problem, where a less critical task is favoured over a high critical task
because the latter exceeded its reserved time partition. They suggested a new approach, termed
zero-slack scheduling, to avoid this problem.\\
Sensitivity analysis was previously studied by Bini et al.
\cite{bini_buttazzo,Bini2006}. They discussed the concept of a feasibility
region which allows one to determine the space of possible values for a particular task parameter.
The allowance mechanism was initially introduced by Bougueroua et al. \cite{bougueroua_george_midonnet}
in the context of traditional task sets subject to WCET overruns faults. Their work focused on
determining the largest value that could be added to a task's worst-case execution time such that
the whole task set remains schedulable, assuming a sliding window fault model.

\subsubsection{This research}\label{ourWork}
This research focuses on relaxing the strictness that occurs when scheduling mixed-criticality task sets.
We suggest improvements that can be implemented in existing on-line scheduling strategies, such that
task suspension only occurs when absolutelt necessary, thereby allowing more jobs to meet their deadline.
We also notice that task suspension can be reversed when the system workload decreases.

%%%%%%%%%% Model and Definitions %%%%%%%%%%
\subsection{Model and Definitions}\label{modelDef}
Our study focuses on uniprocessor, preemptive systems. We consider mixed-criticality sporadic task sets
$\tau = \{\tau_1, \tau_2, ..., \tau_n\}$ where the maximum criticality of a task is bound by
$\mathnormal{L}$. A task in a mixed-criticality system is characterized by a 5-tuple
$\tau_i = \{R_i, T_i, D_i, \chi_i, C_i\}$ where:
\begin{itemize}
	\item[$\bullet$] $R_i \in \mathbb{N}$ is the release time of the first job of task $\tau_i$;
	\item[$\bullet$] $T_i \in \mathbb{N}^*$ is the period of task $\tau_i$;
	\item[$\bullet$] $D_i \in \mathbb{N}^*$ is the deadline of task $\tau_i$, $D_i \leq T_i$;
	\item[$\bullet$] $\chi_i \in \mathbb{N}$ is the maximum criticality of the task $\tau_i$,
          $\chi_i \leq \mathnormal{L}$;
	\item[$\bullet$] $C_i \in \mathbb{N}^{\mathnormal{L}}$ is a size $L$ vector of worst-case
          execution times, where $C_i(\ell)$ is an estimation of the WCET of task $\tau_i$ at
          criticality level $\ell \in [1,\mathnormal{L}]$.
\end{itemize}
We assume $C_i(\ell)$ is monotonically increasing for increasing $\ell$. More precisely, for task $\tau_i$:
\begin{itemize}
	\item[$\bullet$] $\forall m \in [1, \chi_i[$, $C_i(m) \leq C_i(m+1)$;
	\item[$\bullet$] $\forall m \in [\chi_i, \mathnormal{L}[$, $C_i(m) = C_i(\chi_i)$.
\end{itemize}
It follows that no task is supposed to execute longer than its worst-case execution time at its
own criticality level. The $k^\text{th}$ job $J_k^i$ released by a mixed-criticality task $\tau_i$
is characterized by a 5-tuple $\{r_k^i, d_k^i, \mathcal{X}_k^i, C_k^i, c_k^i\}$ where:
\begin{itemize}
	\item[$\bullet$] $r_k^i \in \mathbb{N}$ is the time instant at which $J_k^i$ was released.
          Since we consider sporadic task systems, we have $r_k^i \geq r_{k-1}^i + T_i$, and $r_1^i = R_i$;
	\item[$\bullet$] $d_k^i \in \mathbb{N}^*$ is the absolute deadline of $J_k^i$. More precisely,
          $d_k^i = r_k^i + D_i$;
	\item[$\bullet$] $\mathcal{X}_k^i \in \mathbb{N}$ is the criticality of $J_k^i$, inherited
          from task $\tau_i$: $\mathcal{X}_k^i = \mathcal{X}_i$;
	\item[$\bullet$] $C_k^i \in \mathbb{N}^{\mathnormal{L}}$ is a size $L$ vector of worst-case
          execution times for $J_k^i$, inherited from task $\tau_i$: $C_k^i = C_i$;
	\item[$\bullet$] $c_k^i \in \mathbb{N}^*$ is the exact execution time of job $J_k^i$. From
          the specifications of $\tau_i$, we can say that $c_k^i \leq C_i(\mathcal{X}_i)$, but the
          exact value of $c_k^i$ will not be known until $J_k^i$ completes its execution.
\end{itemize}
\begin{definition}
A task $\tau_i$ of criticality at least $\ell+1$ is subject to a \emph{level-$\ell$ execution overrun},
if a job released by $\tau_i$ exceeds its worst-case execution time at level $\ell$.
\end{definition}

At any time $t$, we call the $j^\text{th}$ job $J_j^i$ released by task $\tau_i$ \emph{available} if
$t \geq r_j^i$ and $J_j^i$ has not yet completed its execution. The actual execution time of job $J_j^i$
is not known from the specification of $\tau_i$, but will only be discovered when $J_j^i$ completes
its execution. At any time $t$, we define the \emph{scenario} $s_i^t$ of task $\tau_i$ as the set
of exact execution times $\{c_1^i, c_2^i, ..., c_k^i\}$ for each of the $k$ jobs $J_j^i$ released
by $\tau_i$ that already completed their execution at time $t$. The scenario of the mixed-criticality
task set $\tau$ at time $t$ is defined as $s^t = \{s_1^t, s_2^t, ..., s_n^t\}$.\\
The \emph{criticality level} of scenario $s^t$ is the smallest integer $\ell$ such that, for each
$s_i^t \in s^t$, $c_k^i \leq C_i(\ell)\, \forall k$. Intuitively, it represents the smallest criticality
level such that no task exceeded its worst-case execution time for that particular criticality level.
If no such $\ell$ exists, then the scenario is said to be \emph{erroneous}, since at least one task
exceeded its worst-case execution time at its own criticality level ($c_j^i > C_i(\chi_i)$).

\begin{definition}
A schedule for a scenario $s^t$ of criticality level $\ell$ is feasible if every job $J_j^i \in s_i^t$,
$\forall i$, with $\chi_i \geq \ell$, receives execution time $c_j^i$ between its release time and its
deadline.
\end{definition}
This definition implies that mixed-criticality scheduling is only concerned with respecting temporal
constraints of tasks of which criticality level is higher than or equal to the criticality level of
the scenario.

\begin{definition}
An on-line scheduling policy is correct for a task set $\tau$ if, for any non-erroneous scenario of
$\tau$, the policy generates a feasible schedule.
\end{definition}
Since an on-line scheduling policy discovers the exact execution times of the jobs when they complete
their execution, the criticality level of the scenario is not known beforehand. All available jobs
are therefore initially scheduled. As soon as a job exceeds its worst-case execution time of level
$\ell-1$, the criticality of the scenario is raised to level $\ell$, all available jobs of criticality 
lower than $\ell$ are dropped, and future requests from tasks of which criticality is lower than $\ell$
are no longer taken into consideration.

\begin{definition}
A mixed-criticality task set $\tau$ is MC-schedulable if it admits a correct on-line scheduling policy.
\end{definition}

Figure \ref{extendedModel} extends the traditional task state model \cite{osekvdx} with additional
transitions, or transition labels, to take into consideration the task suspension mechanism.

\begin{figure}[h!]
	\begin{center}
		\begin{tt}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
				            thick,every label/.style={draw,black}]
		\tikzstyle{every state}=[fill=white,rectangle,draw=black,text=black, rounded corners = 3]
		\tikzstyle{trait}=[rectangle,fill=white,inner sep=0pt,rounded corners = 10]
		\tikzstyle{traite}=[ellipse,fill=white,inner sep=0pt,rounded corners = 10]
		  
		\node[state]		(0)		at (10,2.5)					{Running};
		\node[state]		(1)		at (10,-2.5)				{Ready};
		\node[state]		(2)		at (14,0)					{Suspended};
		
		\path 	(0)	edge[bend left]			node[below]				{}	(1)
				(0)	edge[]					node[above,sloped]		{}	(2)
				(1)	edge[bend left]			node[below]				{}	(0)
				(2)	edge[]					node[below]				{}	(1)
				(1)	edge[in=290,out=340]	node[above, sloped]		{}	(2)
				(2)	edge[loop right]		node[below]				{}	(2);
				
		
		\node[trait]		(3)		at (16.1,0.85)					{	\begin{tabular}{c}
									{\footnotesize Activate }\\
									{\footnotesize $[\ell > \chi_i]$}
								\end{tabular}
							};
		\node[traite]		(3)		at (13,-2.5)					{\footnotesize Drop};
		
		\node[trait]		(3)		at (12.1,-1.2)					{	\begin{tabular}{c}
									{\footnotesize Activate }\\
									{\footnotesize $[\ell \leq \chi_i]$}
								\end{tabular}
							};
		\node[traite]		(3)		at (12,1.25)					{\footnotesize Terminate};
		\node[traite]		(4)		at (11,0)						{\footnotesize Preempt};
		\node[traite]		(5)		at (9,0)						{\footnotesize Start};
		
		%\node[traite]		(6)		at (11.5,4)					{\footnotesize [Condition]};
		%\node[traite]		(7)		at (13.5,4)					{\footnotesize $\langle$Event$\rangle$};

		  
		\end{tikzpicture}
		\end{tt}
	\end{center}
\caption{Extended task state model}\label{extendedModel}
\end{figure}


A task can be in one of the following states:
\begin{itemize}
	\item[$\bullet$] Running: a task $\tau_i$ is running when the scheduler chooses $\tau_i$ for
          execution. While being in this state, the instructions of $\tau_i$ are processed by the CPU.
          At any time, at most one task will be in the running state.
	\item[$\bullet$] Ready: a task $\tau_i$ reaches the ready state if all functional prerequisites 
          for a transition into the running state exist, and $\tau_i$ is only waiting for election from
          the scheduler.
	\item[$\bullet$] Suspended: a task $\tau_i$ reaches the suspended state if it is currently not
          a candidate for election from the scheduler.
\end{itemize}
Transitions have the following meaning:
\begin{itemize}
	\item[$\bullet$] Start: The task is selected by the scheduler to be executed.
	\item[$\bullet$] Preempt: The task is preempted by another task.
	\item[$\bullet$] Terminate: The task successfully completes its execution.
	\item[$\bullet$] Drop: The system's criticality reaches a level higher than the task's 
          criticality, and it is therefore dropped. Notice that this can only happen when $\tau_i$
          is preempted by a higher priority and higher criticality task.
	\item[$\bullet$] Activate [$\ell > \chi_i$]: The task wants to release a new job, but the
          system's criticality is currently higher than the task's criticality.
	\item[$\bullet$] Activate [$\ell \leq \chi_i$]: The task wants to release a new job, and
          the system's criticality is currently lower than or equal to the task's criticality.
\end{itemize}

\begin{exmpl}\label{ex:mcTaskSet1}
Let us consider a triple-criticality ($\mathnormal{L} = 3$) task set $\tau = \{\tau_1, \tau_2, \tau_3\}$ where:
\begin{itemize}
	\item[$\bullet$] $\tau_1: \{0, 5, 5, 1, (1,1,1)\}$;
	\item[$\bullet$] $\tau_2: \{0, 7, 7, 2, (1,3,3)\}$;
	\item[$\bullet$] $\tau_3: \{0, 8, 8, 3, (1,3,7)\}$.\\
\end{itemize}
Any scenario where no job exceeds an execution time of 1 is of criticality 1, while scenarios where
jobs of tasks $\tau_2$ and $\tau_3$ execute for at least 2 time unit and at most 3 time units are of
criticality 2. If some jobs of task $\tau_3$ execute between 4 and 7 time units, then this defines a
scenario of criticality 3. Finally, all others scenarios are erroneous.
\end{exmpl}

\begin{definition}
The worst-case response time $r_i$ of task $\tau_i$ is the maximum duration to execute $\tau_i$, taking
into account the execution of all tasks of higher priority than $\tau_i$. We have $r_i \geq C_i(\chi_i)$.
\end{definition}

\begin{definition}
A \emph{critical instant} for a task $\tau_i$ is defined to be an instant at which $\tau_i$ releases
a job and the response time of that job will be the highest among all jobs released by $\tau_i$.
\end{definition}

Liu \cite{Liu2000} showed that, when considering a traditional task set scheduled with fixed
priority (FP), a critical instant for a task $\tau_i$ occurs whenever $\tau_i$ releases a job
simultaneously with all higher priority tasks $\tau_j$. As shown in theorem \ref{theo:criticalInstant},
this can be extended to mixed-criticality task sets.

\begin{theorem}\label{theo:criticalInstant}
Let $\tau = \{\tau_1, \tau_2, ..., \tau_n\}$ be a sporadic mixed-criticality task set scheduled with a
fixed priority policy. A critical instant for a task $\tau_i$ occurs whenever $\tau_i$ releases a job
simultaneously with all higher priority tasks $\tau_j$.
\end{theorem}
\begin{proof}
Since each task $\tau_i$ is certified for a level of assurance that is equal to its own criticality
$\mathcal{X}_i$, we can map each task $\tau_j \in \tau$ onto a traditional task $\tau_j'$, where the
worst-case execution time of $\tau_j'$ is equal to $C_j(\mathcal{X}_j)$. Since we obtain a traditional
task set, the results proved by Liu \cite{Liu2000} can be reused, and this proves our theorem.
\end{proof}

\subsection{Determining MC-Schedulability}\label{sufficientCondition}
Baruah et al. \cite{bbalmms} proved that the MC-schedulability problem, applied to a finite set of jobs,
is $\mathsf{NP}$-hard. Instead of relying on exact MC-schedulability conditions, we will focus on a
\emph{sufficient} condition that can be verified in polynomial time. \\
The algorithm used to verify this condition, introduced by Vestal \cite{Vestal2007},
determines off-line a total ordering of the tasks in $\tau$. Each task is assigned a distinct priority
and jobs inherit the priority of the task that released them. At each instant $t$, the scheduler executes
the available job with the highest priority. The priority assignment is realized using the Audsley approach
\cite{Audsley_1991} based on the following definition (priority $n$ is the lowest priority and priority
1 the highest).

\begin{definition}
A task $\tau_i$ in a mixed-criticality task set $\tau$ is said to be \emph{viable at the lowest priority
  level} if all of the following conditions are satisfied:
	\begin{enumerate}
		\item the lowest priority is assigned to $\tau_i$;
		\item all other tasks in $\tau$ can be assigned any priority provided that these priorities
                  are higher than the priority assigned to $\tau_i$;
		\item every job released by $\tau_i$ meets its deadline when it is executed for at most
                  $C_i(\chi_i)$ time units and all other tasks $\tau_j$ in $\tau$ generate jobs that run
                  for at most $C_j(\chi_i)$ time units.
	\end{enumerate}
\end{definition}

%\begin{algorithm}[H]
\begin{algorithm}
%\SetLine
\KwIn{$\tau$}
$\mathsf{pr} \leftarrow |\tau|$ \\
\While{$\tau \neq \varnothing$}{
	\If{no task $\tau_i \in \tau$ is viable at the lowest priority level}{
		\textbf{return} error\;
	}
	\Else{
		Let $\tau_i$ be a task viable at the lowest priority level\;
		assign $\tau_i$ the lowest priority $\mathsf{pr}$\;
		$\tau \leftarrow \tau\setminus\{\tau_i\}$\;
		$\mathsf{pr} \leftarrow \mathsf{pr}-1$\;
	}
}
\caption{Priority Assignment}\label{alg:priorityAssignment}
\end{algorithm}

The priority order is then constructed iteratively using algorithm \ref{alg:priorityAssignment}.
If such a priority order exists, i.e. if a task viable at the lowest priority level is found at
each step (w.r.t. the remaining set of unassigned priority tasks), then each task $\tau_i$ is guaranteed
to meet its deadline if it executes for no more than $C_i(\chi_i)$  time units and no task $\tau_j$
with a higher priority than $\tau_i$ executes longer than $C_j(\chi_i)$ time units.

Because the priority of a task is based on its own criticality level, we say that a task set is
\emph{Own Criticality Based Priority (or OCPB)-schedulable} if we can find a complete ordering of
the tasks using algorithm \ref{alg:priorityAssignment}.

\begin{theorem}\label{theo:mcschedulability}
If a mixed-criticality task set $\tau$ is OCBP-schedulable on a given processor, then $\tau$ is
MC-schedulable on the same processor.
\end{theorem}
\begin{proof}
This proof is mainly inspired by the one presented by Baruah et al. \cite{baruah_li_stougie}. Suppose
$\tau$ is OCBP-schedulable, and let, after renaming of the tasks, $\tau_1 \vartriangleright
\tau_2 \vartriangleright ... \vartriangleright \tau_n$ represent a complete ordering that provides
evidence of this. Let $\tau_i$ be any task in this priority ordering. In order to demonstrate
MC-schedulability, one has to prove that each job released by $\tau_i$ can receive $C_i(\chi_i)$ units
of execution between its release date and its deadline in any scenario of criticality level $\chi_i$
or lower. But in any such scenario, each job released by a task $\tau_j$ executes for no more than
$C_j(\chi_i)$ time units. And the OCBP-schedulability of $\tau$ with priority ordering
$\tau_1 \vartriangleright \tau_2 \vartriangleright ... \vartriangleright \tau_n$ implies that each job
released by task $\tau_i$ will receive $C_i(\chi_i)$ time units if no job released by a task
$\tau_j \in \{\tau_1, \tau_2, ..., \tau_{i-1}\}$ executed longer that $C_j(\chi_i)$ time units. It
follows that $\tau_i$ will indeed meet its deadline in any scenario of criticality $\chi_i$ or lower.
\end{proof}

\begin{exmpl}\label{ex:mcTaskSet2}
Let us consider again the task set $\tau$ presented in example \ref{ex:mcTaskSet1}. A priority
assignment which makes $\tau$ OCBP-Schedulable is $\tau_3 \vartriangleright \tau_2 \vartriangleright \tau_1$.
Indeed, since $\tau$ is a periodic task set with simultaneous release times, $t=0$ is a critical 
instant for every task, and it follows that one does only need to consider the response time of
the first job of each task to determine schedulability. Task $\tau_1$ can be assigned the lowest
priority because its response time does not exceed $3 < D_1$ time units when neither of the other
tasks exceed their worst-case execution time at criticality level 1. Task $\tau_2$ can be assigned
the middle priority because its response time does not exceed $6 < D_2$ time units when task 
$\tau_3$ does not exceed its worst-case execution time at criticality level 2. Finally, task
$\tau_3$ can be assigned the highest priority because its response time does not exceed $7 < D_3$
time units. Since $\tau$ is OCBP-Schedulable, it follows from theorem \ref{theo:mcschedulability}
that $\tau$ is also MC-Schedulable.
\end{exmpl}

In the following section, we will focus on task sets which are OCBP-schedulable. Furthermore,
Section\ref{sec:allowanceFP} assumes fixed priorities to be assigned according to algorithm
\ref{alg:priorityAssignment}.

%%%%%%%%%% delayingCriticalityRising %%%%%%%%%%
\subsection{Delaying Criticality Rising}\label{delayingCriticalityRising}
Traditional mixed-criticality scheduling policies stop a job as soon as the criticality 
of the scenario becomes higher than the criticality level of that job. In particular, as
soon as the criticality level of the scenario reaches $\ell$, all tasks $\tau_i$ with
$\chi_i < \ell$ are suspended, even though they could still run for some time without
compromising the temporal constraints of the active jobs of criticality at least $\ell$.
This is because the OCBP-schedulability condition presented in Section~\ref{sufficientCondition}
can no longer guarantee their temporal constraints. \\
We argue that this principle is too restrictive and can be relaxed under certain conditions.
In the case of task sets that are OCBP-schedulable, the following lemma already allows us to
reduce the set of tasks that need to be suspended.

\begin{lemma}\label{lem1}
Let $\tau$ be a mixed-criticality task set which is OCBP-schedulable, and $\tau_i$ be any
task in the priority order constructed by algorithm \ref{alg:priorityAssignment}. When a job
released by $\tau_i$ reaches its worst-case execution time at criticality level
$\ell \leq \mathcal{X}_i$, it is never necessary to suspend the tasks
$\tau_j$ with $\chi_j < \ell$ that have a higher priority than $\tau_i$.
\end{lemma}
\begin{proof}
Let $\tau_i \in \tau$ be a task that reaches its worst-case execution time at criticality
level $\ell$. Since $\tau$ is OCBP-schedulable, each job released by $\tau_i$ can execute
up to $C_i(\chi_i)$ time units and still meet its deadline if no higher priority task
$\tau_j$ executes longer than $C_j(\chi_i)$. Let $\tau_p$ be a higher priority task, whose
criticality $\chi_p$ is lower than $\chi_i$. Since $\chi_i > \chi_p$, $C_p(\chi_i) = C_p(\chi_p)$,
and the OCBP-schedulability condition guarantees that each job released by $\tau_i$ will meet
its deadline, even though lower criticality tasks that have a higher priority than $\tau_i$
execute for their worst-case execution time at their own criticality level. Therefore, only
the lower criticality tasks that have a lower priority than $\tau_i$ need to be suspended. \\
\end{proof}

Lemma \ref{lem1} prevents us from aborting lower criticality tasks that would still have enough
time to fully complete their execution, but we could still go further and allow lower criticality
tasks that would normally have to be suspended immediately, to proceed with their execution as
long as all the jobs are able to meet their deadlines.  This would require to determine for how
long tasks $\tau_i$ of criticality at least $\ell+1$ could delay the suspension of lower criticality
tasks in case of a level-$\ell$ execution overrun. We call this duration the $\ell$-\emph{allowance}
of task $\tau_i$, which is denoted by $A_i(\ell)$. Based on this allowance, we can determine the
latest instant at which criticality has to be raised, since there is no more hope that all lower
criticality tasks meet their deadline after. \\

%%%%%%%%%% Allowance %%%%%%%%%%
\subsection{Allowance}\label{sec:allowance}
The $\ell$-allowance of a task $\tau_i$, the criticality of which is at least $\ell+1$, consists
in computing the margin in its worst-case execution time of level $\ell$. It represents the maximum
execution time that can be added to $C_i(\ell)$ such that lower criticality tasks $\tau_j$ can
still meet their deadline if they execute up to $C_j(\chi_j)$ time units and $\tau_i$ executes
for at most $C_i(\ell) + A_i(\ell)$ time units. The $\ell$-allowance of tasks of which the 
criticality is less than or equal to $\ell$ will be set to zero.

%%%%%%% Concepts and notations %%%%%%%
\subsubsection{Concepts and Notations}
Throughout this section, the following concepts and notations are used:
\begin{itemize}
	\item[$\bullet$] $U_{\ell} = \sum\limits_{\tau_p \, | \, \chi_p \geq \ell} \dfrac{C_p(\ell)}{T_p}$
          is the processor utilization factor at criticality level $\ell$.
	\item[$\bullet$] FP$_\text{MC}$ denotes the preemptive \emph{Fixed Priority Highest
            Priority First} algorithm, with a priority assignment determined by algorithm
          \ref{alg:priorityAssignment}, and which only schedules tasks of criticality greater
          than or equal to the criticality of the scenario.
	\item[$\bullet$] A fault model denoted $\dfrac{k}{W}$. This means that for each 
          criticality level $\ell \in [0, L]$, at most $k$ tasks ($0\leq k \leq n$) will be
          subject to a level-$\ell$ execution overrun, over a sliding window of size $W$. We
          set $W\leq\min\limits_i(T_i)$ to prevent a task to be subject to multiple level-$\ell$
          execution overruns in the same window. Indeed, it might be too pessimistic to assume
          that each task of criticality at least $\ell+1$ will be subject to a level-$\ell$ execution
          overrun. The value of $k$ is bound to the task set and can be obtained statistically by 
          observing the real number of level-$\ell$ execution overruns when executing the system,
          or according to certification constraints.
	\item[$\bullet$] For any task $\tau_i$ scheduled with FP$_\text{MC}$, and a fault model $\dfrac{k}{W}$:
		\begin{itemize}
			\item[$\circ$] $\mathrm{hp}(i)$: the set of tasks $\tau_j$ having a priority higher than $\tau_i$;
			\item[$\circ$] $\mathrm{hp}^R(i)$: the set of available jobs released by tasks $\tau_j \in \mathrm{hp}(i)$;
			\item[$\circ$] $\mathrm{hp}_{k-1}(i,\ell)$: the set of at most $k-1$ tasks $\tau_j$ having a priority higher than $\tau_i$, and a criticality $\chi_j > \ell$. If more than $k-1$ tasks satisfy these conditions, we select the more recurrent tasks, i.e. the $k-1$ tasks of which the periods are the smallest;
			\item[$\circ$] $\mathrm{lp}(i,\ell)$: the set of tasks $\tau_j$ having a priority lower than $\tau_i$ and a criticality $\chi_j \geq \ell$.
			\item[$\circ$] $\mathrm{lp}^R(i,\ell)$: the set of available jobs released by tasks $\tau_j \in \mathrm{lp}(i,\ell)$;
			\item[$\circ$] The following equation allows to compute an upper bound on the worst-case response time $r_i$ of $\tau_i$:
				\begin{equation}\label{equ:responseTime}
					r_i = C_i(\chi_i) + \sum\limits_{\tau_p \in \mathrm{hp}(i)} \left\lceil \dfrac{r_i}{T_p} \right\rceil \times C_p(\chi_i)
				\end{equation}
		\end{itemize}
\end{itemize}
In the following, we will assume a fair distribution of the $\ell$-allowance between all $k$ 
tasks of criticality at least $\ell+1$, i.e. each task of criticality at least $\ell+1$ is
granted the same amount of $\ell$-allowance.

\begin{proposition}
Let $\tau$ be an MC-schedulable mixed-criticality task set. It follows that for any criticality 
level $\ell$, the following condition is satisfied:
	\begin{equation}
		U_{\ell} \leq 1
	\end{equation}
\end{proposition}

%%%%%%% Allowance with FP %%%%%%%
\subsubsection{Allowance with FP}\label{sec:allowanceFP}
The following theorem shows how to compute the static $\ell$-allowance of a task $\tau_i$ with FP 
scheduling. The priorities assigned to the tasks are the ones that bear witness to the OCBP-schedulability
condition. The computation of $A_i(\ell)$ is based on the worst-case response time of the tasks with
the allowance use.
\begin{theorem}
Let $\tau = \{\tau_1, \tau_2, ..., \tau_n\}$ be a set of $n$ mixed-criticality periodic tasks scheduled
with FP, and let $\tau_1 \vartriangleright \tau_2 \vartriangleright ... \vartriangleright \tau_n$ be
the priority order that yields OCBP-schedulability. The maximum $\ell$-allowance that can be granted
to a task $\tau_i$, of criticality at least $\ell+1$, with a fair allowance distribution, and a fault
model $\frac{k}{W}$, is the minimum positive value of $A_i(\ell)$ satisfying the following equations:
	\begin{equation}\label{equ:cond3}
		C_i(\ell) + A_i(\ell) \leq C_i(\chi_i)
	\end{equation}
	
	\begin{equation}\label{equ:cond1}
		U_{\ell} + \sum\limits_{\tau_p \in \mathrm{hp}_{k-1}(i,\ell) \cup \tau_i} \dfrac{A_i(\ell)}{T_p}\leq 1
	\end{equation}
		
	\begin{equation}\label{equ:cond2}
		\begin{split}
			&\forall \tau_j \in \mathrm{lp}(i,\ell): \\
			&r_j^* = C_j(\chi_j) + \sum\limits_{\tau_p \in \mathrm{hp}(j)} \left\lceil \dfrac{r_j^*}{T_p} \right\rceil \times C_p(\chi_j) \\
			&\quad\quad\quad\quad\quad+ \sum\limits_{\tau_p \in \mathrm{hp}_{k-1}(i,\ell) \cup \tau_i} \left\lceil \dfrac{r_j^*}{T_p} \right\rceil \times A_i(\ell) \leq D_j
		\end{split}
	\end{equation}
	
\end{theorem}
\begin{proof}
Equation (\ref{equ:cond3}) grants that the allowance should not allow a task to execute for
more than its worst-case execution time at its own criticality level. Equation (\ref{equ:cond1})
is the processor utilization condition at criticality level $\ell$ updated to take into account
the $\ell$-allowance consumption of at most $k$ tasks. We want to ensure that lower cri\-ti\-cality
tasks still meet their deadline in case of a level-$\ell$ execution overrun  of at most $k$
higher criticality tasks, for a duration limited by the allowance, which leads us to equation
(\ref{equ:cond2}). This equation represents the worst-case response time of $\tau_j\in lp(i,l)$
when it executes at its highest criticality level, higher priority tasks $\tau_p$ including
$\tau_i$ execute for at most $C_p(\chi_j)$ time units, and $k$ higher priority tasks $\tau_r$
including $\tau_i$ consume their $\ell$-allowance.
This equation is only verified for tasks having a lower priority than $\tau_i$ because of Lemma\ref{lem1}.
\end{proof}

It follows that tasks with a criticality $\ell$ should not be dropped until a higher priority
task $\tau_i$ with criticality $\chi_i > \ell$ executes for more than $C_i(\ell) + A_i(\ell)$
time units.

%%%%%%% Allowance with EDF %%%%%%%
\subsubsection{Allowance with EDF}
The allowance principle that was initially introduced by Bougueroua et al. \cite{bougueroua_george_midonnet}
was applied both to Fixed Priority and Earliest Deadline First scheduling strategies, and dealt
only with traditional task sets. Nevertheless, as the following proposition states, the Earliest
Deadline First scheduling strategy is not optimal for mixed-criticality task sets.

\begin{proposition}
Every mixed-criticality task set $\tau$ that is MC-Schedulable does not necessarily admit Earliest 
Deadline First as a correct on-line scheduling policy. In other words, EDF is not optimal for
mixed-criticality task sets.
\end{proposition}
\begin{proof}
Let $\tau$ be the mixed-criticality task set which was proven to me MC-Schedulable in example
\ref{ex:mcTaskSet2}. Figure \ref{fig:edfNotOptimal} shows that the criticality of the scenario
is not raised soon enough to guarantee the temporal constraint of task $\tau_3$ when all tasks
execute for their worst-case execution time at their own criticality level.
\end{proof}
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.40]{./figures/EDFNonOptimal.png}
		\caption{Non-Optimality of EDF algorithm.}\label{fig:edfNotOptimal}
	\end{center}
\end{figure}
Since our study will only focus on task sets $\tau$ which are OCBP-Schedulable, and because this
property does not allow us to determine if Earliest Deadline first is a correct scheduling policy
for $\tau$, we will not implement the allowance principle for the Earliest Deadline First algorithm.

%%%%%%% Allowance Domain %%%%%%%
\subsubsection{Allowance Domain}\label{sec:allowanceDomain}
Section \ref{sec:allowanceFP} showed how to compute a fair $\ell$-allowance for task sets scheduled
with FP. Nevertheless, one might want to use a different allocation rule for the $\ell$-allowance.
In this section, we present a way of representing all possible values for the $\ell$-allowance, by
characterizing the space of feasible $\ell$-allowance.

\begin{definition}
Let $A(\ell) = \{A_1(\ell), A_2(\ell), ..., A_n(\ell)\}$ be the design variables for a mixed-criticality
task set $\tau$. Then, the feasibility region in the $A(\ell)$-space is the set of values of $A(\ell)$
such that $\tau$ remains feasible, if every task $\tau_i \in \tau$ consumes up to $A_i(\ell)$ extra
time units at criticality level $\ell$.
\end{definition}

Bini et al. \cite{bini_buttazzo} introduced a schedulability condition for periodic task sets. This
condition has been adapted to mixed-criticality task sets in the following theorem.
\begin{theorem}
A periodic mixed-criticality task set $\tau$ is schedulable under fixed priorities if and only if:
	\begin{multline}
		\forall i = 1,...,n, \; \exists t \in \mathrm{schedP}_i \, \text{such that} \, \\
		C_i(\chi_i) + \sum_{\tau_p \in \mathrm{hp}(i)} \left\lceil \dfrac{t}{T_p} \right\rceil C_p(\chi_i) \leq t \label{eq:domain}
	\end{multline}
where $\mathrm{schedP}_i$ is a set of scheduling points defined as $\mathrm{schedP}_i = \mathcal{P}_{i-1}(D_i)$, and $\mathcal{P}_i(t)$ is defined as follows:
	\begin{equation}
		\left\{
			\begin{split}
				&\mathcal{P}_0(t) \\
				&\mathcal{P}_i(t) = \mathcal{P}_{i-1}\left(\left\lfloor \frac{t}{T_i} \right\rfloor\right) \cup \mathcal{P}_{i-1}(t)
			\end{split}
		\right.
	\end{equation}
By using a compact notation and logical operators, equation \ref{eq:domain} can be rewritten as:
	\begin{equation}
		\bigwedge \limits_{i = 1,...,n} \bigvee \limits_{t \in \mathrm{schedP}_i} n_i \cdot C_i \leq t \label{eq:domainCompact}
	\end{equation}
where $n_i = \left(\left\lceil \frac{t}{T_1} \right\rceil, \left\lceil \frac{t}{T_2} \right\rceil, ..., \left\lceil \frac{t}{T_{i-1}} \right\rceil, 1\right)$ and $C_i = \left(C_1(\chi_i), C_2(\chi_i), ..., C_i(\chi_i)\right)$
\end{theorem}

Since what we want to represent is the feasibility region in the $A(\ell)$-space, we need to
introduce the allowance variables. Equation \ref{eq:domainCompact} must be extended so as to
consider a level-$\ell$ allowance for each task of criticality at least $\ell$.

\begin{theorem}
Let $\tau$ be a mixed-criticality task set. The feasibility region in the $A(\ell)$-space is
computed as follows:
	\begin{equation}
		\bigwedge \limits_{\tau_i \, | \, \chi_i \geq \ell} \bigvee \limits_{t \in \mathrm{schedP}_i} \left\lgroup n_i \cdot C_i^{\ell} \leq t \wedge A_i(\ell) \leq C_i(\chi_i) - C_i(\ell) \right\rgroup \label{eq:domainCompactMC}
	\end{equation}
where $C_i^{\ell} = \left(C_1(\ell) + A_1(\ell), C_2(\ell) + A_2(\ell), ..., C_i(\ell) + A_i(\ell)\right)$.
\end{theorem}
\begin{proof}
Equation \ref{eq:domainCompactMC} simply extends Equation \ref{eq:domainCompact} by allowing
each task to consume up to its worst-case execution time at criticality level $\ell$ plus its 
allowance, and prevents the allowance to allow a task to exceed its worst-case execution time
at its own criticality level.
\end{proof}

\begin{exmpl}
Let us compute the $A(\ell)$-spaces, for $\ell = 1,2,3$, of the mixed-criticality task set
presented in example \ref{ex:mcTaskSet1}. Let us first rename the task according to the priority
assignment. Task $\tau_3$ becomes task $\tau_1$, and task $\tau_1$ becomes task $\tau_3$. 
Task $\tau_2$ remains unchanged.
\begin{itemize}
	\item[$\bullet$] $A(1)$-space: The set of scheduling points to consider is given in
          table \ref{schedP::1}. \\
		\begin{figure}[h!]
		\begin{center}
			\begin{tabular}{|c||c|}
				\hline
				$\tau_i$ & $\mathrm{schedP}_i$ \\
				\hline
				\hline
				$\tau_1$ & \{8\} \\
				\hline
				$\tau_2$ & \{7\} \\
				\hline
				$\tau_3$ & \{5\} \\
				\hline
			\end{tabular}
		\end{center}
		\caption{Scheduling points in $A(1)$-space}\label{schedP::1}
		\end{figure}
		
		The $A(1)$-space is then represented by the set of equations \ref{equations::1},
                and is depicted in green in Figure \ref{space::1}, with $A_3(1)=0$.
		\begin{equation}
			\left\{
				\begin{split}
					& A_1(1) \leq 7 \wedge A_1(1) \leq 6 \\
					& A_2(1) + A_1(1) \leq 5 \wedge A_2(1) \leq 2 \\
					& A_3(1) + A_2(1) + A_1(1) \leq 2 \wedge A_3(1) \leq 0
				\end{split}\label{equations::1}
			\right.
		\end{equation}
		
	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.25]{./figures/A1_space.png}
		\caption{$A(1)$-space}\label{space::1}
	\end{figure}
	\item[$\bullet$] $A(2)$-space: The set of scheduling points to consider is given in 
          table \ref{schedP::2}. \\
		\begin{figure}[h!]
		\begin{center}
			\begin{tabular}{|c||c|}
				\hline
				$\tau_i$ & $\mathrm{schedP}_i$ \\
				\hline
				\hline
				$\tau_1$ & \{8\} \\
				\hline
				$\tau_2$ & \{7\} \\
				\hline
			\end{tabular}
		\end{center}
		\caption{Scheduling points in $A(1)$-space}\label{schedP::2}
		\end{figure}
	
	
		The $A(2)$-space is then represented by the set of equations \ref{equations::2}.
		
		\begin{equation}
			\left\{
				\begin{split}
					& A_1(2) \leq 5 \wedge A_1(2) \leq 4 \\
					& A_2(2) + A_1(2) \leq 1 \wedge A_2(2) \leq 0
				\end{split}\label{equations::2}
			\right.
		\end{equation}
		Leading to $A_1(2)=1$ and $A_2(2)=0$.
	\item[$\bullet$] $A(3)$-space: The set of scheduling points to consider is given in
          table \ref{schedP::3}. \\
		\begin{figure}[h!]
		\begin{center}
			\begin{tabular}{|c||c|}
				\hline
				$\tau_i$ & $\mathrm{schedP}_i$ \\
				\hline
				\hline
				$\tau_1$ & \{8\} \\
				\hline
			\end{tabular}
		\end{center}
		\caption{Scheduling points in $A(1)$-space}\label{schedP::3}
		\end{figure}
		
		The $A(3)$-space is then represented by equation \ref{equation::3}. As one can
                easily see, no strictly positive allowance can satisfy this equation, leading
                to $A_1(3)=0$.
		\begin{equation}
			A_1(3) \leq 1 \wedge A_1(3) \leq 0
		\end{equation}\label{equation::3}
\end{itemize}
\end{exmpl}

%%%%%%%%%% Allowance Implementation %%%%%%%%%%
\subsection{Allowance implementation}\label{sec:allowanceImplementation}
We now present an on-line mechanism for the allowance management than can be implemented using
traditional timers. We call this mechanism the \emph{Latest Execution Time (LET)}.

\begin{definition}
Let $t_i$ be the request time of the $k^\text{th}$ job $J_k^i$ released by task $\tau_i$, with
$\chi_i \geq \ell$. $\mathsf{LET}_i^{\ell}(t_i)$ is defined as the Latest Execution Time $J_k^i$,
if subject to a level-$\ell$ execution overrun, can proceed with its execution without having
to drop lower criticality tasks for the system to still respect temporal constraints of tasks
of criticality greater than or equal to $\ell$.
\end{definition}

We will now show how to compute the $\mathsf{LET}$ of a task $\tau_i$, and this $\mathsf{LET}$
will be used to determine the instant at which lower criticality tasks should be dropped.

\begin{definition}
Let $J_k^i$ be the $k^\text{th}$ job released at time $t_i$ by task $\tau_i$ with criticality
$\chi_i \geq \ell$. The Latest Execution Time for $J_k^i$ at criticality level $\ell$ is computed
at time $t_i$ as follows:
	\begin{equation}
		\begin{split}
			&\forall \ell \in [1,\mathnormal{L}]: \\
			&\mathsf{LET}_i^{\ell}(t_i) = \max\left\lgroup t_i, \max\limits_{\tau_j \in \mathrm{hp}^R(i)} \left( \mathsf{LET}_j^\ell(t_j) \right) \right\rgroup \\
			&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad+ C_i(\ell) + A_i(\ell)
		\end{split}
	\end{equation}
The Latest Execution Time of jobs $J$ in $\mathrm{lp}^R(i,\ell)$ is then updated as follow:
	\begin{equation}
		\forall J \in \mathrm{lp}^R(i,\ell): \mathsf{LET}_j^{\ell}(t_j) = \mathsf{LET}_j^{\ell}(t_j) + C_i(\ell) + A_i(\ell)
	\end{equation}
\end{definition}
Based on the $\mathsf{LET}_i^{\ell}(t_i)$ of a job of which the criticality is greater than $\ell$,
we can initialize a timer in charge of detecting a level-$\ell$ execution overrun after which all
tasks of criticality lower than $\ell$ need to be dropped. We do not initialize such a timer for 
jobs of which the criticality is equal to $\ell$, since they will not be subject to a level-$\ell$
execution overrun. But we need to compute their $\mathsf{LET}$ though since it may be necessary to
compute the $\mathsf{LET}$ of other jobs. \\

\begin{exmpl}
The allowance implementation will be illustrated using example \ref{ex:mcTaskSet1}, and we will
assume that every task of criticality at least $\ell+1$ can be subject to a level-$\ell$ execution
overrun. The computed allowance as well as the Latest Execution Times of the first job of each task
are represented in table \ref{ex:table}.
\begin{figure}[h!]
	\begin{center}
		\begin{tabular}{|c||c|c|c|c|c|}
		\hline
			Task & $A_i(1)$ & $A_i(2)$ & $\mathsf{LET}_i^{1}(0)$ & $\mathsf{LET}_i^{2}(0)$ & $\mathsf{LET}_i^{3}(0)$ \\
		\hline
		\hline
			$\tau_3$ & 1 & 1 & 2 & 4 & 7 \\
		\hline
			$\tau_2$ & 1 & 0 & 4 & 7 & $\times$ \\
		\hline
			$\tau_1$ & 0 & 0 & 5 & $\times$ & $\times$ \\
		\hline
		\end{tabular}\caption{Example of three tasks with FP.}\label{ex:table}
	\end{center}
\end{figure}

Figure \ref{ex:LET0} illustrates a scenario where both $\tau_3$ and $\tau_2$ exceed their worst-case
execution time at criticality level 1, but without having to interrupt $\tau_1$, which still has enough
time left to complete its execution. If the allowance mechanism had not been used, the criticality
of the scenario would have been raised at time instant 1, thus preventing task $\tau_1$ to complete
its execution.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.45]{./figures/Let(0).png}
		\caption{$\mathsf{LET}_i^1(0)$ example.}\label{ex:LET0}
	\end{center}
\end{figure}

Similarly, Figure\ref{ex:LET1} illustrates a scenario where $\tau_3$ exceeds its worst-case execution
time at criticality level 2, but completes its execution soon enough to prevent the interruption of
task $\tau_2$, which still has enough time to complete its execution for a duration equal to its
worst-case execution time at its own criticality level. If the allowance  mechanism had not been used,
the criticality of the scenario would have been raised at time instant 3, thus preventing task
$\tau_2$ to complete its execution.
	
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.45]{./figures/Let(1).png}
		\caption{$\mathsf{LET}_i^2(0)$ example.}\label{ex:LET1}
	\end{center}
\end{figure}

Finally, Figure\ref{ex:allowanceReuse} illustrates the allowance recovery. Indeed, task $\tau_3$ was
granted an allowance of one time unit, but completes its execution before making use of it. This
allows task $\tau_2$ to make use of two extra time units to complete its execution without having
to interrupt task $\tau_1$.
	
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.45]{./figures/AllowanceReuse.png}
		\caption{Allowance recovery.}\label{ex:allowanceReuse}
	\end{center}
\end{figure}
\end{exmpl}

%%%%%%%%%% Resetting Criticality %%%%%%%%%%
\subsection{Resetting Criticality}\label{sec:resettingCriticality}
The mechanism presented in Section\ref{sec:allowanceImplementation} allows us to delay the suspension
of lower critically tasks at the latest possible time, thus avoiding to drop jobs that would still have
enough time to complete their execution. Nevertheless, in some cases, at a given criticality level,
a task might need more time to complete its execution than what the allowance offers, and the criticality
of the scenario needs to be raised. \\
In this section, we investigate the relevance of keeping lower criticality tasks suspended. We argue that,
if it is indeed ne\-cessary to suspend lower criticality tasks to guarantee temporal constraints of higher
criticality tasks, this decision is not irreversible, and there exists a time at which all tasks can
again execute concurrently, whatever their criticality level. \\

Throughout this section, we will assume that contrary to what Lemma\ref{lem1} suggests, as soon as the
scenario's criticality reaches level $\ell$, every task of criticality less than $\ell$ will be dropped.
The definitions and results we present can nevertheless be a adapted to take into account Lemma\ref{lem1}.

\begin{definition}
If the current scenario is of criticality level $\ell$, a \emph{level-$\ell$ idle time} $t$ on a processor
is defined as an instant in time, such that there are no available jobs of criticality at least $\ell$
waiting to be scheduled at time $t$.
\end{definition}

\begin{theorem}\label{theo:decreaseCriticality}
Let $\tau$ be a periodic mixed-criticality task set which is MC-Schedulable. Whenever level-$\ell$ idle
time occurs in the scheduling of $\tau$, the criticality of the scenario can be reset to its lowest level.
\end{theorem}
\begin{proof}
Let $t_{\text{idle}}$ be the first level-$\ell$ idle time in the scheduling of $\tau$ (with
$t_{\text{idle}} > 0$). This means that every job released by a task of which the criticality is at least
as high as the scenario criticality at time $t_{\text{idle}}$ completed its execution, and no job is
currently available. Let $\tau_i$ be any task in $\tau$ that had to be dropped before $t_{\text{idle}}$.
We need to show that at time $t_{\text{idle}}$, every job released by $\tau_i$ can again receive $C_i(\chi_i)$
units of execution in any scenario of criticality at most $\chi_i$. Let $J_i$ be the first job released
by $\tau_i$ at time $t_i$ ($t_i \geq t_{\text{idle}}$). We distinguish two cases:
\begin{enumerate}
	\item $t_i$ is a critical instant for $\tau_i$: it follows that the response time of $J_i$ will be
          the highest among all jobs generated by $\tau_i$. But since $\tau$ is MC-Schedulable, we know that
          $J_i$ will meet its deadline if it executes up to $C_i(\chi_i)$, and none of the other tasks
          $\tau_j$ releases a job that executes for more than $C_j(\chi_i)$ time units.
	\item $t_i$ is not a critical instant for $\tau_i$: in this case, we know that the response time
          of $J_i$ will be less than or equal to the worst response time of task $\tau_i$. But we proved
          that $J_i$ could meet its deadline if it was released at a critical instant. It follows that
          $J_i$ will also meet its deadline if it is released at a non-critical instant.
\end{enumerate}
\end{proof}

\begin{exmpl}
Let us consider again the task set $\tau$ presented in example \ref{ex:mcTaskSet1}, and a possible scenario
for this task set illustrated in Figure\ref{ex:decreaseCriticality}. The first job released by task $\tau_3$
completes its execution after 7 time units. The criticality of the scenario reached level 2 at time $t = 1$,
and level 3 at time $t = 3$, so both $\tau_1$ and $\tau_2$ were dropped. At time $t = 7$, a level-3 idle
time occurs, so the criticality level can be reset to level 1, and the jobs released by $\tau_1$ and $\tau_2$
can be considered again. Task $\tau_2$ releases its next job at time $t = 7$, but is preempted by task
$\tau_3$ at time $t = 8$. Since $\tau_3$ completed its execution after 3 time units, the criticality of the
scenario reached level 2, and task $\tau_1$ had to be dropped. But $\tau_2$ is still able to meet its deadline.
At time $t = 13$, a level-2 idle time occurs, and the criticality level can be reset again. This allows every
task to generate a job which is able to meet its deadline.
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.5]{./figures/decreaseCriticality.png}
		\caption{Priority decreasing.}\label{ex:decreaseCriticality}
	\end{center}
\end{figure}
\end{exmpl}

%%%%%%%%%% Simulations %%%%%%%%%%
\subsection{Simulations}\label{sec:simulation}
In this section we compare the performances of three possible solutions to deal with the criticality rising
that happens when scheduling mixed-criticality task sets:
\begin{enumerate}
	\item The traditional approach (TA): as soon as a task $\tau_i$ exceeds its worst-case execution 
          time at criticality level $\ell$, the criticality of the scenario is raised, and all task $\tau_j$ 
          with $\chi_j = \ell$, having a priority lower than $\tau_i$, are dropped.
	\item The traditional approach, extended with a criticality decreasing mechanism (CD): as soon as a
          task $\tau_i$ exceeds its worst-case execution time at criticality level $\ell$, the criticality
          of the scenario is raised, and all task $\tau_j$ with $\chi_j = \ell$, having a priority lower than
          $\tau_i$, are dropped. Never\-theless, as soon as a level-$\ell$ idle time occurs, the criticality
          of the system is reset to its lowest level.
	\item The traditional approach, extended with a criticality decreasing mechanism and the allowance
          concept (CD-A): as soon as a task $\tau_i$ exceeds its worst-case execution time at criticality
          level $\ell$ plus its allowance, the criticality of the scenario is raised, and all task $\tau_j$
          with $\chi_j = \ell$, having a priority lower than $\tau_i$, are dropped. Nevertheless, as soon
          as a level-$\ell$ idle time occurs, the criticality of the scenario is reset to its lowest level.
\end{enumerate}

Each job $J_k^i$ released by a task $\tau_i$ is assigned an exact duration $c_{i,k}^e$ in the interval
$[1,C_i(\chi_i)]$, using a triangular distribution. As a job usually does not complete its execution after
a very short nor a very long amount of time, but rather after an intermediate one, the use of a triangular
distribution is a way to represent this. Indeed, the latter makes it possible to concentrate most of the 
probabilities around a value called the mode. As Figure\ref{fig:triangularDistribution} depicts, for a task
$\tau_i$, the distribution lower limit is set to $0$, the distribution upper limit is set to $C_i(\chi_i)$,
and the distribution mode is equal to $\frac{C_i(\chi_i)}{3}$. This means that a job will most of the time
complete its execution after an amount of time close to 33\% of its worst-case execution time at its own
criticality level. We then generate a random number $r$ following that particular distribution, and $c_{i,k}^e = \lceil r \rceil$.
\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.3]{./figures/triangularDistribution.png}
		\caption{Triangular Distribution for task $\tau_i$.}\label{fig:triangularDistribution}
	\end{center}
\end{figure}
Each job's exact duration is completely independant of the exact duration of the previous jobs released by
the same task. A job $J_k^i$ will complete its execution as soon as the scheduler grants him a total of
$c_{i,k}^e$ time units, but this duration is not known beforehand by the scheduler. \\

We analyze task sets $\tau$ composed of a number of mixed-criticality tasks $|\tau|$ varying between 4 and 8,
and with a criticality level bounded by 4. For the CD-A approach, the number of faulty tasks is set to
$k = |\tau|$, to make sure the criticality is raised due to a task exceeding its worst-case execution time at
a particular criticality level plus its allowance, and not because the number of faulty tasks exceeds $k$.

We compared the average number of jobs that had to be dropped for the three methods. We therefore generated
100 mixed-criticality task sets, and we reiterated the simulations for various upper bounds on each task
utilization (this upper bound is refered to as $U_{\text{max}}(\tau_i)$). Figures \ref{fig:results} and
\ref{fig:resultsGraphical} depicts the results of those simulations. The x-axis represents the maximum 
utilization per task $U_{\text{max}}(\tau_i)$, while the y-axis represents the average percentage of jobs
that were dropped.
\begin{figure}[h!]
	\begin{center}
		\begin{tabular}{|c||c|c|c|}
			\hline
			$U_{\text{max}}(\tau_i)$ & TA & CD & CD-A \\
			\hline
			\hline
			0.1	& 33.4223\% & 9.3976\% & 1.4071\% \\
			\hline
			0.2 & 32.9436\% & 11.7511\% & 4.2982\% \\
			\hline
			0.3	& 34.0782\% & 13.3883\% & 6.2279\% \\
			\hline
			0.4	& 38.2408\% & 15.6153\% & 9.4358\% \\
			\hline
			0.5	& 42.7098\% & 19.4652\% & 13.052\% \\
			\hline
			0.6	& 42.3251\% & 20.0583\% & 14.2854\% \\
			\hline
			0.7	& 42.9011\% & 21.8738\% & 16.7087\% \\
			\hline
			0.8	& 48.0304\% & 23.05\% & 17.1582\% \\
			\hline
			0.9	& 50.266\% & 24.0808\% & 19.1672\% \\
			\hline
			1.0 & 50.1776\% & 26.1121\% & 21.1875\% \\
			\hline
		\end{tabular}\caption{Average percentage of jobs dropped w.r.t. $U_{\text{max}}(\tau_i)$}\label{fig:results}
	\end{center}
\end{figure}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[scale=0.57]{./figures/resultats.png}
		\caption{Average number of jobs dropped (Graphical).}\label{fig:resultsGraphical}
	\end{center}
\end{figure}

We observe that the criticality decreasing mechanism could reduce the number of dropped jobs by 25\% in
the worst case, while this reduction could reach more than 30\% when each task utilization is low. We
furthermore notice that when this mechanism is supplemented with the allowance, the number of dropped
jobs could be reduced even more by 5\% to 9\%. \\

We observe that the lower each task utilization is, the more efficient the allowance mechanism is. This
is due to the fact that a higher allowance can be granted to each task.

%%%%%%%%%% Conclusion %%%%%%%%%%
\subsection{Conclusion}\label{conclusion}
In this paper, we have studied two principles that allow us to relax the strictness of mixed-criticality
tasks scheduling using a fixed priority strategy. Those principles are the allowance on WCETs and the
criticality decreasing mechanism. We showed how the allowance could be computed using feasibility conditions
relying on the worst-case response time of a task according to a criticality level, and suggested a simple
mechanism that implements it denoted LET. We then proved that idle times could be used to decrease the
overall criticality of the system. Experiments furthermore attested that the criticality decreasing mechanism 
could reduce up to 24\% the number of jobs that had to be suspended, while the allowance could decrease
this number by an additional 8\%.




%%%% END


Changement de criticité: priorisation aux flux dont la QoS IP n'est pas la
plus élevée


\FIXME{exigences de criticité}:\\
La transmission des flux doit respecter les exigences temporelles du domaine
de criticité le plus haut

