%%
%%
%% hardware_impacts.tex for thesis in /doctorat/these/tex
%%
%% Made by Philippe THIERRY
%% Login   <Philippe THIERRYreseau-libre.net>
%%
%% Started on  Fri Mar 12 16:36:41 2010 Philippe THIERRY
%% Last update Mon Aug 30 17:00:21 2010 Philippe THIERRY


\chapter{Concepts}
\label{sec:notions}
\doMinitoc

\section{Principes généraux de la sécurité}

\subsection{Terminologie militaire de la sécurité}

\subsubsection{Les niveaux de confidentialité}

\paragraph{}
Il existe un grand nombre de niveaux de confidentialité formalisés dans le vocabulaire
militaire. Pour la France, on parle des niveaux suivants, du plus faible au
plus élevé :
\begin{itemize}
  \item Diffusion Restreinte (DR). Bien que très employé, c'est le seul niveau pour
    lequel aucune peine n'est définie dans le code pénal en cas de non-respect
    des exigences de confidentialité associées.
  \item Confidentiel Défense (CD). Premier niveau impliquant un risque pénal. Ce
    niveau peut être présent jusque dans les véhicules blindés positionnés sur
    le théâtre d'opération.
  \item Secret Défense (SD). Ce niveau est en général présent jusque dans les
    centres de contrôle militaires locaux au théâtre d'opération. Il s'agit des
    centres de contrôle militaires déployés dans les pays en guerre dans les
    bâtiments civils protégés (type aéroport) ou dans des bâtiments militaires
    proches de la zone de conflit (frégate, porte-avions).
  \item Très secret défense (TSD). Plus haut niveau de sécurité.
\end{itemize}
Il existe de plus un grand nombre de niveaux de confidentialité liés à l'OTAN
(Organisation du Traité de l'Atlantique Nord, NATO en anglais), ou à diverses organisations
impliquant une force armée. Il existe ainsi les niveaux Nato Restricted (NR),
Nato Confidential (NC), Nato Secret (NS) et encore bien d'autres niveaux. Il
existe une problématique claire de comparaison des niveaux France avec les
niveaux OTAN. Ainsi, peut-on transférer des données du niveau NS au niveau SD
? Cette problématique est encore aujourd'hui traitée au cas par cas.

\paragraph{}
Lorsqu'un niveau de confidentialité est mis en place, c'est au travers d'un
système d'information respectueux d'une politique de sécurité donnée. Cette
politique de sécurité (nommée PSSI - Politique de Sécurité d'un Système
d'Information) définit des exigences de sécurité pour assurer la
confidentialité, l'intégrité et la disponibilité (au sens sécuritaire du
terme) du domaine. Ces exigences sont mises en place sous forme:
\begin{itemize}
  \item d'un choix d'architecture réseau, logicielle et matérielle
  \item de la définition d'algorithmes de chiffrements spécifiques et de
    solutions implémentant ses algorithmes
  \item d'une définition d'un processus de gestion du domaine (protection physique et logique)
  \item d'une définition des responsabilités et des rôles de chaque
    intervenant dans le domaine
\end{itemize}
Plus le niveau de confidentialité est élevé, plus la PSSI est stricte et
complexe.

\subsubsection{Changement de niveau et problématique rouge-noir}

\paragraph{}
La terminologie dite {\it rouge/noir} est une terminologie militaire, définie
ci-après:

\PhDdefinition{domr}{Domaine rouge}{
On appelle domaine rouge un domaine de sécurité dans lequel les données
transitant ne sont pas protégées. On parle alors de données rouges. Le réseau
faisant transiter ces données dans un domaine rouge est alors appelé réseau
rouge.
}

\PhDdefinition{domn}{Domaine noir}{
On appelle domaine noir un domaine de sécurité dans lequel les données
transitant sont modifiées pour en assurer la confidentialité, l'intégrité et
la disponibilité. On dit alors que les données sont noircies.
}

\PhDdefinition{rn}{Système rouge-noir}{
Un système est dit rouge-noir lorsqu'il interconnecte un domaine rouge à un
domaine noir. Une telle interconnexion implique un élément de confiance en
bordure des domaines, en charge d'intégrer un mécanisme de protection des
données du domaine rouge avant de les transmettre au domaine noir. Cet élément
de confiance est également en charge de valider toute transmission venant du
domaine noir vers le domaine rouge.
}

En général, un
domaine noir est en charge de faire transiter des données dont le niveau de sécurité
est supérieur à celui du domaine lui-même. Ainsi, faire
transiter des données de niveau Confidentiel Défense dans un domaine de niveau
Diffusion Restreinte ne peut être fait que si ces données sont préalablement
noircies.\\
Noircir des données implique en général un chiffrement de ces dernières, mais
pas seulement. Le profil de flux peut être modifié afin de rendre difficile
la détection du type de communication sur la base de son profil \cite{berk2005detection}. Du bruit peut
ainsi être généré et du {\it padding} incorporé afin d'empêcher toute déduction sur
simple écoute du flux noir.

\paragraph{}
La problématique de passage d'un domaine rouge à un domaine noir est appelé
problématique rouge/noir. On la retrouve principalement dans les réseaux radio
militaires, mais reste un principe général applicable à toute interconnexion
de réseaux militaires.

\subsubsection{Niveaux de confidentialité et droit d'en connaître}

\PhDdefinition{besoinden}{Besoin d'en connaître}{
On appelle droit d'en connaître l'ensemble de connaissances minimum suffisant
pour un sujet (une personne ou encore un processus logiciel) pour mener à bien
une tâche.
}

\PhDdefinition{droitden}{Droit d'en connaître}{
On appelle droit d'en connaître l'ensemble de connaissances formalisé
dans une politique de gestion des droits afin de répondre au(x) besoin(s) d'en
conaître d'un sujet.
}


\paragraph{}
Ainsi, une personne ayant une habilitation Confidentiel Défense sur
un projet donné n'a pas besoin pour autant de connaître l'ensemble des
informations classifiées Confidentiel Défense de ce dernier. On lui fournit alors
uniquement les informations nécessaires à son travail, appelé droit d'en
connaître. Le principe est le même pour les processus logiciels faisant du
traitement sur des données d'un niveau de confidentialité donné.

\subsection{Fuite d'informations: Canaux cachés et canaux auxiliaires}

\paragraph{}
Assurer la confidentialité est une tâche complexe. En effet, limiter l'accès à
la donnée n'est pas suffisant pour assurer sa protection. Ainsi, il est
possible de récupérer des informations sur la donnée de plusieurs manières,
sans y accéder directement. On parle alors de canal auxiliaire ou de canal
caché, selon la méthode.

\subsubsection{Canal auxiliaire et étude comportementale}

\paragraph{}
\PhDdefinition{canalaux}{Canal Auxiliaire}{
On appelle canal auxiliaire un élément comportemental lié au fonctionnement de
la solution permettant à un attaquant, de manière non-invasive (e.g. via une
mesure de temps), de déterminer des informations sur un élément auquel
il n'a pas les droits d'accès.
}

La plupart des études sur le sujet sont dans le domaine de la
cryptanalyse \cite{sidechancube}\cite{sidechan}. Ces
différentes études démontrent la capacité, au travers de l'étude de la consommation électrique
des différents éléments d'un processeur \cite{sidechancube} ou via l'étude de
l'impact des caches processeurs sur le comportement temporel de
l'algorithmique \cite{sidechan}, de récupérer des données sur la clef
de chiffrement utilisée.\\
Le risque lié au canal auxiliaire est connu depuis longtemps et a permis de
démontrer que des algorithmes cryptographiques réputés sûrs restent fragiles à
ce type d'attaque. C'est le cas par exemple d'AES \cite{aes} ou de
Serpent \cite{serpent}.

\subsubsection{Canal caché pour le transfert de données}
\label{subsubsec:canaux_caches}

\paragraph{}
\PhDdefinition{canalcac}{Canal caché}{
On appelle canal caché tout élément logiciel ou matériel dont il est possible
de détourner l'usage afin de pouvoir faire transiter volontairement de
l'information vers un sujet n'ayant pas les droits d'accès à celle-ci.
}

L'usage d'un canal caché
implique un usage volontaire de part et d'autre pour émettre les données et
pour les recevoir. Il est donc invasif, contrairement aux canaux auxiliaires.\\
Un exemple typique est de s'appuyer sur la période intertrame d'un flux
réseau donné pour fournir une information \cite{berk2005detection}. En faisant varier la période, il
est possible d'émettre de l'information, qui peut être ainsi interprétée par un
récepteur. De manière générale, les canaux cachés permettent l'émission de
données de faible taille, car leur débit est en général très faible. Ils sont
cependant tout à fait suffisants pour faire transiter des données de type
cryptographie (clefs symétriques et/ou asymétriques) ou des messages courts.\\
Le principe d'exploitation d'un canal caché est également utilisé dans les
systèmes virtualisés pour transmettre de l'information entre les machines
virtuelles. C'est en général le comportement des contrôleurs de caches qui
sont alors exploités \cite{xu2011exploration}\cite{zhang2012cross}.

\paragraph{}
Lorsque un transfert de données non autorisé a lieu entre deux domaines de
sécurité, il est important de pouvoir en imputer la responsabilité. On parle
alors d'imputabilité.

\PhDdefinition{imput}{Imputabilité}{
On appelle imputabilité la garantie de traçabilité entre une action effectuée
sur le système et l'identité de l'utilisateur responsable de cette action.
}

Du fait des contraintes légales liées aux niveaux de confidentialité, le besoin
d'imputabilité est omniprésent dans les systèmes militaires.

\subsection{Ce que cible la sécurité logicielle}

\paragraph{}
Dans leur globalité, les architectures de sécurité sont là pour répondre à ces
trois besoins initiaux. Selon le niveau de certifiabilité, les architectures
de sécurité peuvent prendre plusieurs formes, allant d'une séparation
matérielle complète et stricte jusqu'à la simple intégration d'une politique de
gestion de contrôle d'accès (propriétaire, groupe, droit d'accès) aux fichiers,
comme par exemple les systèmes DAC (Discretionary Access Control), MAC
(Mandatory Access Control) ou RBAC (Role-based Access Control) \cite{sandhu1994access}.


\subsection{Cibles de sécurité et certifiabilité}

\paragraph{}
La complexité du système dépend du besoin de certifiabilité. On définit
initialement une cible d'évaluation (couramment appelée Target Of Evaluation.

\PhDdefinition{toe}{Target Of Evaluation}{
  On appelle Target Of Evaluation (TOE) l'ensemble des éléments logiciels et
  matériels qui seront évalués dans le cadre d'une certification, ainsi que le
  niveau d'évaluation attendu, au sens des Critères Communs.
}

Le niveau de sécurité du TOE est donc dépendant du niveau
souhaité. Les systèmes Linux bien configurés en terme de sécurité peuvent
atteindre un niveau de certification de l'ordre de l'EAL 4. Il faut cependant
prendre soin d'étudier précisément quelle est la TOE. En effet, le niveau de
sécurité d'un système certifié dans le cadre d'un fonctionnement autonome n'est plus assuré
si ce dernier est connecté à un réseau.

\paragraph{}
Associé à la TOE, on définit le socle de confiance couramment appelé Trusted
Computing Base.

\PhDdefinition{tcb}{Trusted Computing Base}{
  On appelle Trusted Computing Base (TCB) l'ensemble des éléments logiciels
  et matériels de la solution cible qui sont considérés comme de confiance.
  La corruption de tout élément de cet ensemble génère en conséquence une
  faille dans les propriétés de sécurité de la solution.
}

Ainsi les éléments de la TCB doivent
non seulement être certifiables au plus haut niveau souhaité dans le cadre de
l'évaluation du système, mais doivent être également protégés contre toute
corruption extérieure (comme par exemple à partir d'un élément
non-certifiable). On leur impose alors une exigence d'{\it inviolabilité},
comme décrite dans les exigences générales des systèmes MILS et MLS. Ces dernières
sont rappelées dans le Chapitre \ref{sec:mils} 

\paragraph{}
Lorsque l'on s'appuie sur une architecture de type compartimentation stricte
de fonctions exécutées de manière autonome (comme par exemple via des
technologies de virtualisation), la solution peut être compatible du principe
de certification par composition.

\PhDdefinition{certcomp}{Certification par composition}{
On appelle certification par composition un principe permettant, lorsque la
TCB est non modifiée, de ne certifier que les nouveaux composants
venant s'appuyer sur cette dernière. Il n'est alors pas nécessaire de
certifier à nouveau l'ensemble de l'architecture logicielle, TCB comprise.
}
Les systèmes compartimentés peuvent intégrer des compartiments certifiables
implémentant un algorithme simple répondant à un besoin précis. Lorsque ce
besoin est sécuritaire, il s'agit d'une fonction de sécurité.

\PhDdefinition{foncsec}{Fonction de sécurité}{
On appelle fonction de sécurité un traitement logiciel spécialisé dans la
réponse à un besoin sécuritaire donné. Une fonction de sécurité peut être
non-certifiable (par exemple en tant que processus logiciel d'un système
d'exploitation riche) ou certifiable (sous forme d'un élément logiciel
de petite taille et autonome ou s'appuyant sur un socle lui-même certifiable.
}

\section{Principes généraux des systèmes temps réel}

\subsection{Rappels sur les politiques d'ordonnancement}

\paragraph{}
Il existe différents ordonnanceurs temps réel. Ces derniers sont divisés en deux grandes familles
d'ordonnanceurs, les ordonnanceurs en ligne et hors ligne.

\paragraph{L'ordonnancement en ligne}
Les choix d'ordonnancement se font durant l'exécution, en fonction de la liste
des travaux présents et de leurs paramètres (par exemple en fonction de leur échéances
relatives).

\paragraph{L'ordonnancement hors ligne}
La politique d'ordonnancement est définie par avance. Elle est donc optimisée pour un cas particulier de jeu
de tâches, mais n'est pas réactive en cas d'impact non prévu sur la dynamique d'exécution (effets de caches, de surcharge
de bus, WCET trop grand ou trop faible, etc.).

\paragraph{}
Parmi tous ces ordonnanceurs, certains peuvent préempter un travail en cours
d'exécution avant sa terminaison. On parle alors d'ordonnancement préemptif. A
l'inverse, si un travail en cours d'exécution ne peut être arrêté avant sa
terminaison, on parle d'ordonnancement non-préemptif. La préemption peut être
liée à un paramètre temporel (time-driven) ou événementiel (event-driven).\\
Dans le cadre de ma thèse, je cible deux ordonnanceurs:
\begin{itemize}
\item L'ordonnancement TDM (hors ligne, préemptif de type time-driven)
\item L'ordonnancement EDF (en ligne, préemptif de type event-driven)
\end{itemize}

\section{Principes généraux de la virtualisation}

\paragraph{}
Il existe plusieurs types de virtualisation, en fonction de l'architecture
logicielle et parfois matérielle qui entre en compte dans la solution.

\subsection{Concepts liés à la virtualisation et la compartimentation}

\PhDdefinition{vm}{Machine virtuelle}{
On appelle machine virtuelle (ou VM, Virtual Machine) une implémentation
logicielle d'une machine (i.e d'un ordinateur) qui exécute des programmes
comme une machine physique. Il existe deux familles de machines virtuelles:
\begin{itemize}
\item Une machine virtuelle système, représentant une plateforme matérielle
complète et qui supporte l'exécution d'un système d'exploitation.
\item Une machine virtuelle de processus, dont le but est d'exécuter un seul
programme et qui permet d'assurer la portabilité de ce dernier. C'est par
exemple le cas de la machine virtuelle Java.
\end{itemize}
}

\PhDdefinition{vmm}{Moniteur de Machine Virtuelle}{
On appelle Moniteur de Machine Virtuelle \cite{popek} (ou VMM, Virtual Machine Monitor) le
composant logiciel en charge de permettre l'exécution d'une
ou plusieurs machines virtuelles.
}

\PhDdefinition{ev}{Environnement Virtuel}{
On appelle Environnement Virtuel (ou VE, Virtual Environment) un compartiment
logiciel en charge d'exécuter un environnement logiciel complet, à l'exception
du noyau. C'est le cas par exemple des technologies OpenVZ ou LXC
\cite{bhanage2011experimental}. Une telle architecture logicielle ne nécessite
pas de moniteur de machine virtuelle (ou VMM).
}

\subsection{Les solutions de virtualisation type I}

\paragraph{}
Les solutions de virtualisation de type I s'appuient sur un moniteur de machine
virtuelle autonome, s'exécutant directement sur le matériel. Il est seul
maître de l'environnement logiciel de la machine et assure l'ordonnancement
de ces dernières. La Figure \ref{fig:vmtype1} décrit l'architecture
logicielle de ce type de solution.\\
Les interactions entre le moniteur de machine virtuelle et les machines
virtuelles peuvent varier selon la solution utilisée pour assurer la bonne exécution de
l'environnement invité (i.e. l'environnement logiciel de la machine virtuelle)
et plus particulièrement du noyau invité.\\
Pour répondre à ce besoin, il existe trois solutions:
\begin{itemize}
  \item La virtualisation complète
  \item La paravirtualisation
  \item La virtualisation matérielle
\end{itemize}

\begin{figure}
  \label{fig:vmtype1}
  \input{figures/vmtype1_2.tex}
  \caption{Architecture logicielle générique des solutions de virtualisation de type I}
\end{figure}

\subsubsection{Principe de la virtualisation complète}

\paragraph{}
Historiquement la plus ancienne, elle fut formalisée sous forme d'exigences par
G.J. Popek et R.P. Goldberg \cite{popek} dès 1974. Le principe est de pouvoir
exécuter un environnement logiciel dans une machine virtuelle tout en
assurant que son comportement soit identique à une exécution native.
L'environnement invité n'est pas modifié, et c'est au travers des exceptions
processeur dues à l'exécution en tant qu'environnement invité que le moniteur
de machine virtuelle réagit à des traitements qui ne peuvent être exécutés
directement par l'environnement invité sans impact sur la sécurité et la
sûreté du système.\\
Dans leur Article \cite{popek}, G.J. Popek et R.P. Goldberg  décrivent les
incompatibilités du matériel avec ce type de principe. En effet, seules les instructions générant
des exceptions processeur peuvent être substituées par le moniteur de machine
virtuelle. Les
instructions sensibles \cite{popek} ne générant pas d'exception sont une problématique
forte car ces dernières peuvent s'exécuter sans que le moniteur de machine
virtuelle en soit informé. La conséquence peut être plus ou moins grave car
ces dernières impactent la sûreté et la sécurité du système.

\paragraph{}
Afin de pouvoir virtualiser des environnements logiciels sans l'aide du matériel
et sans modifier l'environnement à virtualiser, plusieurs solutions ont été
mises en place, entre autres par VMWare. Dans ses
solutions, ce dernier est capable de déterminer les instructions sensibles qui seront
demandées par l'environnement invité et de modifier ces dernières afin
d'injecter des traitements simulés en lieu et place. Ce type de solution est
transparent pour l'environnement invité, mais génère des variations dans le
coût d'exécution de ces traitements, du fait de la simulation. La mesure de
ces variations par le logiciel fait partie des mécanismes lui permettant de détecter que
son environnement d'exécution est virtualisé.

\subsubsection{Principe de la paravirtualisation}/usr/share/texlive/texmf-dist/fonts/type1/urw/courier/ucrr8a.pfb></u
sr/share/texlive/texmf-dist/f

\paragraph{}
Afin de limiter la problématique de simulation des traitements consécutifs à
la détection d'instructions incompatibles avec la virtualisation, une solution
a été définie visant à impliquer le noyau de l'environnement invité dans le
support de la virtualisation. Le noyau est alors modifié pour substituer les
instructions sensibles par des appels volontaires à des fonctions du moniteur
de machine virtuelle, appelés hypercalls. Dans ce cas, le moniteur réagit soit en
exécutant le traitement initial avec les droits qui sont les siens, soit en
simulant le traitement.

\paragraph{}
Il existe un certain nombre de solutions implémentant les mécanismes de paravirtualisation.
C'est par exemple le cas de Xen ou encore de la solution propriétaire PikeOS
de Sysgo. La limitation du principe de virtualisation est due à la nécessité 
d'adaptation du noyau invité à l'Application Programming Interface (API) du moniteur de machine virtuelle. Cette
API n'étant pas normalisée, cela implique une implémentation souvent
spécifique à la solution de virtualisation utilisée. Cela implique également
d'avoir accès au code source du noyau invité, limitant ainsi les choix en
terme d'environnements paravirtualisables.

\subsubsection{Principe de la virtualisation matérielle}

\paragraph{}
Définie initialement par IBM, puis reprise depuis le début des années 2000 par
Intel et AMD, la virtualisation matérielle consiste à intégrer dans le
matériel des aides à la virtualisation. Le but est de limiter la complexité du
moniteur de machine virtuelle, ce dernier reposant sur des instructions
privilégiées de configuration du matériel pour gérer des environnements
matériels d'exécution des machines virtuelles séparés. Un exemple de solution
est l'architecture VT-d d'Intel, proposant des structures de configuration et
d'hébergement matérielles nommées VMCS (Virtual Machine Control Structure) qui
dupliquent les registres sensibles du processeur. Lorsque le moniteur de
machine virtuelle ordonnance l'environnement virtualisé, il indique au
processeur le VMCS à utiliser en lieu et place des registres utilisés
par le moniteur lui-même. La solution s'appuie également sur des éléments de
filtrage d'accès aux périphériques (nommés I/OMMU pour Intel, SMMU pour ARM
Cortex A15, A53 et A57). Il n'existe à ce jour aucune norme et chaque fondeur possède
ses propres instructions.\\
Cette solution n'est pas considérée ici car incompatible avec les exigences de
portabilité.

\subsection{Les solutions de virtualisation type II}

\paragraph{}
Les solutions de virtualisation type II s'appuient sur la présence d'un hôte qui
accueille le moniteur de machine virtuelle comme un processus. Ce dernier est
donc lui-même ordonnancé par l'hôte. Ce type de solution de virtualisation
correspond à des produits tels que VMWare Workstation ou encore VirtualBox.
La Figure \ref{fig:vmtype2} décrit l'architecture logicielle d'une telle
solution.
\begin{figure}
  \label{fig:vmtype2}
  \input{figures/vmtype2_2.tex}
  \caption{Architecture logicielle d'une solution de virtualisation type II}
\end{figure}

\paragraph{}
Ces solutions étant basées sur des hôtes de type GNU/Linux ou Windows dans
lesquels elles s'exécutent comme des tâches non temps réel, elles sont
donc incompatibles avec les contraintes de temps réel, y compris lorsque ces
dernières sont relativement souples. Elles sont également incompatibles des exigences de
sécurité forte, du fait de la non-certifiabilité du système hôte.

\paragraph{}
Les solutions de moniteurs de machine virtuelle utilisant ce type
d'architecture ne permettent pas d'assurer un cloisonnement spatial fort
(répartition stricte en mémoire physique) ni un cloisonnement temporel fort
(l'ordonnancement des machines virtuelles étant assujetti aux propriétés de
l'ordonnanceur de l'environnement hôte).

\subsection{Les solutions de virtualisation applicative}

\paragraph{}
Les solutions de virtualisation applicative sont parentes des solutions de
virtualisation de type II dans le sens où elles s'appuient sur un hôte.
Cependant, celles-ci ne nécessitent pas la présence de noyau invité. Elles
permettent d'instancier des Environnements Virtuels dont le noyau est partagé
entre ces derniers ainsi qu'avec l'hôte.
L'ordonnancement des tâches des VEs est géré directement par
l'ordonnanceur de l'hôte, sous forme d'un groupe de tâches autonome. Ce type
d'ordonnancement se rapproche ainsi du principe d'ordonnancement hiérarchique
qui sera décrit dans le Chapitre \ref{sec:hierarchique}.\\
Un exemple de solution de virtualisation applicative est la solution LXC
(LinuX Containers) de Linux. La Figure \ref{fig:vmlxc} décrit
l'architecture logicielle d'une telle solution. 

\paragraph{}
Les solutions de virtualisation applicatives permettent de créer des VEs
dont le système de fichiers, le jeu de tâches, les interfaces réseau et la
vision du stockage de masse sont séparés. 

\begin{figure}
  \label{fig:vmlxc}
  \input{figures/vmlxc_2.tex}
  \caption{Architecture logicielle d'une solution de virtualisation applicative}
\end{figure}

\section{Les interactions entre le matériel, la sécurité et le temps réel}

\subsection{Principes de base de la MMU}

\paragraph{}
Un applicatif, construit dans un fichier binaire lors de l'édition de lien, possède un certain nombre de sections
servant de conteneur à des données différentes, et ayant en conséquence des propriétés de sécurité (en terme d'accès)
également différentes. De ce fait, lors du déploiement de ces différentes
sections en mémoire vive, il est nécessaire
de   considérer   pour   chacune   de   ces   sections   des    propriétés    d'accès    spécifiques.

\paragraph{}
Dans un fichier binaire au format ELF \cite{DefElf}, il existe plusieurs
sections, dont entre autres:
\begin{itemize}
  \item la section {\tt .text}, contenant la partie exécutable du programme
  \item la section {\tt .rodata}, contenant les données en lecture seule (e.g. les chaînes de
    caractères)
  \item la section {\tt .data}, contenant les données en lecture et écriture
\end{itemize}

\paragraph{}
Lorsque plusieurs applicatifs s'exécutent en concurrence, il est
important de les cloisonner dans des environnements logiques séparés, ceci
afin d'éviter que ces derniers puissent interagir sans contrôle, par simple
accès à la mémoire physique.

\paragraph{}
Il existe plusieurs niveaux d'abstraction de la mémoire physique :
\begin{itemize}
\item Sans aucune abstraction, on est en mode dit  {\it  flat  memory}.
L'ensemble  des  logiciels s'exécutant en mémoire traitent des adresses physiques.
Ainsi,  ils  connaissent  leur  position  en mémoire,   et    ont    une    visibilité
sur    l'ensemble    de    la    mémoire    physique.\\
De ce fait, il leur est possible d'accéder au contenu des autres applicatifs en  cours  d'exécution.
Il devient ainsi aisé d'espionner les sections d'autres logiciels, comme les
sections .data ou .rodata, dans  lesquelles   il   est   possible   de   récupérer   par   exemple   des   mots   de   passe.\\
De plus, l'accès à la pile des autres applicatifs peut permettre de dériver
leur exécution vers un bloc de code spécifique, en remplaçant l'adresse de la fonction parente par une adresse  dont  on
maîtrise le contenu. Du fait que la pile est par essence accessible en écriture, tous les processus
peuvent modifier les champs d'un processus donné en mode {\it flat memory}, à partir  du  moment  où
l'adresse physique de sa pile est connue.
\item Décomposition de la mémoire physique en segments.  Il s'agit du mode  de  segmentation  présent
uniquement sur les processeurs de la famille x86.  Ce mode étant aujourd'hui remplacé  par  le
système de pagination, il ne sera pas décrit ici.
\item En s'appuyant sur une abstraction de la mémoire physique, appelé {\it mémoire virtuelle}. On
parle alors de mode dit de  {\it  pagination  mémoire}.   Un  processus  donné  possède  son  propre
environnement mémoire appelé Espace d'adressage (AS pour Address Space), dont la taille correspond à la valeur maximum d'un mot mémoire (e.g. dans un
processeur 32 bits, l'environnement  mémoire  virtuelle  à  une  taille  de  quatre  giga-octets).\\
Le processus, lors de son  exécution,  ne
traîte alors que des adresses virtuelles, dont la  translation  en  adresses
physiques  se  fait  au travers d'un élément matériel nommé Memory Management
Unit (MMU).
\end{itemize}

\paragraph{}
La gestion de la pagination est gérée par la MMU, sous les ordres du
noyau du système d'exploitation. Ce dernier construit l'association entre
mémoire physique et mémoire virtuelle dans des structures en mémoire nommées
tables de pages (Pages Tables). Sur les architectures x86, ces dernières sont accédées par la MMU
via un registre nommé PDBR. Ce dernier pointe vers la racine de l'arbre de
mapping de mémoire virtuelle du processus en cours, et s'appuie sur ce dernier
pour retrouver l'adresse mémoire physique associée à l'adresse mémoire
virtuelle demandée par le c{\oe}ur du processeur. Afin d'optimiser les accès à cet
arbre de page, la MMU s'appuie sur un cache local nommé TLB (Translation
Lookaside Buffer), réduisant ainsi les requêtes au contrôleur mémoire.

\paragraph{}
Le   principe   de   fonctionnement   de   la   pagination    fournit    plusieurs    avantages    :
\begin{itemize}
\item A chaque processus est associé un AS (espace d'adressage) qui  lui  est  propre.
Ainsi, il se voit seul en mémoire et il ne possède plus la capacité d'accéder aux autres  processus.
En effet, l'accès à la mémoire physique se fait via un accès à une adresse  virtuelle.   Pour  qu'un
processus y accède, une association entre adresse physique et adresse virtuelle, locale à cet espace
d'adressage doit avoir été faite par le noyau au préalable, via configuration de la MMU. Lors de la
création d'un processus, seules les adresses physiques correspondant  à  ces  propres  données  sont
intégrées à l'espace d'adressage, interdisant à ce dernier  d'accéder  aux  données  de  tout  autre
processus.
\item De plus, la gestion de la fragmentation de la mémoire centrale, dépendant des  allocations  et
désallocations successives de la mémoire, est rendue opaque en mémoire
virtuelle. Des espaces mémoires
physiques disjoints peuvent être positionnés de manière contigüe dans l'espace
d'adressage virtuel,  faisant croire  au  processus  que  ses  données  sont
contigües  sans  pour autant  qu'elles  le   soient.
\item Le contrôleur MMU permet d'associer à chaque page mémoire des propriétés
en terme d'accès:
  \begin{itemize}
    \item Niveau d'exécution minimum (mode administrateur ou utilisateur)
    \item Droits d'accès (lecture, écriture et exécution)
  \end{itemize}
Ces droits d'accès permettent, dans un espace d'adressage donné, de limiter
les accès à chaque page. Ainsi, les sections .rodata ou .text du processus ne
sont pas accessibles en écriture. Une telle protection réduit la surface
d'attaque, rendant plus difficile la corruption de l'application.
\end{itemize}
La Figure \ref{fig:mmu} décrit le principe général de pagination entre mémoire
physique et mémoire virtuelle.

\begin{figure}
\label{fig:mmu}
\input{figures/mmu_as.tex}
\caption{Principe général du mapping de page mémoire dans les espaces
d'adressage des processus}
\end{figure}

\paragraph{}
Les attaques entre les différents processus ne sont pas les  seules  possibles.   Il  est  également
possible de corrompre le comportement d'un processus donné, pour lui faire faire un  traitement  non
prévu.\\
Une des attaques les plus classiques est l'injection  de  {\it  shellcode}.
Un shellcode est un morceau de code binaire intégré sous forme d'une chaîne de
caractère. Il est utilisé afin d'être intégré quelque part dans l'espace d'adressage du processus,  avant  de
corrompre   sa   pile   pour   provoquer   un   branchement   vers   ce    bloc    d'instructions.\\
Cette attaque est donc faite en deux temps, en positionnant des instructions
en  mémoire puis en forçant le branchement. En général, les shellcodes sont placés dans la pile,  en
amont du positionnement courant du pointeur de pile. Il convient, pour éviter ce type d'attaque, de
rendre l'espace mémoire utilisé pour la pile non-exécutable.  A l'inverse, l'espace  mémoire  où  se
situe le code du processus doit lui être exécutable, mais ne doit pas  être
accessible en écriture,  ce  dernier étant immuable durant l'exécution du processus.\\
On définit ici des droits d'accès à des  espaces  mémoire  (lecture,  écriture,  exécution)  variants
selon le type de données qui y sont hébergées. Les MMU modernes, lors de la création d'une association
entre mémoire physique et mémoire virtuelle, supportent  ces  trois  propriétés,  fournissant  un
durcissement   supplémentaire   pour   réduire   les   risques   de   corruption   d'un   processus.

\subsection{A propos des contrôleurs de cache}
\label{sec:notions_caches}

\subsubsection{Description générale}

\paragraph{}
Le \index{Cache}cache est une mémoire statique à accès  rapide,  permettant  d'accélérer  les  accès
mémoire du processeur, en copiant les données (code et data) de manière à ce  qu'elles  soient  accessibles
directement  par  ce dernier  sans nécessité de  transfert   sur   le   bus \cite{cachalgo}.

\paragraph{}
La figure \ref{fig:cache_soa} montre une architecture mémoire générique, avec plusieurs  niveaux  de
cache, suivis d'un bus d'accès mémoire et  du  contrôleur  mémoire.

\begin{figure}
\input{figures/cache_architecture.tex}
\caption{Architecture en couche générique\label{fig:cache_soa}}
\end{figure}

\paragraph{}
Il existe dans les processeurs modernes plusieurs niveaux de cache :
\begin{enumerate}
\item Le cache niveau 1, le plus petit et accessible le plus rapidement.  Ce  cache  est  spécialisé  :
il définit un conteneur dit {\it cache code},
pour les éléments de code, et un  conteneur  dit  {\it  cache
data},   pour   le   reste   des    données.
\item Le cache niveau 2, de taille plus  conséquente,  mais  dont  l'accès  est  moins  rapide  que  le
cache L1. Ce cache n'est pas spécialisé.
\item Le cache niveau  3,  de  taille encore plus  grande. Ce dernier est en
  général partagé  entre  les  c{\oe}urs  sur  les architectures
  multi-c{\oe}urs. Ce cache n'est pas spécialisé et n'est pas toujours présent.
\end{enumerate}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\textbf{Niveau de mémoire}} & {\textbf{Temps d'accès}} \\
\hline
Cache L1 & 2-8 nano secondes \\
Cache L2 (partagé) & 5-12 nano secondes \\
Mémoire centrale & 10-60 nano secondes \\
\hline
\end{tabular}
\end{center}
\label{tab:cache_access_time}
\caption{Temps d'accès mémoire sur un Intel Core 2 duo}
\end{table}

\paragraph{}
Le tableau \ref{tab:cache_access_time}\footnote{confer \url{http://expertester.wordpress.com/2008/05/30/core-2-duo-vs-pentium-dual-core/}} fournit
un exemple de comparaison  du temps  d'accès  à   une   donnée   selon   son   positionnement   en   mémoire.

\paragraph{}
Les algorithmes de cache ont divers niveaux d'associativité :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Les algorithmes de type {\textit{one-way associative}} associent  une  cellule  mémoire  à  une
et une seule cellule de cache.  Il s'agit d'un algorithme surjectif, du fait de la taille du  cache.
Le choix de remplacement étant de type $1:1$, aucun algorithme spécifique n'est utilisé. Les données
sont écrites au seul emplacement possible en cache.
\item Les algorithmes de type {\textit{N-way associative}} associent une cellule mémoire avec
{\it N} (resp. 2, 4, 8, ...)
cellules de cache. Cette associativité permet d'optimiser l'écrasement des cellules du cache en donnant
un choix de positionnement à la cellule mémoire à mettre en cache.  Les algorithmes de  ce  type  se
différencient par leur politique de choix de la cellule à remplacer.
\end{itemize}
On a ainsi :
\paragraph{{\it N}-Way Set Associative}
Cet algorithme permet, pour chaque élément mémoire à charger  en  cache,  de  pouvoir  se  placer  à
{\it N} endroits dans le cache.  Ces {\it N} positions sont dépendantes  de  l'adresse  physique  en
mémoire centrale de la donnée.\\
Le choix de  la  cellule  parmi  la  liste  des  cellules  admissibles  se  fait  au  travers  d'un
algorithme de gestion de cache tel que décrit plus loin.

\paragraph{Direct Mapped}
L'algorithme Direct Mapped est un algorithme simple, optimisant le coût d'exécution du choix de la cellule.
Un élément mémoire n'est placé qu'à un seul endroit dans le cache, en fonction de son adresse physique,
quelle que soit la donnée précédemment présente.


\paragraph{}
Lorsqu'une donnée n'est pas en cache, on parle de {\it cache-miss}. Le processeur va successivement
vérifier  les  caches  L1,  L2  et  L3  (si présent),  avant   d'initier   une   recopie   à   partir
de    la    mémoire    centrale.

\paragraph{}
Il existe plusieurs algorithmes de gestion du cache, souvent de type
probabiliste. De récents travaux de recherche tendent à montrer que ces caches
peuvent être pris en compte dans les architectures temps réel \cite{guan2012wcet}. Cependant, les systèmes
temps réel très  contraints d'aujourd'hui,  comme  dans  l'aviation  ou  le  spatial,
utilisent encore beaucoup d'architectures avec caches désactivés.

\subsubsection{Positionnement du cache}

\paragraph{Cache \index{Cache!Look-aside}look-aside}
Cette architecture place le cache et la mémoire centrale côte à côte, comme le
montre la partie gauche de la Figure \ref{fig:cache_arch}.  Ainsi,
le c{\oe}ur du processeur peut accéder  à la mémoire centrale sans passer par le cache.\\
Le principe de fonctionnement est le suivant :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le processeur démarre un cycle de lecture
\item Si le cache possède la donnée en local (cache hit), il répond au processeur, et clôture le  cycle  de
lecture sur le bus.
\item Si le cache ne possède pas la donnée (cache miss), il laisse la mémoire centrale  répondre  au
processeur,  et  espionne  les  données  transitant  sur  le  bus  afin  de  se   mettre   à   jour.
\end{itemize}

\begin{figure}[h]
\input{figures/cache_memory_bus.tex}
\caption{Comparaison des architectures de types cache through et cache lookaside}
\label{fig:cache_arch}
\end{figure}


\paragraph{Cache \index{Cache!Look-through}look-through}
Cette architecture place le cache en coupure entre le c{\oe}ur du processeur et la mémoire
centrale, comme le montre la partie droite de la Figure \ref{fig:cache_arch}. Le processeur passe donc par le cache pour l'accès mémoire.\\
Le principe est le suivant :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le processeur démarre un cycle de lecture.
\item Si  le  cache  possède  la  donnée  en  local  (cache  hit),   il   répond   au   processeur.
\item Si le cache ne possède pas la donnée, il transfère la requête de lecture sur le bus. La mémoire
centrale renvoie la donnée, qui est mise en cache en même temps qu'elle est renvoyé  au  processeur.
\end{itemize}

Cette architecture provoque des cache-miss plus long (vérification par le cache avant  de  faire  la
requête au contrôleur de bus), mais permet d'utiliser le cache indépendamment
des autres c{\oe}urs du processeurs (pour un cache local).

\subsubsection{\index{Cache!Consistance}Consistance du cache}

\paragraph{}
Du fait que le cache contienne des duplicatas d'une partie de la mémoire, il est important de s'assurer
que la copie  et  l'originale  restent  identiques.   À  défaut,  cela  peut
entraîner  des  erreurs logicielles  difficilement  détectables.

\paragraph{}
La consistance du cache peut être impactée dans différentes situations, comme
par exemple :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Mise à jour de la mémoire par un contrôleur DMA
\item Écriture  en  mémoire  par  un  autre  processeur  (architecture   multi-CPU/multi-c{\oe}urs)
\item Modification de la donnée en cache sur un autre processeur
\end{itemize}

Il existe différentes méthodes permettant de garantir la consistance des
données en cache, comme par exemple :

\paragraph{\index{Cache!Snooping}Snooping}
Le snooping consiste en l'espionnage du bus d'adresse, afin de détecter  une  modification  sur  une
adresse mémoire contenue en cache.\\
Il y a deux manières de réagir à une détection positive :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Prévoir un rechargement de la cellule impactée (méthode dite {\it write-back}). Le rechargement ne
se fait pas nécessairement sur le moment, mais peut être ordonnancé pour s'exécuter au moment du prochain
accès à cette cellule, ou avant, selon les implémentations.
\item Désactiver la cellule impactée.  Le prochain accès aux données de cette cellule  provoquera  un
cache-miss,    suivi    d'une    recopie    à    partir    de    la    mémoire    centrale.
\end{itemize}

\paragraph{\index{Cache!Snarfing}Snarfing}
Le snarfing consiste à associer la détection d'une modification d'une donnée en cache à l'espionnage du
bus de donnée. En espionnant le bus, il est capable à tout instant de mettre à jour la cellule impactée
et de maintenir la consistance.

\subsubsection{Synchronisation entre le cache et la mémoire centrale}

\paragraph{}
Lorsque des données sont modifiées en local sur le cache, il est nécessaire de mettre à jour la mémoire
centrale afin de prendre en compte ce changement.

\paragraph{Write-back}
Le cache garde en mémoire, pour chaque cellule, son état. Lorsque la cellule est écrasée, ou que le cache
est  vidé,  les  données  modifiées  localement  sont  synchronisées  avec  la  mémoire  centrale.\\
La mise à jour de la mémoire centrale n'est pas synchrone avec la modification locale. Cela implique, au
niveau logiciel, de prévoir les problèmes d'incohérence sur les architectures
multi-c{\oe}urs ou multi-processeurs, au travers de l'usage des {\it \index{Memory barrier}memory barriers}. Ces dernières ont pour but de
fournir des mécanismes de synchronisation entre les différents c{\oe}urs afin
d'assurer la cohérence d'une donnée partagée entre plusieurs instances logicielles
exécutées en parallèle.
L'implémentation des barrières mémoires est dépendante de l'architecture
matérielle.

\paragraph{Write-through}
Une écriture sur cache implique une remontée synchrone de la mise à jour de la donnée, afin de maintenir
une cohérence entre le contenu du cache et la mémoire.

\subsubsection{Les algorithmes de gestion de cache}

\paragraph{\index{Cache!LRU}Least Recently Used}
L'algorithme LRU s'appuie sur les dates d'accès aux items du cache. En cas de cache-miss, c'est l'élément
qui a été accédé il y a le plus longtemps qui est remplacé.

\paragraph{\index{Cache!MRU}Most Recently Used}
L'algorithme MRU, à l'opposé du LRU, remplace l'élément dont l'accès est le plus récent. Il considère
l'hypothèse  selon  laquelle  un  élément  accédé   très   récemment   ne   le   sera   plus   avant
une    certaine    durée.

\paragraph{\index{Cache!Pseudo-LRU}Pseudo-LRU}
L'algorithme Pseudo-LRU utilise un arbre binaire représentant l'ensemble des cellules de caches pouvant
être remplacées. Chaque ensemble de cellules possède un tag fournissant une indication sur l'emplacement
(branche droite, branche gauche) de la cellule qui sera remplacée.  Ce tag est mis à jour  à  chaque
traversée de l'arbre.

\paragraph{}
Il s'agit d'un algorithme jugé efficace en comparaison au LRU pour les caches dont l'associativité est
supérieure à 4.  Il est utilisé dans les architectures Intel 486, et dans les
architectures PowerPC, comme le PowerQuicc de Freescale \cite{plru}. Le schéma \ref{fig:cache_plru_e300} est tiré du manuel
de   référence    du    processeur    e300,    utilisé    dans    les PowerQuicc 2 pro.

\begin{figure}
\input{figures/cache_plru_e300.tex}
\caption{Automate   à   état   de    l'algorithme    pseudo-LRU    du    Power    Quicc    2    Pro}
\label{fig:cache_plru_e300}
\end{figure}


\paragraph{\index{Cache!Segmented-LRU}Segmented LRU}
L'algorithme SLRU utilise deux segments de caches complémentaires, dont les cellules sont ordonnées de
la dernière utilisée à la plus anciennement utilisée :

\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le segment probatoire ({\it probationary segment}), qui est celui sur lequel sont mises les données
mémoire, en cas de cache miss. C'est également le seul segment où des données peuvent être retirées
du cache.
\item Le segment protégé ({\it protected segment}), qui contient la liste des cellules de cache ayant
été accédées au moins deux fois.
\end{itemize}
Le schéma \ref{fig:slru_1} montre ainsi que l'algorithme de cache fait une différence entre des données
accédées à faible fréquence et des données accédées régulièrement.

\begin{figure}
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_1.tex}
\end{minipage}\hfill
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_2.tex}
\end{minipage}\hfill
\caption{Intégration et déplacement des données accédées fréquemment}
\label{fig:slru_1}
\end{figure}


\paragraph{}
En effet, lorsqu'une cellule est accédée une seconde fois après un premier cache miss, celle-ci  est
transférée  sur  le  segment  protégé,  en  tête  de  liste.\\
Cela a pour conséquence, si le segment protégé est plein, de faire descendre la cellule la plus anciennement
accédée sur le segment probatoire. Elle est alors positionnée en tête de liste, ceci afin de donner une deuxième chance à
toute donnée accédée au moins deux fois. Lorsque le segment probatoire est plein et que de nouvelles
données y sont intégrées, que ce soit via un déplacement d'une cellule du segment protégé ou du fait d'un
cache miss, la cellule la moins récemment utilisée est écrasée.

\begin{figure}
\input{figures/slru_3.tex}
\caption{Principe de deuxième chance du SLRU}
\end{figure}

\paragraph{}
Cet algorithme fait donc une différence entre les données souvent accédées (au moins deux fois) et les
données rarement accédées (une   fois)   pendant    leur    durée    de    vie    en    cache.
Il  permet  donc  d'optimiser  la  gestion  du  cache  lors   d'accès   mémoire   peu   fréquents.\\
La  taille  des  deux   segments   est   dépendante   dynamiquement   du   schéma   d'entrée/sortie.

\paragraph{\index{Cache!LFU}Least Frequently Used}

Lors d'un cache miss, la cellule utilisée pour charger les données mémoire est celle dont le contenu est
utilisé le moins fréquemment. En cas d'égalité de fréquence, l'algorithme LRU est utilisé pour départager
les cellules.

\subsection{Les algorithmes de gestion de caches}
\label{subsec:cachealgo}

\subsubsection{Les caches et la sécurité}

\paragraph{}
L'impact des caches processeurs est non nul sur la sécurisation d'un système lorsque deux domaines de
sécurité sont ordonnancés sur un même matériel. Cela est encore plus vrai si
ces deux domaines sont ordonnancés sur un même c{\oe}ur du processeur.\\
En effet, la modification de l'état des caches du processeur est induit par le
comportement des différentes applications en cours d'exécution, générant un
canal auxiliaire entre ces dernières, comme décrit plus loin.

\paragraph{}
Dans \cite{percival2005cache}, l'auteur a ainsi démontré qu'il était possible, au travers de
mesures des coûts d'accès mémoire d'une tâche, de détecter l'impact d'une
autre tâche exécutée en concurrence, dans le cadre d'un ordonnancement
préemptif. Cet impact étant variant selon le comportement de celle-ci,
il est alors possible d'en déduire certaines informations sur l'arbre
d'exécution associé, et donc sur les choix qui ont été effectués par cette
tâche. Une telle information est utile si l'attaquant a connaissance du code source
de la tâche cible, afin d'associer le chemin d'exécution détecté à une
succession d'algorithmes connus. Cette hypothèse est aujourd'hui plus réaliste dans le monde
de l'open-source. Un tel mécanisme a été montré fonctionnel dans
\cite{bernstein2005cache}\cite{bonneau2006cache} et dans \cite{banescu2011cache}, où les auteurs
démontrent la capacité pour une tâche à récupérer des
informations sur les données d'entrée d'algorithmes cryptographiques utilisées
par une tâche concurrente, grâce à l'étude du chemin d'exécution détecté.
Dans \cite{bernstein2005cache}, la démonstration a été effectuée dans le cadre de deux tâches
exécutées de manière simultanée sur un processeur Pentium 4 supportant
l'hyperthreading. Dans \cite{banescu2011cache}, l'auteur n'impose pas l'usage simultané du c{\oe}ur
processeur. L'espionnage se fait au travers de l'étude de son propre comportement en terme
de mesure de temps d'accès mémoire, induit par le comportement de la tâche
concurrente (grâce à la mesure des caches-miss imprévus).\\
De manière générale, l'espionnage des algorithmes cryptographiques au travers
des collisions des lignes de caches est facilité par leur implémentation.
En effet, ils s'appuyent sur des tables de
taille fixe présentes en mémoire à des emplacements disjoints. A ce jour, les
clefs de chiffrement restent donc un élément confidentiel difficile à
protéger si tout ou partie de l'architecture logicielle est corrompue.

\paragraph{}
Aujourd'hui, lorsque deux compartiments s'exécutent en concurrence sur un même
c{\oe}ur processeur, il est d'usage de flusher et réécrire le contenu des cellules des caches à chaque préemption
des compartiments. On considère ainsi qu'un compartiment donné ne protège pas
ses propres tâches entre elles, mais que deux compartiments concurrents doivent être
ordonnancés avec un cache dont le contenu est connu et prédictible.\\
Avec une telle architecture logicielle, la fonction de sécurité correspondant
au flush de cache est considérée suffisante. Cette dernière peut en effet être
bornée dans le temps, et garantit le contenu des cellules du cache lorsque le compartiment
reprend son exécution.

\subsubsection{Les caches et le temps réel}
\label{sec:soacache_wcet}

\paragraph{}
Les caches processeurs sont également impactant dans le cadre du respect des
contraintes temps réel.
Dans les systèmes industriels très exigeants, les caches sont souvent désactivés.
Malheureusement, une telle configuration est de plus en plus difficile à mettre en {\oe}uvre. En effet, les
architectures matérielles et logicielles sont de plus en plus construites pour
être performantes grâce aux contrôleurs de cache (grâce à la présence de deux
voire trois niveaux de
cache, ou encore de mécanismes de synchronisation avancée dans le cadre des architectures
MPSoC, etc.). La désactivation des caches génère un ralentissement difficile à
accepter selon les hypothèses de performances initiales.

\paragraph{}
Il existe un grand nombre de travaux sur la prise en compte des contrôleurs de
caches dans le temps réel. Ces travaux sont séparables en deux grandes
familles:
\begin{itemize}
  \item Prise en compte des besoins temps réel dans le design du contrôleur de caches
    et optimisation des cache-miss (cache locks et cache partitioning)
  \item Caclul du nombre pire cas de cache-miss au travers de l'étude du chemin
    d'exécution des différentes tâches en exécution concurrente, avec ou sans
    l'aide du contrôleur de cache
\end{itemize}

\paragraph{}
Des travaux existent sur l'usage des méthodes de cache-locking afin de limiter le nombre de cache-miss
\cite{campoy2001static}\cite{puaut2006wcet}, grâce au blocage de certaines lignes
de cache dans le contrôleur, afin d'en assurer la présence lors d'un
accès ultérieur. Il faut alors choisir judicieusement les lignes de cache
devant être maintenues, sans pour autant réduire trop fortement le nombre de
lignes de cache pouvant être remplacées pour ne pas impacter trop fortement
l'exécution des autres tâches.

\paragraph{}
Lorsque les contrôleurs de caches ne permettent pas un tel contrôle par le
logiciel, il faut alors étudier leur comportement en terme d'algorithme de
remplacement de ligne de cache, afin de déterminer, en fonction du chemin
d'exécution de la tâche courante, le taux de cache-miss associés. Le but est
dans ce cas d'intégrer au mieux l'impact positif de la présence du cache du
processeur sur l'exécution de la tâche.\\
Ainsi, dans \cite{guan2012wcet}, les auteurs ont montré qu'il était possible de
prendre en compte l'impact d'un contrôleur de cache s'appuyant sur
l'algorithme MRU dans le calcul du WCET d'une tâche. Ils ont
ainsi montré qu'il était possible de déterminer, sous certaines conditions,
que le maintien de certaines lignes de caches dans le contrôleur pouvait être
prédictible, réduisant ainsi fortement le pessimisme dans la mesure du WCET.

\subsubsection{État de l'art des problématiques temps réel dans le support MMU}

\paragraph{}
Les contrôleurs de caches des CPU ont un impact reconnu sur le respect des
contraintes temps réel d'un système. Il en va de même pour la TLB
(Translation Lookaside Buffer).\\
Le TLB est le cache spécialisé permettant d'optimiser la traduction
d'adresse que la MMU effectue entre l'adressage logique (tel que vu par le
c{\oe}ur processeur) et l'adressage physique (tel que vu au niveau du bus
d'interconnexion). La présence du TLB découle du fait que les données de
traduction sont gérées en mémoire centrale, induisant des accès mémoire
fréquents par le contrôleur MMU afin de faire l'association mémoire
logique/mémoire physique. Le TLB est donc un cache local permettant de garder
en mémoire les dernières traductions d'adresses afin d'éviter une
surconsommation des accès mémoire centrale par le contrôleur MMU.\\
Le contenu du TLB est impacté par l'ordonnancement de tâches, et doit donc
être considéré dans le cadre de la gestion de tâches avec des contraintes de
temps réel dur.
Dans \cite{groesbrink2008modular} les auteurs décrivent un mécanisme apparenté
aux cache-locking afin de limiter l'impact du TLB sur le coût d'exécution des
tâches temps réel.

\subsubsection{Synthèse}

\paragraph{}
L'usage des caches est aujourd'hui nécessaire afin de pouvoir exploiter
correctement les capacités des processeurs modernes. Cependant, leur usage
impacte à la fois la sécurité et le temps réel. Dans le cadre de ma thèse, je
ne propose pas de design matériel de contrôleur de cache.
Néanmoins, je propose un axe de solution, décrit dans le Chapitre \ref{sec:caches},
afin de limiter l'impact des contraintes de sécurité sur les performances et
le temps réel. Plus généralement, je considère la problématique sécuritaire associée à la présence
d'entités logicielles de domaines de sécurité hétérogènes sur une même
architecture matérielle, et son impact sur la gestion des caches.

\paragraph{}
Dans le cadre de la définition de la solution finale, je m'appuie néanmoins,
sur demande de Thales, sur du matériel existant de type
PowerPC pour des raisons de certifiabilité. En effet, les
architectures PowerPC sont celles ayant le moins de bugs matériels référencés
parmi les grandes familles (ARM, PowerPC, x86 et Sparc). Elles ont comme bonne
propriété d'allier une consommation raisonnable à de bonnes performances.
Aujourd'hui, les contrôleurs de caches utilisés par les c{\oe}urs PowerQuicc
sont de type Pseudo-LRU, comme vu plus haut. Les c{\oe}urs e500mc, succédant
aux c{\oe}urs e300 des PowerQuicc 2, fournissent de plus plusieurs propriétés
intéressantes, dans le cadre des archtiectures QorIQ de Freescale
\cite{QorIQP4080} :
\begin{itemize}
  \item La capacité de dériver une partie du contrôleur de cache L2 en mémoire
    SRAM utilisable par le logiciel.
  \item Le support du {\it secure boot}, qui permet, via l'injection d'une
    clef de chiffrement dans une mémoire au travers de fusibles à usage
    unique, d'assurer l'intégrité des différents éléments de la chaîne de boot
    (bootloader, puis noyau du système d'exploitation).
  \item La présence de PAMU (Peripherical Access Management Unit) dont le but
    est de permettre au logiciel d'injecter une politique de filtrage des accès
    des différents composants matériels initiateurs sur le bus.
\end{itemize}
Les mécanismes de secure boot et de PAMU, du fait de leur récente intégration
et pour des raisons de confidentialité liés aux travaux de Thales autour de
leur usage, ne sont pas étudiés dans ma thèse.

