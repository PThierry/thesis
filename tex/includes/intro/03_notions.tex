%%
%%
%% hardware_impacts.tex for thesis in /doctorat/these/tex
%%
%% Made by Philippe THIERRY
%% Login   <Philippe THIERRYreseau-libre.net>
%%
%% Started on  Fri Mar 12 16:36:41 2010 Philippe THIERRY
%% Last update Mon Aug 30 17:00:21 2010 Philippe THIERRY


\chapter{Concepts}
\label{sec:notions}
\doMinitoc

\section{Principes généraux de la sécurité}

\subsection{Terminologie militaire de la sécurité}

\subsubsection{Les niveaux de confidentialité}

\paragraph{}
Il existe un grand nombre de niveaux de confidentialité formalisés dans le vocabulaire
militaire. Pour la France, on parle des niveaux suivants, du plus faible au
plus élevé :
\begin{itemize}
  \item Diffusion Restreinte (DR). Bien que très employé, c'est le seul niveau pour
    lequel aucune peine n'est définie dans le code pénal en cas de non-respect
    des exigences de confidentialité associées.
  \item Confidentiel Défense (CD). Premier niveau impliquant un risque pénal. Ce
    niveau peut être présent jusque dans les véhicules blindés positionné sur
    le théâtre d'opération.
  \item Secret Défense (SD). Ce niveau est en général présent jusque dans les
    centre de contrôle militaire locaux au théâtre d'opération. Il s'agit des
    centres de contrôle militaire déployés dans les pays en guerre dans les
    bâtiments civils protégés (type aéroport) ou dans des bâtiments militaires
    proche de la zone de conflit (frégate, porte-avions).
  \item Très secret défense (TSD). Plus haut niveau de sécurité.
\end{itemize}
Il existe de plus un grand nombre de niveaux de confidentialité lié à l'OTAN
(Organisation du Traité de l'Atlantique Nord, NATO en anglais), ou à diverses organisations
impliquant une force armée. Il existe ainsi les niveaux Nato Restricted (NR),
Nato Confidential (NC), Nato Secret (NS) et encore bien d'autres niveaux. Il
existe une problématique claire de comparaison des niveaux France avec les
niveaux OTAN. Ainsi, peut-on transférer des données du niveau NS au niveau SD
? Cette problématique est encore aujourd'hui traité au cas par cas.

\paragraph{}
Lorsqu'un niveau de confidentialité est mis en place, c'est au travers d'un
système d'information respectueux d'une politique de sécurité donnée. Cette
politique de sécurité (nommée PSSI - Politique de Sécurité d'un Système
d'Information) définit des exigences de sécurité pour assurer la
confidentialité, l'intégrité et la disponibilité (au sens sécuritaire du
terme) du domaine. Ces exigences sont mise en place sous forme:
\begin{itemize}
  \item d'un choix d'architecture réseau, logicielle et matérielle
  \item de la définition d'algorithmes de chiffrements spécifiques et de
    solutions implémentant ses algorithmes
  \item d'une définition d'un processus de gestion du domaine (protection physique et logique)
  \item d'une définition des responsabilités et des rôles de chaque
    intervenant dans le domaine
\end{itemize}
Plus le niveau de confidentialité est élevé, plus la PSSI est stricte et
complexe.

\subsubsection{Changement de niveau et problématique rouge/noir}

\paragraph{}
On appelle domaine rouge un domaine de sécurité dans lequel les données
transitant ne sont pas protégées. On parle alors de données rouges. Le réseau
faisant transiter ces données dans un domaine rouge est alors appelé réseau
rouge.

\paragraph{}
On appelle domaine noir un domaine de sécurité dans lequel les données
transitant sont modifiées pour en assurer la confidentialité, l'intégrité et
la disponibilité. On dit alors que les données sont noircies. En général, un
domaine noir est en charge de faire transiter des données dont le niveau de sécurité
est supérieur à celui du domaine lui-même. Ainsi, faire
transiter des données de niveau Confidentiel Défense dans un domaine de niveau
Diffusion Restreinte ne peut être fait que si ces données sont préalablement
noircies.\\
Noircir des données implique en général un chiffrement de ces dernière, mais
pas seulement. Le profil de flux peut être modifié afin de rendre difficile
la détection du type de communication sur la base de son profil. Du bruit peut
ainsi être généré et du {\it padding} incorporé afin d'empêcher toute déduction sur
simple écoute du flux noir.

\paragraph{}
La problématique de passage d'un domaine rouge à un domaine noir est appelé
problématique rouge/noir. On la retrouve principalement dans les réseaux radio
militaires, mais reste un principe général applicable à toute interconnexion
de réseaux militaires.

\subsubsection{niveau de confidentialité et droit d'en connaître}
\paragraph{}
On appelle droit d'en connaître l'ensemble de connaissances minimum suffisant
pour un sujet (une personne ou encore un processus logiciel) pour mener à bien
une tâche. Ainsi, une personne ayant une habilitation Confidentiel Défense sur
un projet donné n'a pas besoin pour autant de connaître l'ensemble des
informations classifiées Confidentiel Défense de ce dernier. On lui fournit alors
uniquement les informations nécessaires à son travail, appelé droit d'en
connaître. Le principe est le même pour les processus logiciel faisant du
traitement sur des données d'un niveau de confidentialité donné.

\subsection{Fuite d'informations: Canaux cachés et canaux auxiliaires}

\paragraph{}
Assurer la confidentialité est une tâche complexe. En effet, limiter l'accès à
la donnée n'est pas suffisant pour assurer sa protection. En effet, il est
possible de récupérer des informations sur la donnée de plusieurs manières,
sans y accéder directement. On parle alors de cannal auxiliaire ou de cannal
caché, selon la méthode.

\subsubsection{Canal auxiliaire et étude comportementale}

\paragraph{}
Le principe d'un canal auxiliaire consiste à s'appuyer sur une etude
comportementale non-invasive sur des fonctions ou des traitements afin de
récupérer des informations sur les données traitées. La plupart des études sur
le sujet sont dans le domaine de la
cryptanalyse \cite{sidechancube}\cite{sidechan}. Ces
différentes études démontrent la capacité, au travers de l'étude de la consommation électrique
des différents éléments d'un processeur \cite{sidechancube} ou via l'étude de
l'impact des caches processeurs sur le comportement temporel de
l'algorithmique \cite{sidechan}, de récupérer des données sur la clef
de chiffrement utilisée.\\
Le risque lié au canal auxiliaire est connu depuis longtemps et ont permis de
démontrer que des algorithmes cryptographique réputés sûr restent fragiles à
ce type d'attaque. C'est le cas par exemple d'AES \cite{aes} ou de
Serpent \cite{serpent}.

\subsubsection{Canal caché pour le transfert de données}

\paragraph{}
Le principe d'un canal caché consiste en la capacité à utiliser la bande
passante d'un canal logiciel ou matériel pour faire transiter des données
pour lesquels il n'est pas initialement prévu. L'usage d'un canal caché
implique un usage volontaire de part et d'autre pour émettre les données et
pour les recevoir. Il est donc invasif, contrairement aux canaux auxiliaires.\\
Un exemple typique est de s'appuyer sur la période inter trame d'un flux
réseau donné pour fournir une information \cite{berk2005detection}. En faisant varier la période, il
est possible d'émettre de l'information, qui peut être ainsi interprété par un
récepteur. De manière générale, les canaux cachés permettent l'émission de
données de faible taille, car leur débit est en général très faible. Ils sont
cependant tout à fait suffisant pour faire transiter des données de type
cryptographie (clefs symétrique et/ou asymétrique) ou des messages courts.\\
Le principe d'exploitation d'un canal caché est également utilisé dans les
systèmes virtualisés pour transmettre de l'information entre les machines
virtuelles. C'est en général le comportement des contrôleurs de caches qui
sont alors exploités \cite{xu2011exploration}\cite{zhang2012cross}.

\subsection{Ce que cible la sécurité logicielle}

\paragraph{}
Dans leur globalité, les architectures de sécurité sont là pour répondre à ces
trois besoins initiaux. Selon le niveau de certifiabilité, les architectures
de sécurité peuvent prendre plusieurs formes, allant d'une séparation
matérielle complète et stricte jusqu'à la simple intégration d'une politique de
gestion de contrôle d'accès (propriétaire, groupe, droit d'accès) aux fichiers,
comme par exemple les systèmes DAC (Discretionary Access Control), MAC
(Mandatory Access Control) ou RBAC (Role-based Access Control) \cite{sandhu1994access}.


\subsection{Cibles de sécurité et certifiabilité}

\paragraph{}
La complexité du système dépend du besoin de certifiabilité. On définit
initialement une {\it TOE} (Target Of Evaluation) \cite{cnssi_4009}, intégrant ce qui, dans le
système cible, doit être évalué, ainsi que le niveau de l'évaluation, au sens
des Critères Communs. Le niveau de sécurité du TOE est donc dépendant du niveau
souhaité. Les systèmes Linux bien configurés en terme de sécurité peuvent
atteindre un niveau de certification de l'ordre de l'EAL 4. Il faut cependant
prendre soin d'étudier précisément quel est la TOE. En effet, le niveau de
sécurité d'un système certifié dans le cadre d'un fonctionnement autonome n'est plus assuré
si ce dernier est connecté à un réseau.

\paragraph{}
Associé à la TOE, on définit la {\it Trusted Computing Base} (TCB) \cite{cnssi_4009}. Cette
notion définit l'ensemble des éléments logiciels et matériels de
la solution cible qui sont considérés comme de confiance. La corruption de
tout élément de cet ensemble génère en conséquence une faille dans les
propriétés de sécurité de la solution. Ainsi les éléments de la TCB doivent
non seulement être certifiables au plus haut niveau souhaité dans le cadre de
l'évaluation du système, mais doivent être également protégés contre toute
corruption extérieure (comme par exemple à partir d'un élément
non-certifiable). On leur impose alors une exigence d'{\it inviolabilité},
comme décrit dans les exigences générales des systèmes MILS, décrit dans le
Chapitre \ref{sec:mils} 

\section{Principes généraux des systèmes temps réel}

subsection{Rappels sur les politiques d'ordonnancement}

\paragraph{}
Il existe différents ordonnanceurs temps réel. Il existe deux grandes familles
d'ordonnanceurs, les ordonnanceurs en-ligne et hors ligne.

\paragraph{L'ordonnancement en ligne}
Les choix d'ordonnancement se font durant l'exécution, en fonction de la liste
des travaux présents et de leurs paramètres (par exemple en fonction de leur échéances
relatives).

\paragraph{L'ordonnancement hors ligne}
La politique d'ordonnancement est définie par avance. Elle est donc optimisée pour un cas particulier de jeu
de taches, mais n'est pas réactive en cas d'impact non prévu sur la dynamique d'exécution (effets de caches, de surcharge
de bus, WCET trop grand ou trop faible, etc).

\paragraph{}
Parmis tous ces ordonnanceurs, certains peuvent préempter un travail en cours
d'exécution avant sa terminaison. On parle alors d'ordonnancement préemptifs. A
l'inverse, si un travail en cours d'exécution ne peut être arrêté avant sa
terminaison, on parle d'ordonnancement non-préemptif. La préemption peut être
liée à un paramètre temporel (time-driven) ou évènementiel (event-driven).\\
Dans le cadre de ma thèse, je cible deux ordonnanceurs:
\begin{itemize}
\item L'ordonnancement TDM (hors ligne, préemptif de type time-driven)
\item L'ordonnancement EDF (en ligne, préemptif de type event-driven)
\end{itemize}

\section{Les domaines de sécurité}

\paragraph{}
Un domaine de sécurité est un environnement ou un contexte incluant un
ensemble de ressources systèmes et d'entités respectant une même politique de
sécurité.\\
Les interactions intra-domaines ne sont pas soumises à des exigences fortes de
sécurité. Cependant, il arrive qu'un même domaine de sécurité soit
physiquement séparé en plusieurs sous-systèmes qui, pour communiquer, doivent
traverser des domaines de sécurité différents. Les passerelles
d'interconnexion doivent alors assurer le {\it noircissement} des données à
faire transiter. L'action de noircissement consiste à assurer la
confidentialité des données de diverses manière afin d'assurer que les
domaines par lesquels transitent ces données ne soient pas aptes à en tirer
une information dont le niveau de sécurité est supérieur au leur.\\
De manière générale, le noircissement implique un chiffrement des données et
potenciellement une modification du profil de flux, ceci afin de limiter la
problématique de canal auxiliaire dérivant du profil de flux initial \cite{berk2005detection}.

\section{La virtualisation}

\paragraph{}
Il existe plusieurs type de virtualisation, en fonction de l'architecture
logicielle et parfois matérielle qui entre en compte dans la solution.

\subsection{Concepts liés à la virtualisation et la compartimentation}

\begin{definition}
On appelle machine virtuelle (ou VM, Virtual Machine), est une implémentation
logicielle d'une machine (i.e d'un ordinateur) qui exécute des programmes
comme une machine physique. Il existe deux famille de machines virtuelles:
\begin{itemize}
\item Une machine virtuelle système, représentant une plateforme matérielle
complète et qui supporte l'exécution d'un système d'exploitation.
\item Une machine virtuelle de processus, dont le but est d'exécuter un seul
programme et qui permet d'assurer la portabilité de ce dernier. C'est par
exemple le cas de la machine virtuelle Java.
\end{itemize}
\end{definition}

\begin{definition}
On appelle Moniteur de Machine Virtuelle \cite{popek} (ou VMM, Virtual Machine Monitor) le
composant logiciel en charge de permettre l'exécution d'une
ou plusieurs machines virtuelles.
\end{definition}

\begin{definition}
On appelle Environnement Virtuel (ou VE, Virtual Environment) un compartiment
logiciel en charge d'exécuter un environnement logiciel complet, a l'exception
du noyau. C'est le cas par exemple des technologies OpenVZ ou LXC
\cite{bhanage2011experimental}. Une telle architecture logicielle ne nécessite
pas de moniteur de machine virtuelle (ou VMM).
\end{definition}

\subsection{Les solutions de virtualisation type I}

\paragraph{}
Les solutions de virtualisation de type I s'appuient sur un moniteur de machine
virtuelle autonome, s'exécutant directement sur le matériel. Il est seul
maître de l'environnement logiciel de la machine et assure l'ordonnancement
de ces dernières. La Figure \ref{fig:vmtype1} décrit l'architecture
logicielle de ce type de solution.\\
Les interactions entre le moniteur de machine virtuelle et les machines
virtuelles peuvent varier selon la solution utilisée pour assurer la bonne exécution de
l'environnement invité (i.e. l'environnement logiciel de la machine virtuelle)
et plus particulièrement du noyau invité.\\
Pour répondre à ce besoin, il existe trois solutions:
\begin{itemize}
  \item La virtualisation complète
  \item La paravirtualisation
  \item La virtualisation matérielle
\end{itemize}

\begin{figure}
  \label{fig:vmtype1}
  \input{figures/vmtype1.tex}
  \caption{Architecture logicielle générique des solutions de virtualisation de type I}
\end{figure}

\subsubsection{Principe de la virtualisation complète}

\paragraph{}
Historiquement la plus ancienne, elle fut formalisée en terme d'exigences par
G.J. Popek et R.P. Goldbergi \cite{popek} dès 1974. Le principe est de pouvoir
exécuter un environnement logiciel dans une machine virtuelle tout en
assurant que son comportement est identique à une exécution native.
L'environnement invité n'est pas modifié, et c'est au travers des exceptions
processeurs dues à l'exécution en tant qu'environnement invité que le moniteur
de machine virtuelle réagit à des traitements qui ne peuvent être exécutés
directement par l'environnement invité sans impact sur la sécurité et la
sûreté du système.\\
Dans leur Article \cite{popek}, G.J. Popek et R.P. Goldberg  décrivent les
incompatibilités du matériel avec ce type de principe. En effet, seules les instructions générant
des exceptions processeur peuvent être substituées par le moniteur de machine
virtuelles. Les
instructions sensibles \cite{popek} ne générant pas d'exception sont une problématique
fortes car ces dernières peuvent s'exécuter sans que le moniteur de machines
virtuelles en soit informé. La conséquence peut être plus ou moins grave car
ces dernières impactent la sûreté et la sécurité du système.

\paragraph{}
Afin de pouvoir virtualiser des environnements logiciels sans l'aide du matériel
et sans modifier l'environnement à virtualiser, plusieurs solutions ont été
mises en place, entre autre par VMWare. Dans ses
solutions, ce dernier est capable de déterminer les instructions sensibles qui seront
demandées par l'environnement invité et de modifier ces dernières afin
d'injecter des traitements simulés en lieu et place. Ce type de solution est
transparent pour l'environnement invité, mais génère des variations dans le
coût d'exécution de ces traitements, du fait de la simulation. La mesure de
ces variations par le logiciel font parties des mécanismes lui permettant de détecter que
son environnement d'exécution est virtualisé.

\subsubsection{Principe de la paravirtualisation}

\paragraph{}
Afin de limiter la problématique de simulation des traitements consécutifs à
la détection d'instructions incompatibles avec la virtualisation, une solution
a été définie visant à impliquer le noyau de l'environnement invité dans le
support de la virtualisation. Le noyau est alors modifié pour substituer les
instructions sensibles par des appels volontaires à des fonctions du moniteur
de machine virtuelle, appelés hypercalls. Dans ce cas, le moniteur réagit soit en
exécutant le traitement initial avec les droits qui sont les siens, soit en
simulant le traitement.

\paragraph{}
Il existe un certain nombre de solutionis implémentant les mécanismes de paravirtualisation.
C'est par exemple le cas de Xen ou encore de la solution propriétaire PikeOS
de Sysgo. La limitation du principe de virtualisation est dûe à la nécessité 
d'adaptation du noyau invité à l'Application Programming Interface (API) du moniteur de machine virtuelle. Cette
API n'étant pas normalisée, cela implique une implémentation souvent
spécifique à la solution de virtualisation utilisée. Cela implique également
d'avoir accès au code source du noyau invité, limitant ainsi les choix en
terme d'environnements paravirtualisables.

\subsubsection{Principe de la virtualisation matérielle}

\paragraph{}
Définie initialement par IBM, puis reprise depuis le début des années 2000 par
Intel et AMD, la virtualisation matérielle consiste à intégrer dans le
matériel des aides à la virtualisation. Le but est de limiter la complexité du
moniteur de machines virtuelles, ce dernier reposant sur des instructions
privilégiées de configuration du matériel pour gérer des environnements
matériels d'exécution des machines virtuelles séparés. Un exemple de solution
est l'architecture VT-d d'Intel, proposant des structures de configuration et
d'hébergement matérielles nommée VMCS (Virtual Machine Control Structure) qui
dupliquent les registres sensibles comme le PDBR. Lorsque le moniteur de
machine virtuelle ordonnance l'environnement virtualisé, il indique au
processeur le VMCS à utiliser en lieu et place des registres utilisés
par le moniteur lui même. La solution s'appuie également sur des élément de
filtrage d'accès au périphériques (nommés I/OMMU pour Intel, SMMU pour ARM
Cortex A15, A53 et A57). Il n'existe à ce jour aucune norme et chaque fondeur possède
ses propres instructions.\\
Cette solution n'est pas considérée ici car incompatible avec les exigences de
portabilité.

\subsection{Les solution de virtualisation type II}

\paragraph{}
Les soltions de virtualisation type II s'appuient sur la présence d'un hôte qui
accueille le moniteur de machine virtuelle comme un processus. Ce dernier est
donc lui-même ordonnancé par l'hôte. Ce type de solution de virtualisation
correspond à des produits tels que VMWare Workstation ou encore VirtualBox.
La Figure \ref{fig:vmtype2} décrit l'architecture logicielle d'une telle
solution.
\begin{figure}
  \label{fig:vmtype2}
  \input{figures/vmtype2.tex}
  \caption{Architecture logicielle d'une solution de virtualisation type II}
\end{figure}

\paragraph{}
Ces solutions étant basées sur des hôtes de type GNU/Linux ou Windows dans
lesquels elles s'exécutent comme des tâches non temps réel, elles sont
donc incompatibles avec les contraintes de temps réel, y compris lorsque ces
dernières sont relativement souples. Elles sont également incompatibles des exigences de
sécurité forte, du fait de la non-certifiabilité du système hôte.

\paragraph{}
Les solutions de moniteurs de machine virtuelle utilisant ce type
d'architecture ne permettent pas d'assurer un cloisonnement spatial fort
(répartition stricte en mémoire physique) ni un cloisonnement temporel fort
(l'ordonnancement des machines virtuelles étant assujetti aux propriétés de
l'ordonnanceur de l'environnement hôte).

\subsection{Les solutions de virtualisation applicative}

\paragraph{}
Les solutions de virtualisation applicative sont parentes des solution de
virtualisation de type II dans le sens où elles s'appuie sur un hôte.
Cependant, celles-ci ne nécessite pas la présence de noyau invité. Elles
permettent d'instancier des Environnements Virtuels dont le noyau est partagé
entre ces derniers ainsi qu'avec l'hôte. La gestion de
l'ordonnancement des taches des VEs est géré directement par
l'ordonnanceur de l'hôte, sous forme de groupes de tâches autonomes. Ce type
d'ordonnancement se rapproche ainsi du principe d'ordonnancement hiérarchique
qui sera décrit dans le Chapitre \ref{sec:hierarchique}.\\
Un exemple de solution de virtualisation applicative est la solution LXC
(LinuX Containers) de Linux. La Figure \ref{fig:vmlxc} décrit
l'architecture logicielle d'une telle solution. 

\paragraph{}
Les solutions de virtualisation applicatives permettent de créer des VEs
dont le système de fichiers, le jeu de tâches, les interfaces réseau et la
vision du stockage de masse sont séparés. 

\begin{figure}
  \label{fig:vmlxc}
  \input{figures/vmlxc.tex}
  \caption{Architecture logicielle d'une solution de virtualisation applicative}
\end{figurew

\section{Les interactions entre le matériel, la sécurité et le temps réel}

\subsection{Principes de base de la MMU}

\paragraph{}
Un applicatif, construit dans un fichier binaire lors de l'édition de lien, possède un certain nombre de sections
servant de conteneur à des données différentes, et ayant en conséquence des propriétés de sécurité (en terme d'accès)
également différente. De ce fait, lors du déploiement de ces différentes section en mémoire vive, il est nécessaire
de   considérer   pour   chacune   de   ces   sections   des    propriétés    d'accès    spécifiques.

\paragraph{}
Dans un fichier binaire au format ELF \cite{DefElf}, il existe plusieurs
sections, dont entre autres:
\begin{itemize}
  \item la section .text, contenant la partie exécutable du programme
  \item la section .rodata, contenant les données en lecture seule (e.g. les chaînes de
    caractères)
  \item la section .data, contenant les donneés en lecture et écriture
\end{itemize}

\paragraph{}
Lorsque plusieurs applicatifs s'exécutent de manière concurente, il est
important de les cloisonnner dans des environnement logiques séparés, ceci
afin d'éviter que ces derniers puissent interagir sans contrôle, par simple
accès à la mémoire physique.

\paragraph{}
Il existe plusieurs niveaux d'abstraction de la mémoire physique :
\begin{itemize}
\item Sans aucune abstraction, on est en mode dit  {\it  flat  memory}.
L'ensemble  des  logiciels s'exécutant en mémoire traitent des adresses physiques.
Ainsi,  ils  connaissent  leur  position  en mémoire,   et    ont    une    visibilité
sur    l'ensemble    de    la    mémoire    physique.\\
De ce fait, il leur est possible d'accéder au contenu des autres applicatifs en  cours  d'exécution.
Il devient ainsi aisé d'espionner les sections d'autres logiciels, comme les
section .data ou .rodata, dans  lesquelles   il   est   possible   de   récupérer   par   exemple   des   mots   de   passe.\\
De plus, l'accès à la pile des autres applicatifs peut permettre de dériver
leur exécution vers un bloc de code spécifique, en remplaçant l'adresse de la fonction parente par une adresse  dont  on
maîtrise le contenu. Du fait que la pile est par essence accessible en écriture, tous les processus
peuvent modifier les champs d'un processus donné en mode {\it flat memory}, à partir  du  moment  où
l'adresse physique de sa pile est connue.
\item Décomposition de la mémoire physique en segment.  Il s'agit du mode  de  segmentation  présent
uniquement sur les processeur de la famille x86.  Ce mode étant aujourd'hui remplacé  par  le
système de pagination, il ne sera pas décrit ici.
\item En s'appuyant sur une abstraction de la mémoire physique, appellé {\it mémoire virtuelle}. On
parle alors de mode dit de  {\it  pagination  mémoire}.   Un  processus  donné  possède  son  propre
environnement mémoire appelé Espace d'adressage (AS pour Address Space), dont la taille correspond à la valeur maximum d'un mot mémoire (e.g. dans un
processeur 32 bits, l'environnement  mémoire  virtuelle  à  une  taille  de  quatre  giga-octets).\\
Le processus, lors de son  exécution,  ne
traîte alors que des adresses virtuelles, dont la  translation  en  adresses
physiques  se  fait  au travers d'un élément matériel nommé Memory Management
Unit (MMU).
\end{itemize}

\paragraph{}
La gestion de la pagination est gérée par la MMU, sous les ordres du
noyau du système d'exloitation. Ce dernier construit l'association entre
mémoire physique et mémoire virtuelle dans des structures en mémoire nommées
tables de pages (Pages Tables). Sur les architectures x86, ces dernières sont accédées par la MMU
via un registre nommé PDBR. Ce dernier pointe vers la racine de l'arbre de
mapping de mémoire virtuelle du processus en cours, et s'appuie sur ce dernier
pour retrouver l'adresse mémoire physique associée à l'adresse mémoire
virtuelle demandée par le c{\oe}ur du processeur. Afin d'optimiser les accès à cet
arbre de page, la MMU s'appuie sur un cache local nommé TLB (Translation
Lookaside Buffer), réduisant ainsi les requêtes au contrôleur mémoire.

\paragraph{}
Le   principe   de   fonctionnement   de   la   pagination    fournit    plusieurs    avantages    :
\begin{itemize}
\item A chaque processus est associé un AS (espace d'adressage) qui  lui  est  propre.
Ainsi, il se voit seul en mémoire et il ne possède plus la capacité d'accéder aux autres  processus.
En effet, l'accès à la mémoire physique se fait via un accès à une adresse  virtuelle.   Pour  qu'un
processus y accède, une association entre adresse physique et adresse virtuelle, locale à cet espace
d'adressage doit avoir été faite par le noyau au préalable, via configuration de la MMU. Lors de la
création d'un processus, seules les adresses physiques correspondant  à  ces  propres  données  sont
intégrées à l'espace d'adressage, interdisant à ce dernier  d'accéder  aux  données  de  tout  autre
processus.
\item De plus, la gestion de la fragmentation de la mémoire centrale, dépendant des  allocations  et
désallocations successives de la mémoire, est rendue opaque en mémoire virtuelle. Des espace mémoire
physiques disjoints peuvent être positionnés de manière contigüe dans l'espace
d'adressage virtuel,  faisant croire  au  processus  que  ses  données  sont
contigües  sans  pour autant  qu'elles  le   soient.
\item Le contrôleur MMU permet d'associer à chaque page mémoire des propriétés
en terme d'accès:\\
\begin{itemize}
  \item Niveau d'exécution minimum (mode administrateur ou utilisateur)
  \item droits d'accès (lecture, écriture et exécution)
\end{itemize}
Ces droits d'accès permettent, dans un espace d'adressage donné, de limiter
les accès à chaque page. Ainsi, les sections .rodata ou .text du processus ne
sont pas accessibles en écriture. Une telle protection réduit la surface
d'attaque, rendant plus difficile la corruption de de l'application.
\end{itemize}
La Figure \ref{fig:mmu} décrit le principe général de pagination entre mémoire
physique et mémoire virtuelle.

\begin{figure}
\label{fig:mmu}
\input{figures/mmu_as.tex}
\caption{Principe général du mapping de page mémoire dans les espaces
d'adressage des processus}
\end{figure}

\paragraph{}
Les attaques entre les différents processus ne sont pas les  seules  possibles.   Il  est  également
possible de corrompre le comportement d'un processus donné, pour lui faire faire un  traitement  non
prévu.\\
Une des attaques les plus classiques est l'injection  de  {\it  shellcode}.   L'attaque  consiste  à
intégrer quelque part dans l'espace d'adressage du processus  une  suite  d'instructions,  avant  de
corrompre   sa   pile   pour   provoquer   un   branchement   vers   ce    bloc    d'instructions.\\
Cette attaque est donc faite en deux temps, en positionnant des instructions
en  mémoire puis en forçant le branchement.En général, les shellcodes sont placés dans la pile,  en
amont du positionnement courant du pointeur de pile. Il convient, pour éviter ce type d'attaque, de
rendre l'espace mémoire utilisé pour la pile non-exécutable.  A l'inverse, l'espace  mémoire  où  se
situe le code du processus doit lui être exécutable, mais ne doit pas  être
accessible en écriture,  ce  dernier étant immuable durant l'exécution du processus.\\
On définit ici des droits d'accès à des  espaces  mémoire  (lecture,  écriture,  exécution)  variants
selon le type de données qui y sont hébergées. Les MMU modernes, lors de la création d'une association
entre mémoire physique et mémoire virtuelle,  définissent  ces  trois  propriétés,  fournissant  un
durcissement   supplémentaire   pour   réduire   les   risques   de   corruption   d'un   processus.

\subsection{A propos des contrôleurs de cache}
\label{sec:notions_caches}

\subsubsection{Description générale}

\paragraph{}
Le \index{Cache}cache est une mémoire statique à accès  rapide,  permettant  d'accélérer  les  accès
mémoire du processeur, en copiant les données (code et data) de manière à ce  qu'elles  soient  accessibles
directement  par  ce dernier  sans nécessité de  transfert   sur   le   bus \cite{cachalgo}.

\paragraph{}
La figure \ref{fig:cache_soa} montre une architecture mémoire générique, avec plusieurs  niveaux  de
cache, suivis d'un bus d'accès mémoire et  du  controlleur  mémoire.

\begin{figure}
\input{figures/cache_architecture.tex}
\caption{Architecture en couche générique\label{fig:cache_soa}}
\end{figure}

\paragraph{}
Il existe dans les processeurs modernes plusieurs niveaux de cache :
\begin{enumerate}
\item Le cache niveau 1, le plus petit et accessible le plus rapidement.  Ce  cache  est  spécialisé  :
il définit un conteneur dit {\it cache code},
pour les éléments de code, et un  conteneur  dit  {\it  cache
data},   pour   le   reste   des    données.
\item Le cache niveau 2, de taille plus  conséquente,  mais  dont  l'accès  est  moins  rapide  que  le
cache L1. Ce cache n'est pas spécialisé.
\item Le cache niveau  3,  de  taille encore plus  grande. Ce dernier est en
  général partagé  entre  les  c{\oe}urs  sur  les architectures multi-c{\oe}urs. Ce cache n'est pas spécialisé. Ce dernier n'est pas toujours présent.
\end{enumerate}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\textbf{Niveau de mémoire}} & {\textbf{Temps d'accès}} \\
\hline
Cache L1 & 2-8 nano secondes \\
Cache L2 (partagé) & 5-12 nano secondes \\
Mémoire centrale & 10-60 nano secondes \\
\hline
\end{tabular}
\end{center}
\label{tab:cache_access_time}
\caption{Temps d'accès mémoire sur un Intel Core 2 duo}
\end{table}

\paragraph{}
Le tableau \ref{tab:cache_access_time}\footnote{confer \url{http://expertester.wordpress.com/2008/05/30/core-2-duo-vs-pentium-dual-core/}} fournit
un exemple de comparaison  du temps  d'accès  à   une   donnée   selon   son   positionnement   en   mémoire.

\paragraph{}
Les algorithmes de cache ont divers niveaux d'associativité :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Les algorithmes de type {\textit{one-way associative}} associent  une  cellule  mémoire  à  une
et une seule cellule de cache.  Il s'agit d'un algorithme surjectif, du fait de la taille du  cache.
Le choix de remplacement étant de type $1:1$, aucun algorithme spécifique n'est utilisé. Les données
sont écrites au seul emplacement possible en cache.
\item Les algorithmes de type {\textit{N-way associative}} associe une cellule mémoire avec
{\it N} (resp. 2, 4, 8, ...)
cellules de cache. Cette associativité permet d'optimiser l'écrasement des cellules du cache en donnant
un choix de positionnement à la cellule mémoire à mettre en cache.  Les algorithmes de  ce  type  se
différencient par leur politique de choix de la cellule à remplacer.
\end{itemize}
On a ainsi :
\paragraph{{\it N}-Way Set Associative}
Cet algorithme permet, pour chaque élément mémoire à charger  en  cache,  de  pouvoir  se  placer  à
{\it N} endroits dans le cache.  Ces {\it N} positions sont dépendantes  de  l'adresse  physique  en
mémoire centrale de la donnée.\\
Le choix de  la  cellule  parmi  la  liste  des  cellules  admissibles  se  fait  au  travers  d'un
algorithme de gestion de cache tel que décrit plus loin.

\paragraph{Direct Mapped}
L'algorithme Direct Mapped est un algorithme simple, optimisant le coût d'exécution du choix de la cellule.
Un élément mémoire n'est placé qu'à un seul endroit dans le cache, en fonction de son adresse physique,
quelle que soit la donnée précédemment présente.


\paragraph{}
Lorsqu'une donnée n'est pas en cache, on parle de {\it cache-miss}. Le processeur va successivement
vérifier  les  caches  L1,  L2  et  L3  (si présent),  avant   d'initier   une   recopie   à   partir
de    la    mémoire    centrale.

\paragraph{}
Il existe plusieurs algorithmes de gestion du cache, souvent de type
probabiliste. De récents travaux de recherche tendent à montrer que ces caches
peuvent être pris en compte dans les architectures temps réel \cite{guan2012wcet}.\\
Cependant, les systèmes temps réel très  contraints d'aujourd'hui,  comme  dans  l'aviation  ou  le  spatial,
utilisent encore parfois des architectures avec cache désactivé.

\subsubsection{Positionnement du cache}

\paragraph{Cache \index{Cache!Look-aside}look-aside}
Cette architecture place le cache et la mémoire centrale côte à côte.  Ainsi, le CPU peut accéder  à
la mémoire centrale sans passer par le cache.\\
Le principe de fonctionnement est le suivant :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le processeur démarre un cycle de lecture
\item Si le cache possède la donnée en local (cache hit), il répond au processeur, et clôture le  cycle  de
lecture sur le bus.
\item Si le cache ne possède pas la donnée (cache miss), il laisse la mémoire centrale  répondre  au
processeur,  et  espionne  les  données  transitant  sur  le  bus  afin  de  se   mettre   à   jour.
\end{itemize}

\begin{figure}[h]
\input{figures/cache_memory_bus.tex}
\caption{Comparaison   des   architectures   de   type   cache   through   et    cache    lookaside}
\label{fig:cache_look_aside}
\end{figure}


\paragraph{Cache \index{Cache!Look-through}look-through}
Cette architecture place le cache en coupure entre le processeur et la mémoire
centrale. Le processeur passe donc par le cache pour l'accès mémoire.\\
Le principe est le suivant :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le processeur démarre un cycle de lecture
\item Si  le  cache  possède  la  donnée  en  local  (cache  hit),   il   répond   au   processeur.
\item Si le cache ne possède pas la donnée, il transfert la requête de lecture sur le bus. La mémoire
centrale renvoie la donnée, qui est mise en cache en même temps qu'elle est renvoyé  au  processeur.
\end{itemize}

\begin{figure}[h]
\input{figures/cache_look_through.tex}
\caption{Architecture de cache en mode {\it look-through}}
\label{fig:cache_look_through}
\end{figure}

Cette architecture provoque des cache miss plus long (vérification par le cache avant  de  faire  la
requête au contrôleur de bus), mais permet d'utiliser le cache indépendamment des autres processeurs
(pour un cache local).

%%%%%%%%%%%%%%%%%%%%% END OF CORRECTION

\subsubsection{\index{Cache!Consistance}Consistance du cache}

\paragraph{}
Du fait que le cache contient des duplicatas d'une partie de la mémoire, il est important de s'assurer
que la copie  est  l'originale  restent  identique.   À  défaut,  cela  peut  entraîner  des  bogues
logiciels,  difficilement  détectables.

\paragraph{}
La consistance du cache peut être impactée de différentes manières, dont voici une liste non-exhaustive :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Mise à jour de la mémoire par un contrôleur DMA
\item  Écriture  en  mémoire  par  un  autre  processeur  (architecture   multi-CPU/culti-c{\oe}urs)
\item Modification de la donnée en cache sur un autre processeur
\end{itemize}

Il existe différentes méthodes permettant de garantir la consistance des données en caches. En voici
deux largement utilisées :

\paragraph{\index{Cache!Snooping}Snooping}
Le snooping consiste en l'espionnage du bus d'adresse, afin de détecter  une  modification  sur  une
adresse mémoire contenue en cache.\\
Il y a deux manières de réagir à une détection positive :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Prévoir un rechargement de la cellule impactée (méthode dite {\it write-back}). Le rechargement ne
se fait pas nécessairement sur le moment, mais peut être ordonnancée pour s'exécuter au moment du prochain
accès à cette cellule, ou avant, selon les implémentations.
\item Désactiver la cellule impactée.  Le prochain accès au données de cette cellule  provoquera  un
{\it    cache-miss},    suivi    d'une    recopie    à    partir    de    la    mémoire    centrale.
\end{itemize}

\paragraph{\index{Cache!Snarfing}Snarfing}
Le snarfing consiste à associer la détection d'une modification d'une donnée en cache à l'espionnage du
bus de donnée. En espionnant le bus, il est capable à tout instant de mettre à jour la cellule impactée,
et de maintenir la consistance.

\subsubsection{Synchronisation caches/mémoire centrale}

\paragraph{}
Lorsque des données sont modifiées en local sur le cache, il est nécessaire de mettre à jour la mémoire
centrale afin de prendre en compte ce changement.

\paragraph{write-back}
Le cache garde en mémoire, pour chaque cellule, sont état. Lorsque la cellule est écrasée, ou que le cache
est  vidé,  les  données  modifiées  localement  sont  synchronisées  avec  la  mémoire  centrale.\\
La mise à jour de la mémoire centrale n'est pas synchrone avec la modification locale. Cela implique, au
niveau logiciel, de prévoir les problèmes d'incohérence sur les architectures multi-c{\oe}urs, au travers
de l'usage des {\it \index{Memory barrier}memory barriers}.

\paragraph{write-through}
Une écriture sur cache implique une remontée synchrone de la mise à jour de la donnée, afin de maintenir
une cohésion.

\subsubsection{Les algorithmes de gestion de cache}

\paragraph{\index{Cache!LRU}Least Recently Used}
L'algorithme LRU s'appuie sur les dates d'accès aux items du cache. En cas de cache-miss, c'est l'élément
qui a été accédé il y a le plus longtemps qui est remplacé.\\
Pour des processus faisant des accès fréquents à un même espace mémoire, comme dans le cas de processus
calculatoires par exemple, LRU est efficace.

\paragraph{\index{Cache!MRU}Most Recently Used}
L'algorithme MRU, à l'opposé du LRU, remplace l'élément dont l'accès est le plus récent. Il considère
l'hypothèse  selon  laquelle  un  élément  accédé   très   récemment   ne   le   sera   plus   avant
une    certaine    durée.\\
MRU   est   plus   efficace   pour   des   processus   faisant   des   accès   mémoire    cycliques.

\paragraph{\index{Cache!Pseudo-LRU}Pseudo-LRU}
L'algorithme Pseudo-LRU utilise un arbre binaire représentant l'ensemble des cellules de caches pouvant
être remplacées. Chaque ensemble de cellules posséde un tag fournissant une indication sur l'emplacement
(branche droite, branche gauche) de la cellule qui sera remplacée.  Ce tag est mis à jour  à  chaque
traversée de l'arbre.

\paragraph{}
Il s'agit d'un algorithme jugé efficace en comparaison au LRU pour les caches dont l'associativité est
supérieure à 4.  Il est utilisé dans les architectures Intel 486, et dans les architecture  PowerPC,
comme le PowerPC G4 de Freescale \cite{plru}. Le schéma \ref{fig:cache_plru_e300} est tiré du manuel
de   référence    du    processeur    e300,    utilisé    dans    les    power    quicc    2    pro.

\begin{figure}
\input{figures/cache_plru_e300.tex}
\caption{Automate   à   état   de    l'algorithme    pseudo-LRU    du    Power    Quicc    2    Pro}
\label{fig:cache_plru_e300}
\end{figure}


\paragraph{\index{Cache!Segmented-LRU}Segmented LRU}
L'algorithme SLRU utilise deux segments de caches complémentaires, dont les cellules sont ordonnées de
la dernière utilisée (MRU) à la plus anciennement utilisée (LRU) :

\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le premier segment, nommé {\it probationary segment}, est celui sur lequel sont mises les données
mémoire, en cas de cache miss. C'est également le seul segment où des données peuvent être retirées
du cache.
\item Le second segment, nommé {\it protected segment}, contient la liste des cellules de cache ayant
été accédées au moins deux fois.
\end{itemize}
Le schéma \ref{fig:slru_1} montre ainsi que l'algorithme de cache fait une différence entre des données
accédées à faible fréquence et des données accédées régulièrement.

\begin{figure}
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_1.tex}
\end{minipage}\hfill
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_2.tex}
\end{minipage}\hfill
\caption{Intégration et déplacement des données accédées fréquemment}
\label{fig:slru_1}
\end{figure}


\paragraph{}
En effet, lorsqu'une cellule est accédée une seconde fois après un premier cache miss, celle-ci  est
transférée  sur  le  segment  protégé,  en  tête  de  liste  coté  MRU   (Most   Recently   Used).\\
Cela a pour conséquence, si le segment protégé est plein, de faire descendre la cellule la plus anciennement
accédée sur le segment probatoire, en tête de liste coté MRU, ceci afin de donner une deuxième chance à
toute donnée accédée au moins deux fois. Lorsque le segment probatoire est plein et que de nouvelles
données y sont intégrées, que ce soit via un déplacement d'une cellule du segment protégé ou du fait d'un
cache miss, la cellule la moins récemment utilisée et écrasée.

\begin{figure}
\input{figures/slru_3.tex}
\caption{Principe de deuxième chance du SLRU}
\end{figure}

\paragraph{}
Cet algorithme fait donc une différence entre les données accédées souvent (au moins deux fois) et les
données   accédées   rarement   (une   fois)   pendant    leur    durée    de    vie    en    cache.
Il  permet  donc  d'optimiser  la  gestion  du  cache  lors   d'accès   mémoire   peu   fréquents.\\
La  taille  des  deux   segments   est   dépendante   dynamiquement   du   schéma   d'entrée/sortie.

\paragraph{\index{Cache!LFU}Least Frequently Used}

Lors d'un cache miss, la cellule utilisée pour charger les données mémoire est celle dont le contenu est
utilisé le moins fréquemment. En cas d'égalité de fréquence, l'algorithme LRU est utilisé pour départager
les cellules.



