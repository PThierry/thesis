%%
%%
%% hardware_impacts.tex for thesis in /doctorat/these/tex
%%
%% Made by Philippe THIERRY
%% Login   <Philippe THIERRYreseau-libre.net>
%%
%% Started on  Fri Mar 12 16:36:41 2010 Philippe THIERRY
%% Last update Mon Aug 30 17:00:21 2010 Philippe THIERRY


\chapter{Concepts}
\label{sec:notions}
\doMinitoc

\section{Principes généraux de la sécurité}

\subsection{Terminologie militaire de la sécurité}

\subsubsection{Les niveaux de confidentialité}

\paragraph{}
Il existe un grand nombre de niveaux de confidentialité formalisés dans le vocabulaire
militaire. Pour la France, on parle des niveaux suivants, du plus faible au
plus élevé :
\begin{itemize}
  \item Diffusion Restreinte (DR). Bien que très employé, c'est le seul niveau pour
    lequel aucune peine n'est définie dans le code pénal en cas de non-respect
    des exigences de confidentialité associées.
  \item Confidentiel Défense (CD). Premier niveau impliquant un risque pénal. Ce
    niveau peut être présent jusque dans les véhicules blindés positionné sur
    le théâtre d'opération.
  \item Secret Défense (SD). Ce niveau est en général présent jusque dans les
    centre de contrôle militaire locaux au théâtre d'opération. Il s'agit des
    centres de contrôle militaire déployés dans les pays en guerre dans les
    bâtiments civils protégés (type aéroport) ou dans des bâtiments militaires
    proche de la zone de conflit (frégate, porte-avions).
  \item Très secret défense (TSD). Plus haut niveau de sécurité.
\end{itemize}
Il existe de plus un grand nombre de niveaux de confidentialité lié à l'OTAN
(Organisation du Traité de l'Atlantique Nord, NATO en anglais), ou à diverses organisations
impliquant une force armée. Il existe ainsi les niveaux Nato Restricted (NR),
Nato Confidential (NC), Nato Secret (NS) et encore bien d'autres niveaux. Il
existe une problématique claire de comparaison des niveaux France avec les
niveaux OTAN. Ainsi, peut-on transférer des données du niveau NS au niveau SD
? Cette problématique est encore aujourd'hui traité au cas par cas.

\paragraph{}
Lorsqu'un niveau de confidentialité est mis en place, c'est au travers d'un
système d'information respectueux d'une politique de sécurité donnée. Cette
politique de sécurité (nommée PSSI - Politique de Sécurité d'un Système
d'Information) définit des exigences de sécurité pour assurer la
confidentialité, l'intégrité et la disponibilité (au sens sécuritaire du
terme) du domaine. Ces exigences sont mise en place sous forme:
\begin{itemize}
  \item d'un choix d'architecture réseau, logicielle et matérielle
  \item de la définition d'algorithmes de chiffrements spécifiques et de
    solutions implémentant ses algorithmes
  \item d'une définition d'un processus de gestion du domaine (protection physique et logique)
  \item d'une définition des responsabilités et des rôles de chaque
    intervenant dans le domaine
\end{itemize}
Plus le niveau de confidentialité est élevé, plus la PSSI est stricte et
complexe.

\subsubsection{Changement de niveau et problématique rouge/noir}

\paragraph{}
On appelle domaine rouge un domaine de sécurité dans lequel les données
transitant ne sont pas protégées. On parle alors de données rouges. Le réseau
faisant transiter ces données dans un domaine rouge est alors appelé réseau
rouge.

\paragraph{}
On appelle domaine noir un domaine de sécurité dans lequel les données
transitant sont modifiées pour en assurer la confidentialité, l'intégrité et
la disponibilité. On dit alors que les données sont noircies. En général, un
domaine noir est en charge de faire transiter des données dont le niveau de sécurité
est supérieur à celui du domaine lui-même. Ainsi, faire
transiter des données de niveau Confidentiel Défense dans un domaine de niveau
Diffusion Restreinte ne peut être fait que si ces données sont préalablement
noircies.\\
Noircir des données implique en général un chiffrement de ces dernière, mais
pas seulement. Le profil de flux peut être modifié afin de rendre difficile
la détection du type de communication sur la base de son profil. Du bruit peut
ainsi être généré et du {\it padding} incorporé afin d'empêcher toute déduction sur
simple écoute du flux noir.

\paragraph{}
La problématique de passage d'un domaine rouge à un domaine noir est appelé
problématique rouge/noir. On la retrouve principalement dans les réseaux radio
militaires, mais reste un principe général applicable à toute interconnexion
de réseaux militaires.

\subsubsection{niveau de confidentialité et droit d'en connaître}
\paragraph{}
On appelle droit d'en connaître l'ensemble de connaissances minimum suffisant
pour un sujet (une personne ou encore un processus logiciel) pour mener à bien
une tâche. Ainsi, une personne ayant une habilitation Confidentiel Défense sur
un projet donné n'a pas besoin pour autant de connaître l'ensemble des
informations classifiées Confidentiel Défense de ce dernier. On lui fournit alors
uniquement les informations nécessaires à son travail, appelé droit d'en
connaître. Le principe est le même pour les processus logiciel faisant du
traitement sur des données d'un niveau de confidentialité donné.

\subsection{Fuite d'informations: Canaux cachés et canaux auxiliaires}

\paragraph{}
Assurer la confidentialité est une tâche complexe. En effet, limiter l'accès à
la donnée n'est pas suffisant pour assurer sa protection. En effet, il est
possible de récupérer des informations sur la donnée de plusieurs manières,
sans y accéder directement. On parle alors de cannal auxiliaire ou de cannal
caché, selon la méthode.

\subsubsection{Canal auxiliaire et étude comportementale}

\paragraph{}
Le principe d'un canal auxiliaire consiste à s'appuyer sur une etude
comportementale non-invasive sur des fonctions ou des traitements afin de
récupérer des informations sur les données traitées. La plupart des études sur
le sujet sont dans le domaine de la
cryptanalyse \cite{sidechancube}\cite{sidechan}. Ces
différentes études démontrent la capacité, au travers de l'étude de la consommation électrique
des différents éléments d'un processeur \cite{sidechancube} ou via l'étude de
l'impact des caches processeurs sur le comportement temporel de
l'algorithmique \cite{sidechan}, de récupérer des données sur la clef
de chiffrement utilisée.\\
Le risque lié au canal auxiliaire est connu depuis longtemps et ont permis de
démontrer que des algorithmes cryptographique réputés sûr restent fragiles à
ce type d'attaque. C'est le cas par exemple d'AES \cite{aes} ou de
Serpent \cite{serpent}.

\subsubsection{Canal caché pour le transfert de données}

\paragraph{}
Le principe d'un canal caché consiste en la capacité à utiliser la bande
passante d'un canal logiciel ou matériel pour faire transiter des données
pour lesquels il n'est pas initialement prévu. L'usage d'un canal caché
implique un usage volontaire de part et d'autre pour émettre les données et
pour les recevoir. Il est donc invasif, contrairement aux canaux auxiliaires.\\
Un exemple typique est de s'appuyer sur la période inter trame d'un flux
réseau donné pour fournir une information \cite{berk2005detection}. En faisant varier la période, il
est possible d'émettre de l'information, qui peut être ainsi interprété par un
récepteur. De manière générale, les canaux cachés permettent l'émission de
données de faible taille, car leur débit est en général très faible. Ils sont
cependant tout à fait suffisant pour faire transiter des données de type
cryptographie (clefs symétrique et/ou asymétrique) ou des messages courts.\\
Le principe d'exploitation d'un canal caché est également utilisé dans les
systèmes virtualisés pour transmettre de l'information entre les machines
virtuelles. C'est en général le comportement des contrôleurs de caches qui
sont alors exploités \cite{xu2011exploration}\cite{zhang2012cross}.

\subsection{Ce que cible la sécurité logicielle}

\paragraph{}
Dans leur globalité, les architectures de sécurité sont là pour répondre à ces
trois besoins initiaux. Selon le niveau de certifiabilité, les architectures
de sécurité peuvent prendre plusieurs formes, allant d'une séparation
matérielle complète et strict jusqu'à la simple intégration d'une politique de
gestion de contrôle d'accès (propriétaire, groupe, droit d'accès) au fichiers,
comme par exemple les systèmes DAC (Discretionary Access Control), MAC
(Mandatory Access Control) ou RBAC (Role-based Access Control).


\subsection{Cibles de sécurité et certifiabilité}

\paragraph{}
La complexité du système dépend du besoin de certifiabilité. On définit
initialement une {\it TOE} (Target Of Evaluation) \cite{cnssi_4009}, définissant ce qui, dans le
système cible, doit être évalué, ainsi que le niveau de l'évaluation. Les
Critères Communs définissent une échelle dans l'évaluation sécuritaire
dénotée EAL (Evaluation Assurance Level) allant de 1 (la plus basse) à 7 (la
plus haute). Le niveau de sécurité du TOE est donc dépendant du niveau
souhaité. Les systèmes Linux bien configuré en terme de sécurité peuvent
atteindre un niveau de certification de l'ordre de EAL4. Il faut cependant
prendre soin d'étudier précisément quel est le TOE. En effet, le niveau de
sécurité d'un système certifié en étant totalement autonome n'est plus assuré
si ce dernier est connecté à un réseau. Il en va de même en cas de
modification même légère du TOE.

\paragraph{}
Associé à la TOE, on définit la {\it Trusted Computing Base} (TCB) \cite{cnssi_4009}. Cette
notion définit l'ensemble des éléments logiciels et matériels de
la solution cible qui sont considérés comme de confiance. La corruption de
tout élément de cet ensemble génère en conséquence une faille dans les
propriétés de sécurité de la solution. Ainsi les éléments de la TCB doivent
non seulement être certifiables au plus haut niveau souhaité dans le cadre de
l'évaluation du système, mais doivent être également protégés contre toute
corruption extérieure (comme par exemple à partir d'un élément
non-certifiable). On leur impose alors une exigence d'{\it inviolabilité},
comme décrit dans les exigences générales des systèmes MILS, décrit dans le
Chapitre \ref{sec:mils} 

\section{Principes généraux des systèmes temps réel}

\subsection{Définition du temps réel}

\paragraph{}
Comme le montre le schéma \ref{fig:rt_hard_soft}, il existe trois grandes familles de taches :
\begin{itemize}
\item Les taches temps réel dur. Ces dernières doivent respecter scrupuleusement un
temps maximum d'exécution, faute de quoi l'utilisabilité des résultats de la tache est nulle.
\item Les taches temps réel souple. L'utilisabilité des résultats de la tache décroît avec son retard.
\item Les taches non temps réel. Il n'est pas définit de temps maximum d'exécution. L'utilisabilité des résultats
reste constante. La Figure \ref{fig:rt_hard_soft} montre les différences entre les différents types de tache.
\end{itemize}

\begin{figure}[h]
\input{figures/real_time_lines.tex}
\caption{Les trois grandes familles de taches}
\label{fig:rt_hard_soft}
\end{figure}

\subsection{Définitions préliminaires}
\label{sec:primarydef}

\paragraph{}
Une tache temps réel $\tau_{i}$ possède trois attributs principaux:
\begin{itemize}
\item Sa durée d'exécution pire coût, ou \gls{wcet} (Worst Case Execution Time), notée $C_{i}$
\item Sa deadline, définissant l'instant auquel elle doit avoir fini de s'exécuter, notée $D_{i}$
\item Sa période, définissant sa latence minimum d'instanciation, notée $T_{i}$. Chaque instance de tache est appellé un {\it job}
\item Le temps de réponse d'un job, correspondant à la durée entre son instanciation et sa terminaison, noté $R_{i}$
\end{itemize}
\`{A} ces trois attributs, d'autres viendront s'ajouter, en fonction du type de tache.

\paragraph{}
Un ensemble de taches temps réel est appellé un ensemble de taches, ou {\it taskset}. Un ensemble de tache possède
des propriétés dérivées des taches qu'il possède. Ces dernières sont présentée
dans la section \ref{sec:rttasks}.

\subsection{Définition du WCET}

\paragraph{}
Le \gls{wcet} ({\it Worst Case Execution Time}) définit la durée d'exécution maximum d'une tache. Associée à une {\it deadline},
elle permet, pour une liste de taches donnée :
\begin{itemize}
\item de connaître à l'avance si cette liste est ordonnançable ou si certaines taches ne pourront pas
être terminées à temps.
\item de définir une répartition temporelle des taches pour respecter la {\it deadline} de chacune.
\end{itemize}

\paragraph{}
Les attributs définis dans la Section \ref{sec:primarydef} doivent avoir les propriétés définies dans les équations \ref{eqn:rtprop_1},
\ref{eqn:rtprop_2} et \ref{eqn:rtprop_3} pour que chaque tache $\tau_{i}$ soit ordonnançable.


\begin{eqnarray}
\label{eqn:rtprop_1}
\forall i, C_{i} \le D_{i}\\
\label{eqn:rtprop_2}
\forall i, R_{i} \le D_{i}\\
\label{eqn:rtprop_3}
\forall i, D_{i} \le T_{i}
%U_{i}, \displaystyle{\sum_{i \in [1, n]}} U_{i} \le 1
%\label{eqn:rtprop_3}
\end{eqnarray}

\subsection{Définition des ensembles de taches}
\label{sec:rttasks}

\paragraph{}
Pour aborder la problématique temps réel, on définit un profil d'environnement logiciel, nommé ensemble de taches ({\it
\gls{taskset}}). Cet ensemble se définit comme suit :
\begin{itemize}
\item Une liste de taches $\tau$, chaque tache possédant un {\it WCET} et une {\it deadline} qui lui est propre
\item Chaque tache se matérialise sous la forme d'une {\it instance de tache}, correspondant à un processus logiciel.
\item Un profil d'arrivée des instances de tache (arrivée périodique, sporadique ou apériodique)
\end{itemize}

\paragraph{}
On parle alors de l'ordonnançabilité d'une tache, au travers de la capacité d'exécution de toutes ses instances.\\
Un ensemble de tache est nécessairement finit, et possède des caractéristiques fixes dans le temps.

\subsubsection{Les ensembles périodiques}

\paragraph{}
Les ensembles périodiques respectent la propriété suivante :\\
Chaque tache $\tau_{i}$ est instanciée à une période fixe $P_{i}$ connue.

\paragraph{}
Ainsi, en connaissant la date de la première instanciation d'une tache, on est capable de prédire les dates d'instanciation
de cette tache durant toute la durée de vie de l'ensemble de taches.

\paragraph{}
On peut de plus mesurer la charge maximum induite par cette tache, en s'appuyant sur $WCET(\tau_{i})$ et $P_{i}$. De
plus, la variation de la charge induite par cette tache dépend uniquement du de la variation du coût d'exécution de
chaque instance de $\tau_{i}$.

\subsubsection{Les ensembles sporadiques}

\paragraph{}
Les ensemble sporadiques respectent la propriété suivante :\\
Chaque tache $\tau_{i}$ est instanciées avec une période {\it minimum} $P_{i}$ définie.

\paragraph{}
On connaît donc la charge pire cas de chaque tache,  en s'appuyant sur $WCET(\tau_{i})$ et $P_{i}$.\\
Par contre, on ne peux pas prédire les dates d'instanciation successives de cette tache. De plus,
la variation de la charge induite par cette tache dépend à la fois du coût d'exécution effectif de
chaque instance de $\tau_{i}$ et de l'élongation de la période $P_{i}$.

\subsubsection{Les ensembles apériodiques}

\paragraph{}
Les ensembles apériodique respectent la propriété suivante :\\
Aucune tache n''est soumise à une période d'instanciation minimum. Une instance peut être créée n'importe quand,
indépendament de la finition de l'instance précédente.

\paragraph{}
On ne connaît pas la charge pire cas de chaque tache, la valeur de $P_{i}$ appartenant à l'ensemble $[0, +\infty[$

\subsubsection{Les ensembles fenêtrés}

Une fenêtre glissante de temps ${\Delta}t$ est définie, et on lui associe un nombre d'interruptions
maximum. Cette valeur est utilisée en entrée de la mesure du WCET.

\subsection{Rappels sur les politiques d'ordonnancement}

\paragraph{}
Les différents ordonnanceurs temps réel possèdent plusieurs propriétés quant à la gestion des taches.
Ce chapitre définit séparément chaque propriété, construisant ainsi un ensemble homogène. Chaque
ordonnanceur se situe à l'intersection des ensembles décrivant toutes ses propriétés.

\subsubsection{Par dynamique d'ordonnancement}

\paragraph{L'ordonnancement en ligne}

Les choix d'ordonnancement se font durant l'exécution, en fonction de l'état du système (nombre de taches,
priorités, durée d'exécution).

\paragraph{L'ordonnancement hors ligne}

La politique d'ordonnancement est définie par avance. Elle est donc optimisée pour un cas particulier de jeu
de taches ({\it taskset}), mais pose problème en cas d'impact non prévu sur la dynamique d'exécution (effets de caches, de surcharge
de bus, WCET trop grand ou trop faible, etc).

\subsubsection{Par schéma d'exécution}

\paragraph{Les ordonnanceurs préemptifs}

Les taches sont préemptées à une période prédéfinie, fixe ou dynamique, nommée {\it \index{quantum}quantum}.  En conséquence,
une tache s'exécute en plusieurs fois. Cela implique que l'ordonnanceur s'exécute plus souvent que
dans le cadre du mode non préemptif. La plupart du temps, l'ordonnanceur est de type {\it \index{time-driven}time-driven} (il s'appuie
sur une horloge pour régler sa période d'éxection). C'est le cas par exemple du Round-Robin, qui préempte les taches sur quantum.\\

\paragraph{Les ordonnanceurs non préemptifs}

Une fois élue, chaque tache est exécutée dans son intégralité. Cela écarte les problématiques de rechargement
du cache lié à la préemption de tache, et les algorithmes d'ordonnancement ont l'avantage de la simplicité.\\
Cependant, il est plus difficile de garantir le {\it \index{WCET}\gls{wcet}} que dans le cadre du mode préemptif. Un ordonnancement en mode
FIFO s'exécute sans préemption, faisant s'exécuter les taches les unes à la
suite des autres, dans l'ordre d'arrivée.\\
L'ordonnanceur peut également être de type {\it \index{event-driven}event-driven}, comme dans le cadre de préemption sur arrivée d'une tache
de priorité supérieure à celle en cours d'exécution. C'est le cas par exemple de l'EDF, qui ne préempte que sur l'arrivée d'une
tache plus prioritaire.

%\paragraph{Les ordonnanceurs mixtes}
%
%Selon la priorité de la tache, cette dernière s'exécute sans préemption ou avec préemption.
%
%\subsubsection{Par oisiveté}
%
%\paragraph{Les ordonnanceurs oisifs}
%
%Les ordonnanceurs oisifs ne profitent pas de la
%
%\paragraph{Les ordonnanceurs non oisif}
%
%\subsubsection{Par politique de priorisation}
%
%\paragraph{Les ordonnanceurs à priorité fixe}
%
%La priorité est associée à une tache au moment de sa création. Cette dernière est maintenue tout au long de la vie de cette
%tache.
%
%\paragraph{Les ordonnanceurs à priorité dynamique}
%
%La priorité de la tache varie, en fonction par exemple de son vieillissement et/ou de son historique de consommation CPU.
%C'est le cas de la plupart des ordonnanceurs nom temps réels, comme SFQ (\FIXME à confirmer)n ou encore SCHED\_OTHER (\FIXME trouver
%son vrai nom).
%
%\paragraph{Les ordonnanceurs à priorité hybride}
%
%La priorité peut varier, selon le type de tache. On parle d'ordonnanceurs {\it FP/DP} ({\it Fixed Priority/Dynamic Priority}).
%
%\subsubsection{Cas des ordonnanceurs SMP}
%
%\paragraph{}
%Les grandes familles d'ordonnanceurs SMP temps-réel sont les suivantes :\\
%\begin{enumerate}
%\item \index{Partitioned scheduling}Partitioned scheduling
%\item \index{Global scheduling}Global scheduling
%\item \index{Semi-partitioned scheduling}Semi-partitioned scheduling
%\end{enumerate}
%
%\paragraph{Partitioned scheduling}
%
%Il y a autant de queues de processus que de c{\oe}urs. La répartition des tâches se fait
%statiquement. Cela pose cependant la problématique de l'impact des synchronisations et des locks (besoin de
%précédance entre les tâches).
%
%\paragraph{Global scheduling}
%
%Il y a une et une seule queue de processus pour l'ensemble des tâches. Cela pose le
%problème d'accès en écriture à cette queue dans les différentes instanciations
%de l'algorithme d'ordonnancement (une instance par c{\oe}ur \textit{actif}, afin
%de maintenir une bonne optimisation du coût de l'euristique d'ordonnancement).
%
%\paragraph{Semi-partitioned scheduling}
%Il en existe deux types :
%\begin{itemize}
%\item Job partitioning:\\
%Une instance d'une tâche s'exécute complètement sur un seul c{\oe}ur. Par contre,
%à chaque ré-exécution de la tâche, son positionnement dans les c{\oe}urs peut varier.
%Cette variation est cependant fixée par avance et maîtrisée. Elle respecte donc un schéma
%prédéfini et le ré-exécute indéfiniment.
%\item Instance partitioning:\\
%Un processus en cours d'exécution peut migrer d'un c{\oe}ur à un autre.
%Kato $(jap)$ propose l'algorithme suivant :
%Le WCET de la tâche est découpé en blocs de taille $\frac{D_{i}}{s}$, où
%$D_{i}$ est le temps avant lequel la tâche doit être exécutée, et $s$ le
%nombre de c{\oe}urs \textit{actifs}.\\
%À chaque fin de bloc, le processus migre d'un c{\oe}ur au suivant.
%Cela a pour conséquence de consommer le moins de c{\oe}urs possible, tout
%en les chargeant au maximum. C'est à double tranchant dans le sens où
%toute variation des hypothèses (inexactitude de WCET, surcharge temporelle) impacte violemment
%la capacité d'ordonnancement des tâches.\\
%L'algorithme ne semble donc pas résistant à toute élasticité du WCET, même contrôlée.
%Cette euristique ne possède pas de viabilité (\textit{sustainability} - capacité à résister à
%l'élasticité de valeurs de $c_{i}$, $T_{i}$, ou $D_{i}$, quel que soit la t\^{a}che $\tau_{i}$).\\
%Il s'agit d'une propriété importante du fait que la mesure du WCET
%peut être trop pessimiste (mesure via pire cas du graphe d'exécution,
%mais où le processus ne s'exécute que rarement en suivant cette branche) ou trop optimiste
%(mesure au travers d'une étude empirique sur le comportement du processus, n'ayant pas
%forcément pris en compte le véritable \textit{pire cas} si celui ci ne s'est pas produit).
%\end{itemize}
%
\section{Les domaines de sécurité}

\paragraph{}
Un domaine de sécurité est un environnement ou un contexte incluant un
ensemble de ressources système et d'entités respectant une même politique de
sécurité.\\
Les interaction intra-domaines ne sont pas soumises à des exigences fortes de
sécurité. Cependant, il arrive qu'un même domaine de sécurité soit
physiquement séparé en plusieurs sous-systèmes qui, pour communiquer, doivent
traverser des domaines de sécurité différents. Les passerelles
d'interconnexion doivent alors assurer le {\it noircissement} des données à
faire transiter. L'action de noircissement consiste à assurer la
confidentialité des données de diverses manière afin d'assurer que les
domaines par lesquels transitent ces données ne soient pas aptes à en tirer
une information dont le niveau de sécurité est supérieur au leur.\\
De manière générale, le noircissement implique un chiffrement des données et
potenciellement une modification du profil de flux, ceci afin de limiter la
problématique de cannal auxiliaire dérivant du profil de flux initial.

\section{La virtualisation}

\paragraph{}
Il existe plusieurs type de virtualisation, en fonction de l'architecture
logicielle et parfois matérielle qui entre en compte dans la solution.

\subsection{Les solution de virtualisation type I}

\paragraph{}
Les solutions de virtualisation de type I s'appuie sur un moniteur de machines
virtuelles autonome, s'exécutant directement sur le matériel. Il est seul
maître de l'environnement logiciel de la machine et assure l'ordonnancement
des compartiments. La Figure \ref{fig:vmtype1} décrit l'architecture
logicielle de ce type de solution.\\
Les interactions entre le moniteur de machines virtuelles et ces dernières
peuvent varier selon la solution utilisée pour assurer la bonne exécution de
l'environnement invité (i.e. l'environnement logiciel de la machine virtuelle)
et plus particulièrement du noyau invité.\\
Pour répondre à ce besoin, il existe trois solutions:
\begin{itemize}
  \item La virtualisation complète
  \item La paravirtualisation
  \item La virtualisation matérielle
\end{itemize}

\begin{figure}
  \label{fig:vmtype1}
  \input{figures/vmtype1.tex}
  \caption{Architecture logicielle générique des solutions de virtualisation de type I}
\end{figure}

\subsubsection{Principe de la virtualisation complète}

\paragraph{}
Historiquement la plus ancienne, elle fut formalisée en terme d'exigences par
G.J. Popek et R.P. Goldberg\cite{popek} dès 1974. Le principe est de pouvoir
exécuter un environnement logicielle dans une machine virtuelle tout en
assurant que sont comportement est identique à une exécution native.
L'environnement invité n'est pas modifié, et c'est au travers des exceptions
processeurs dûes à l'exécution en tant qu'environnement invité que le moniteur
de machines virtuelles réagit à des traitements qui ne peuvent être exécutés
directement par l'environnement invité sans impact sur la sécurité et la
sûreté du système.\\
Dans l'Article \cite{popek}, les auteurs décrivent les incompatibilité du
matériel avec ce type de principe. En effet, seules les instructions générant
des exception processeur peuvent être substituées par le moniteur de machine
virtuelles. Les
instructions sensibles ne générant pas d'exception sont une problématique
fortes car ces dernières peuvent s'exécuter sans que le moniteur de machines
virtuelles en soit informé. La conséquence peut être plus ou moins grave car
ces dernières impactent la sûreté et la sécurité du système.

\paragraph{}
Afin de pouvoir virtualiser des environnements logiciels sans l'aide du matériel
et sans modifier l'environnement à virtualiser, plusieurs solutions ont été
mises en place, entre autre par VMWare. Dans ses
solutions, ce dernier est capable de déterminer les instructions sensibles qui seront
demandées par l'environnement invité et de modifier ces dernières afin
d'injecter des traitements simulés en lieu et place. Ce type de solution est
transparent pour l'environnement invité, mais génère des variations dans le
coût effectif d'un certain nombre de traitement. Ces variations font parties
des mécanismes permettant de détecter que l'environnement d'exécution est
virtualisé.

\subsubsection{Principe de la paravirtualisation}

\paragraph{}
Afin de limiter la problématique de simulation des traitements consécutifs à
la détection d'instructions incompatibles avec la virtualisation, une solution
a été définie visant à impliquer le noyau de l'environnement invité dans le
support de la virtualisation. Le noyau est alors modifié pour substituer les
instructions sensibles par des appels volontaires à des fonctions du moniteur
de machines virtuelles. Dans ce cas, le moniteur traîte la fonction soit en
exécutant le traitement initial avec les droits qui sont les siens, soit en
simulant le traitement. On parle alors d'{\it hypercalls}.

\paragraph{}
Il existe un certain nombre de solution permettant la paravirtualisation.
C'est par exemple le cas de Xen ou encore de la solution propriétaire PikeOS
de Sysgo. La limitation du principe de virtualisation est dûe à l'éxigence
d'adaptation du noyau invité à l'API du moniteur de machine virtuelle. Cette
API n'étant pas normalisée, cela implique une implémentation souvent
spécifique à la solution de virtualisation utilisée. Cela implique également
d'avoir accès au code source du noyau invité, limitant ainsi les choix en
terme d'environnements paravirtualisables.

\subsubsection{Principe de la virtualisation matérielle}

\paragraph{}
Définie initialement par IBM, puis reprise depuis le début des années 2000 par
Intel et AMD, la virtualisation matérielle consiste à intégrer dans le
matériel des aides à la virtualisation. Le but est de limiter la complexité du
moniteur de machines virtuelles, ce dernier reposant sur des instructions
privilégiées de configuration du matériel pour gérer des environnements
matériels d'exécution des machines virtuelles séparés. Un exemple de solution
est l'architecture VT-d d'Intel, proposant des structure de configuration et
d'hébergement matérielles nommée VMCS (Virtual Machine Control Structure) qui
dupliquent les registres sensibles comme le PDBR. Lorsque le moniteur de
machines virtuelles ordonnance l'environnement virtualisé, il indique au
processeur le VMCS à utiliser en lieu et place des registres utilisés
par le moniteur lui même. La solution s'appuie également sur des élément de
filtrage d'accès au périphériques (nommés I/OMMU pour Intel, SMMU pour ARM
Cortex A15). Il n'existe à ce jour aucune norme et chaque fondeur possède
ses propres instructions.\\
Cette solution n'est pas considérée ici car incompatible des exigences de
portabilité.



\subsection{Les solution de virtualisation type II}

\paragraph{}
Les soltions de virtualisation type II s'appuie sur la présence d'un hôte qui
accueil le moniteur de machine virtuelle comme un processus. Ce dernier est
donc lui-même ordonnancé par l'hôte. Ce type de solution de virtualisation
correspond à des produits tels que VMWare Workstation ou encore VirtualBox.
La Figure \ref{fig:vmtype2} décrit l'architecture logicielle d'une telle
solution.
\begin{figure}
  \label{fig:vmtype2}
  \input{figures/vmtype2.tex}
  \caption{Architecture logicielle d'une solution de virtualisation type II}
\end{figure}

\paragraph{}
Ces solutions étant basée sur des hôtes de type GNU/Linux ou Windows dans
lesquels elles s'exécutent comme des tâches non temps réel, elles sont
donc incompatibles de besoin temps réel, y compris lorsque ces derniers sont
relativement faible. Elles sont également incompatibles des exigences de
sécurité forte, du fait de la non-certifiabilité du système hôte.

\paragraph{}
Les solutions de moniteurs de machines virtuelles utilisant ce type
d'architecture ne permettent pas d'assurer un cloisonnement spatial fort
(répartition stricte en mémoire physique) ni un cloisonnement temporel fort
(l'ordonnancement des machines virtuelles étant assujetti aux propriétés de
l'ordonnanceur de l'environnement hôte.

\subsection{Les solutions de virtualisation applicative}

\paragraph{}
Les solutions de virtualisation applicative sont parentes des solution de
virtualisation de type II, mais ne gèrent pas de noyau invité. Elles
permettent d'instancier aisément des compartiments dont le noyau est partagé
entre les compartiment, mais également avec l'hôte. La gestion de
l'ordonnancement des taches des compartiment est géré directement par
l'ordonnanceur de l'hôte, sous forme de groupes de tâches autonomes. Ce type
d'ordonnancement se rapproche ainsi du principe d'ordonnancement hiérarchique
qui sera décrit dans le Chapitre \ref{sec:hierarchique}.\\
Un exemple de solution de virtualisation applicative est la solution LXC
(LinuX Containers) de Linux. La Figure \ref{fig:vmlxc} décrit
l'architecture logicielle d'une telle solution. 

\paragraph{}
Les solution de virtualisation applicatives permettent de créer des compartiments
dont le système de fichiers, le jeu de tâches, les interfaces réseau et la
vision du stockage de masse est séparé. 

\begin{figure}
  \label{fig:vmlxc}
  \input{figures/vmlxc.tex}
  \caption{Architecture logicielle d'une solution de virtualisation applicative}
\end{figure}


\section{Les interactions entre le matériel, la sécurité et le temps réel}

\subsection{Principes de base de la MMU}

\paragraph{}
Un applicatif, construit dans un fichier binaire lors de l'édition de lien, possède un certain nombre de sections
servant de conteneur à des données différentes, et ayant en conséquence des propriétés de sécurité (en terme d'accès)
également différente. De ce fait, lors du déploiement de ces différentes section en mémoire vive, il est nécessaire
de   considérer   pour   chacune   de   ces   section   des    propriétés    d'accès    spécifiques.

\paragraph{}
Lorsque plusieurs applicatifs s'exécutent de manière concurente, il est
important de les cloisonnner dans des environnement logiques séparés, ceci
afin d'éviter que ces derniers puissent interagir sans contrôle, par simple
accès à la mémoire physique.

\paragraph{}
Il existe plusieurs niveau d'abstraction de la mémoire physique :
\begin{itemize}
\item Sans aucune abstraction, on est en mode dit  {\it  flat  memory}.
L'ensemble  des  logiciels s'exécutant en mémoire traite des adresses physiques.
Ainsi,  ils  connaissent  leur  position  en mémoire,   et    ont    une    visibilité
sur    l'ensemble    de    la    mémoire    physique.\\
De ce fait, il leur est possible d'accéder au contenu des autres applicatifs en  cours  d'exécution.
Il devient ainsi aisé d'espionner les sections d'autres logiciels, comme les section data ou rodata,
dans  lesquelles   il   est   possible   de   récupérer   par   exemple   des   mots   de   passe.\\
Autre risque : l'accès à la pile des autres applicatifs peut permettre de dériver son exécution vers
un bloc de code spécifique, en remplaçant l'adresse de la fonction parente par une adresse  dont  on
maîtrise le contenu. Du fait que la pile est par essence accessible en écriture, tous les processus
peuvent modifier les champs d'un processus donné en mode {\it flat memory}, à partir  du  moment  où
l'adresse physique de la pile est connue.
\item Décomposition de la mémoire physique en segment.  Il s'agit du mode  de  segmentation  présent
uniquement sur les processeur de la famille Intel IA32.  Ce mode étant aujourd'hui remplacé  par  le
système de pagination, il ne sera pas décrit ici.
\item En s'appuyant sur une abstraction de la mémoire physique, appellé {\it mémoire virtuelle}. On
parle alors de mode dit de  {\it  pagination  mémoire}.   Un  processus  donné  possède  son  propre
environnement mémoire, dont la taille correspond à la valeur maximum d'un mot mémoire (e.g. dans un
processeur 32 bits, l'environnement  mémoire  virtuelle  à  une  taille  de  quatre  giga-octets).\\
Cet environnement mémoire est appellé AS (Address Space).  Le processus, lors de son  exécution,  ne
traîte alors que des adresses virtuelles, dont la  translation  en  adresses  physique  se  fait  au
travers d'un élément matériel nommé MMU.   La  conséquence  directe  est  que  le  coeur  processeur
travaille en adressage virtuel.
\end{itemize}

\paragraph{}
La gestion de la pagination est géré par le contrôleur MMU, sous les ordre du
noyau du système d'exloitation. Ce dernier construit l'association entre
mémoire physique et mémoire virtuelle dans des structures en mémoire nommées
Pages Tables. Sur les architecture x86, ces dernières sont accédées par la MMU
via un registre nommé PDBR. Ce dernier pointe vers la racine de l'arbre de
mapping de mémoire virtuelle du processus en cours, et s'appuie sur ce dernier
pour retrouver l'adresse mémoire physique associée à l'adresse mémoire
virtuelle demandée par le coeur processeur. Afin d'optimiser les accès à cet
arbre de page, la MMU s'appuie sur un cache local nommé TLB (Translation
Lookaside Buffer), réduisant ainsi les requêtes au contrôleur mémoire.

\paragraph{}
Le   principe   de   fonctionnement   de   la   pagination    fournit    plusieurs    avantages    :
\begin{itemize}
\item A chaque processus est associé un AS (espace d'adressage) qui  lui  est  propre.
Ainsi, il se voit seul en mémoire et il ne possède plus la capacité d'accéder aux autres  processus.
En effet, l'accès à la mémoire physique se fait via un accès à une adresse  virtuelle.   Pour  qu'un
processus y accède, une association entre adresse physique et adresse virtuelle, locale à cet espace
d'adressage, doit avoir été faite par le noyau en préalable, via configuration de la MMU. Lors de la
création d'un processus, seules les adresses physiques correspondant  à  ces  propres  données  sont
intégrées à l'espace d'adressage, interdisant à ce dernier  d'accéder  aux  données  de  tout  autre
processus.
\item De plus, la gestion de la fragmentation de la mémoire centrale, dépendant des  allocations  et
désallocations successives de la mémoire, est rendue opaque en mémoire virtuelle. Des espace mémoire
physiques disjoints peuvent être positionnés de manière contigüe dans l'espace
d'adressage virtuel,  faisant croire  au  processus  que  ses  données  sont  contigüe  sans  pour
autant  qu'elles  le   soient.
\item Le contrôleur MMU permet d'associer à chaque page mémoire des propriété
en terme d'accès:\\
\begin{itemize}
  \item Niveau d'exécution minimum (mode administrateur ou utilisateur)
  \item droits d'accès (lecture, écriture et/ou exécution)
\end{itemize}
Ces droits d'accès permettent, dans un espace d'adressage donnée, de limiter
les accès à chaque page au processus. Une telle protection réduit la surface
d'attaque, comme en complexifiant par exemple l'exploitation de dépassement de pile
\end{itemize}
La Figure \ref{fig:mmu} décrit le principe général de pagination entre mémoire
physique et mémoire virtuelle.

\begin{figure}
\label{fig:mmu}
\input{figures/mmu_as.tex}
\caption{Principe général du mapping de page mémoire dans les espaces
d'adressage des processus}
\end{figure}

\paragraph{}
Les attaques entre les différents processus ne sont pas les  seules  possibles.   Il  est  également
possible de corrompre le comportement d'un processus donné, pour lui faire faire un  traitement  non
prévu.\\
Une des attaques les plus classiques est l'injection  de  {\it  shellcode}.   L'attaque  consiste  à
intégrer quelque part dans l'espace d'adressage du processus  une  suite  d'instructions,  avant  de
corrompre   la   pile   pour   provoquer   un   branchement   vers   ce    bloc    d'instructions.\\
Cette attaque est donc faite en deux temps, en positionnant des instructions en  mémoire,  avant  de
forcer un branchement.  Cela implique de pouvoir écrire des données  dans  une  partie  de  l'espace
mémoire qui soit exécutable.  Cela implique de pouvoir écraser des données déjà présente  sans  pour
autant provoquer un plantage du processus.  En général, les shellcodes sont placés dans la pile,  en
amont du positionnement courant du pointeur de pile. Il convient, pour éviter ce type d'attaque, de
rendre l'espace mémoire utilisé pour la pile non-exécutable.  A l'inverse, l'espace  mémoire  où  se
situe le code du processus doit lui être exécutable, mais ne doit pas  être  écrivable,  ce  dernier
étant immuable durant l'exécution du processus.\\
On définit ici des droits d'accès à des  espace  mémoire  (lecture,  écriture,  exécution)  variants
selon le type de données qui y sont hébergé. Les MMU modernes, lors de la création d'une association
entre mémoire physique et mémoire virtuelles,  définissent  ces  trois  propriétés,  fournissant  un
durcissement   supplémentaire   pour   réduire   les   risques   de   corruption   d'un   processus.

\subsection{A propos des contrôleurs de cache}
\label{sec:notions_caches}

\subsubsection{Description générale}

\paragraph{}
Le \index{Cache}cache est une mémoire statique à accès  rapide,  permettant  d'accélérer  les  accès
mémoire du CPU, en copiant les données (code et data) de manière à ce  qu'elles  soient  accessibles
directement  par  le  CPU,  sans   la   lourdeur   d'un   transfert   sur   le   bus\cite{cachalgo}.

\paragraph{}
La figure \ref{fig:cache_soa} montre une architecture mémoire générique, avec plusieurs  niveaux  de
cache, suivis d'un bus d'accès mémoire et  du  controlleur  mémoire  principale  (DDR  en  général).

\begin{figure}
\input{figures/cache_architecture.tex}
\caption{Architecture en couche générique\label{fig:cache_soa}}
\end{figure}

\paragraph{}
Il existe dans les processeurs modernes plusieurs niveaux de cache :
\begin{enumerate}
\item Cache niveau 1, le plus petit et accessible le plus rapidement.  Ce  cache  est  spécialisé  :
il définit un conteneur dit {\it cache code},
pour les éléments de code (section {\texttt .code} des binaires), et un  conteneur  dit  {\it  cache
data},   pour   le   reste   des    données    ({\texttt    .bss},    {\texttt    .rodata},    ...).
\item Cache niveau 2, de taille plus  conséquente,  mais  dont  l'accès  est  moins  rapide  que  le
cache L1. Ce cache n'est pas spécialisé.
\item Cache niveau  3,  de  taille  plus  grande  encore.   Partagé  entre  les  c{\oe}urs  sur  les
architectures multi-c{\oe}urs. Ce cache n'est pas spécialisé. Ce dernier n'est pas toujours présent.
\end{enumerate}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\textbf{Niveau de mémoire}} & {\textbf{Temps d'accès}} \\
\hline
Cache L1 & 2-8 nano secondes \\
Cache L2 (partagé) & 5-12 nano secondes \\
Mémoire centrale & 10-60 nano secondes \\
\hline
\end{tabular}
\end{center}
\label{tab:cache_access_time}
\caption{Temps d'accès mémoire sur un Intel Core 2 duo}
\end{table}

\paragraph{}
Le tableau \ref{tab:cache_access_time}\footnote{confer \url{http://expertester.wordpress.com/2008/05/30/core-2-duo-vs-pentium-dual-core/}} fournit
un exemple de comparaison  du  coût  d'accès  à   une   donnée   selon   son   positionnement   en   mémoire.

\paragraph{}
Les algorithmes de cache ont divers niveaux d'associativité :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Les algorithmes de type {\textit one-way associative} associent  une  cellule  mémoire  à  une
et une seule cellule de cache.  Il s'agit d'un algorithme surjectif, du fait de la taille du  cache.
Le choix de remplacement étant de type $1:1$, aucun algorithme spécifique n'est utilisé. Les données
sont écrites au seul emplacement possible en cache.
\item Les algorithmes de type {\textit N-way associative} et supérieur associe une cellule mémoire avec
{\it N} (resp. 2, 4, 8, ...)
cellules de cache. Cette associativité permet d'optimiser l'écrasement des cellules du cache en donnant
un choix de positionnement à la cellule mémoire à mettre en cache.  Les algorithmes de  ce  type  se
différencient par leur politique de choix de la cellule à remplacer.
\end{itemize}
On a ainsi :
\paragraph{{\it N}-Way Set Associative}
Cet algorithme permet, pour chaque élément mémoire à charger  en  cache,  de  pouvoir  se  placer  à
{\it N} endroits dans le cache.  Ces {\it N} positions sont dépendantes  de  l'adresse  physique  en
mémoire centrale de la donnée.\\
Le choix de  la  cellule  parmis  la  liste  des  cellules  admissibles  se  fait  au  travers  d'un
algorithme de gestion de cache tel que décrit plus loin.

\paragraph{Direct Mapped}
L'algorithme Direct Mapped est un algorithme simple, optimisant le coût d'exécution du choix de la cellule.
Un élément mémoire n'est placé qu'à un seul endroit dans le cache, en fonction de son adresse physique,
quelle que soit la donnée précédemment présente.


\paragraph{}
Lorsqu'une donnée n'est pas en cache, on parle de {\it cache-miss}. Le processeur va successivement
vérifier  les  caches  L1,  L2  et  L3  (siprésent),  avant   d'initier   une   recopie   à   partir
de    la    mémoire    centrale.

\paragraph{}
Il existe plusieurs algorithmes de gestion du cache, tous de type  probabiliste,  ce  qui  les  rend
malheureusement incompatibles avec les besoins du temps réel dur.\\
En conséquence, les systèmes temps réel très  contraints,  comme  dans  l'aviation  ou  le  spatial,
utilisent des architectures avec cache désactivé.

\subsubsection{Positionnement du cache}

\paragraph{Cache \index{Cache!Look-aside}look-aside}
Cette architecture place le cache et la mémoire centrale côte à côte.  Ainsi, le CPU peut accéder  à
la mémoire centrale sans passer par le cache.\\
Le principe de fonctionnement est le suivant :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le processeur démarre un cycle de lecture
\item Si le cache possède la donnée en local (cache hit), il répond au CPU, et clôture le  cycle  de
lecture sur le bus.
\item Si le cache ne possède pas la donnée (cache miss), il laisse la mémoire centrale  répondre  au
processeur,  et  espionne  les  données  transitant  sur  le  bus  afin  de  se   mettre   à   jour.
\end{itemize}

\begin{figure}[h]
\input{figures/cache_memory_bus.tex}
\caption{Comparaison   des   architectures   de   type   cache   through   et    cache    lookaside}
\label{fig:cache_look_aside}
\end{figure}


\paragraph{Cache \index{Cache!Look-through}look-through}
Cette architecture place le cache en coupure entre le processeur et la mémoire centrale. Le CPU passe
donc par le cache pour l'accès mémoire.\\
Le principe est le suivant :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le processeur démarre un cycle de lecture
\item  Si  le  cache  possède  la  donnée  en  local  (cache  hit),   il   répond   au   processeur.
\item Si le cache ne possède pas la donnée, il transfert la requête de lecture sur le bus. La mémoire
centrale renvoie la donnée, qui est mise en cache en même temps qu'elle est renvoyé  au  processeur.
\end{itemize}

\begin{figure}[h]
\input{figures/cache_look_through.tex}
\caption{Architecture de cache en mode {\it look-through}}
\label{fig:cache_look_through}
\end{figure}

Cette architecture provoque des cache miss plus long (vérification par le cache avant  de  faire  la
requête au contrôleur de bus), mais permet d'utiliser le cache indépendamment des autres processeurs
(pour un cache local).

\subsubsection{\index{Cache!Consistance}Consistance du cache}

\paragraph{}
Du fait que le cache contient des duplicatas d'une partie de la mémoire, il est important de s'assurer
que la copie  est  l'originale  restent  identique.   À  défaut,  cela  peut  entraîner  des  bogues
logiciels,  difficilement  détectables.

\paragraph{}
La consistance du cache peut être impactée de différentes manières, dont voici une liste non-exhaustive :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Mise à jour de la mémoire par un contrôleur DMA
\item  Écriture  en  mémoire  par  un  autre  processeur  (architecture   multi-CPU/culti-c{\oe}urs)
\item Modification de la donnée en cache sur un autre processeur
\end{itemize}

Il existe différentes méthodes permettant de garantir la consistance des données en caches. En voici
deux largement utilisées :

\paragraph{\index{Cache!Snooping}Snooping}
Le snooping consiste en l'espionnage du bus d'adresse, afin de détecter  une  modification  sur  une
adresse mémoire contenue en cache.\\
Il y a deux manières de réagir à une détection positive :
\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Prévoir un rechargement de la cellule impactée (méthode dite {\it write-back}). Le rechargement ne
se fait pas nécessairement sur le moment, mais peut être ordonnancée pour s'exécuter au moment du prochain
accès à cette cellule, ou avant, selon les implémentations.
\item Désactiver la cellule impactée.  Le prochain accès au données de cette cellule  provoquera  un
{\it    cache-miss},    suivi    d'une    recopie    à    partir    de    la    mémoire    centrale.
\end{itemize}

\paragraph{\index{Cache!Snarfing}Snarfing}
Le snarfing consiste à associer la détection d'une modification d'une donnée en cache à l'espionnage du
bus de donnée. En espionnant le bus, il est capable à tout instant de mettre à jour la cellule impactée,
et de maintenir la consistance.

\subsubsection{Synchronisation caches/mémoire centrale}

\paragraph{}
Lorsque des données sont modifiées en local sur le cache, il est nécessaire de mettre à jour la mémoire
centrale afin de prendre en compte ce changement.

\paragraph{write-back}
Le cache garde en mémoire, pour chaque cellule, sont état. Lorsque la cellule est écrasée, ou que le cache
est  vidé,  les  données  modifiées  localement  sont  synchronisées  avec  la  mémoire  centrale.\\
La mise à jour de la mémoire centrale n'est pas synchrone avec la modification locale. Cela implique, au
niveau logiciel, de prévoir les problèmes d'incohérence sur les architectures multi-c{\oe}urs, au travers
de l'usage des {\it \index{Memory barrier}memory barriers}.

\paragraph{write-through}
Une écriture sur cache implique une remontée synchrone de la mise à jour de la donnée, afin de maintenir
une cohésion.

\subsubsection{Les algorithmes de gestion de cache}

\paragraph{\index{Cache!LRU}Least Recently Used}
L'algorithme LRU s'appuie sur les dates d'accès aux items du cache. En cas de cache-miss, c'est l'élément
qui a été accédé il y a le plus longtemps qui est remplacé.\\
Pour des processus faisant des accès fréquents à un même espace mémoire, comme dans le cas de processus
calculatoires par exemple, LRU est efficace.

\paragraph{\index{Cache!MRU}Most Recently Used}
L'algorithme MRU, à l'opposé du LRU, remplace l'élément dont l'accès est le plus récent. Il considère
l'hypothèse  selon  laquelle  un  élément  accédé   très   récemment   ne   le   sera   plus   avant
une    certaine    durée.\\
MRU   est   plus   efficace   pour   des   processus   faisant   des   accès   mémoire    cycliques.

\paragraph{\index{Cache!Pseudo-LRU}Pseudo-LRU}
L'algorithme Pseudo-LRU utilise un arbre binaire représentant l'ensemble des cellules de caches pouvant
être remplacées. Chaque ensemble de cellules posséde un tag fournissant une indication sur l'emplacement
(branche droite, branche gauche) de la cellule qui sera remplacée.  Ce tag est mis à jour  à  chaque
traversée de l'arbre.

\paragraph{}
Il s'agit d'un algorithme jugé efficace en comparaison au LRU pour les caches dont l'associativité est
supérieure à 4.  Il est utilisé dans les architectures Intel 486, et dans les architecture  PowerPC,
comme le PowerPC G4 de Freescale \cite{plru}. Le schéma \ref{fig:cache_plru_e300} est tiré du manuel
de   référence    du    processeur    e300,    utilisé    dans    les    power    quicc    2    pro.

\begin{figure}
\input{figures/cache_plru_e300.tex}
\caption{Automate   à   état   de    l'algorithme    pseudo-LRU    du    Power    Quicc    2    Pro}
\label{fig:cache_plru_e300}
\end{figure}


\paragraph{\index{Cache!Segmented-LRU}Segmented LRU}
L'algorithme SLRU utilise deux segments de caches complémentaires, dont les cellules sont ordonnées de
la dernière utilisée (MRU) à la plus anciennement utilisée (LRU) :

\begin{itemize}
\renewcommand{\labelitemi}{\textbullet}
\item Le premier segment, nommé {\it probationary segment}, est celui sur lequel sont mises les données
mémoire, en cas de cache miss. C'est également le seul segment où des données peuvent être retirées
du cache.
\item Le second segment, nommé {\it protected segment}, contient la liste des cellules de cache ayant
été accédées au moins deux fois.
\end{itemize}
Le schéma \ref{fig:slru_1} montre ainsi que l'algorithme de cache fait une différence entre des données
accédées à faible fréquence et des données accédées régulièrement.

\begin{figure}
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_1.tex}
\end{minipage}\hfill
\begin{minipage}[c]{.46\linewidth}
  \input{figures/slru_2.tex}
\end{minipage}\hfill
\caption{Intégration et déplacement des données accédées fréquemment}
\label{fig:slru_1}
\end{figure}


\paragraph{}
En effet, lorsqu'une cellule est accédée une seconde fois après un premier cache miss, celle-ci  est
transférée  sur  le  segment  protégé,  en  tête  de  liste  coté  MRU   (Most   Recently   Used).\\
Cela a pour conséquence, si le segment protégé est plein, de faire descendre la cellule la plus anciennement
accédée sur le segment probatoire, en tête de liste coté MRU, ceci afin de donner une deuxième chance à
toute donnée accédée au moins deux fois. Lorsque le segment probatoire est plein et que de nouvelles
données y sont intégrées, que ce soit via un déplacement d'une cellule du segment protégé ou du fait d'un
cache miss, la cellule la moins récemment utilisée et écrasée.

\begin{figure}
\input{figures/slru_3.tex}
\caption{Principe de deuxième chance du SLRU}
\end{figure}

\paragraph{}
Cet algorithme fait donc une différence entre les données accédées souvent (au moins deux fois) et les
données   accédées   rarement   (une   fois)   pendant    leur    durée    de    vie    en    cache.
Il  permet  donc  d'optimiser  la  gestion  du  cache  lors   d'accès   mémoire   peu   fréquents.\\
La  taille  des  deux   segments   est   dépendante   dynamiquement   du   schéma   d'entrée/sortie.

\paragraph{\index{Cache!LFU}Least Frequently Used}

Lors d'un cache miss, la cellule utilisée pour charger les données mémoire est celle dont le contenu est
utilisé le moins fréquemment. En cas d'égalité de fréquence, l'algorithme LRU est utilisé pour départager
les cellules.



